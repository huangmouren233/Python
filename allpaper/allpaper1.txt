3D Face Reconstruction from Light Field Images:
A Model-free Approach

Mingtao Feng1 , Syed Zulqarnain Gilani2 , Yaonan Wang1 , and Ajmal Mian2

1 College of Electrical and Information Engineering, Hunan University,410006, China

{mintfeng,yaonan}@hnu.edu.cn

2 Computer Science and Software Engineering, The University of Western Australia, 6009,
Australia

{zulqarnain.gilani,ajmal.mian}@uwa.edu.au

Abstract. Reconstructing 3D facial geometry from a single RGB image has re-
cently instigated wide research interest. However, it is still an ill-posed problem
and most methods rely on prior models hence undermining the accuracy of the
recovered 3D faces. In this paper, we exploit the Epipolar Plane Images (EPI)
obtained from light ﬁeld cameras and learn CNN models that recover horizontal
and vertical 3D facial curves from the respective horizontal and vertical EPIs.
Our 3D face reconstruction network (FaceLFnet) comprises a densely connected
architecture to learn accurate 3D facial curves from low resolution EPIs. To train
the proposed FaceLFnets from scratch, we synthesize photo-realistic light ﬁeld
images from 3D facial scans. The curve by curve 3D face estimation approach
allows the networks to learn from only 14K images of 80 identities, which still
comprises over 11 Million EPIs/curves. The estimated facial curves are merged
into a single pointcloud to which a surface is ﬁtted to get the ﬁnal 3D face. Our
method is model-free, requires only a few training samples to learn FaceLFnet
and can reconstruct 3D faces with high accuracy from single light ﬁeld images
under varying poses, expressions and lighting conditions. Comparison on the BU-
3DFE and BU-4DFE datasets show that our method reduces reconstruction errors
by over 20% compared to recent state of the art.

1

Introduction

Three dimensional face analysis has the potential to address the challenges that con-
found its two dimensional counterpart such as variations in illumination, pose and
scale [4]. This modality has achieved state-of-the-art performances on applications such
as face recognition [14, 36, 39, 65], syndrome diagnosis [17, 16, 47, 55], gender classi-
ﬁcation [15] and face animation [9, 49]. Reconstructing 3D facial geometry from RGB
images is, therefore, receiving a signiﬁcant interest from the research community. How-
ever, using a single RGB image to recover the 3D face is an ill-posed problem [31] since
the depth information is lost during the projection process. In fact, many different 3D
shapes can result in similar 2D projections. The scale and bas-relief ambiguities [6] are
common examples.

2

M Feng, SZ Gilani, Y Wang and A Mian

Most existing methods have resorted to the use of prior models such as the Basal
Face Model (BFM) [43] and the Annotated Face Model(AFM) [12] to generate syn-
thetic data with ground truth to train CNN [11, 40] models and to recover the model
parameters at test time. However, model-based approaches are inherently biased and
constrained to the 
3D Recurrent Neural Networks with Context
Fusion for Point Cloud Semantic Segmentation

Xiaoqing Ye1,2 [0000−0003−3268−880X ] , Jiamao Li1 [0000−0002−7478−4544] , Hexiao

Huang3 , Liang Du1,2 , and Xiaolin Zhang1

1 Shanghai Institute of Microsystem and Information Technology, Chinese Academy
of Sciences, Shanghai, China
2 University of Chinese Academy of Sciences, Beijing, China

3 School of Science and Technology, Shanghai Open University, Shanghai, China

{qingye,jmli}@mail.sim.ac.cn

Abstract. Semantic segmentation of 3D unstructured point clouds re-
mains an open research problem. Recent works predict semantic label-
s of 3D points by virtue of neural networks but take limited context
knowledge into consideration. In this paper, a novel end-to-end approach
for unstructured point cloud semantic segmentation, named 3P-RNN,
is proposed to exploit the inherent contextual features. First the eﬃ-
cient pointwise pyramid pooling module is investigated to capture local
structures at various densities by taking multi-scale neighborhood into
account. Then the two-direction hierarchical recurrent neural networks
(RNNs) are utilized to explore long-range spatial dependencies. Each re-
current layer takes as input the local features derived from unrolled cells
and sweeps the 3D space along two directions successively to integrate
structure knowledge. On challenging indoor and outdoor 3D datasets,
the proposed framework demonstrates robust performance superior to
state-of-the-arts.

Keywords: 3D semantic segmentation · Unstructured point cloud · Re-
current neural networks · Pointwise pyramid pooling

1

Introduction

Scene understanding has been extensively studied due to its critical role in au-
tonomous driving, robot navigation, augmented reality and 3D reconstruction.
Despite of the tremendous progress made in the ﬁeld of semantic segmentation
with the help of deep learning strategies, most approaches cope with 2D images
[1–3], whereas 3D semantic segmentation of unstructured point clouds remain-
s a challenging problem due to its large-scale point data, irregular shape and
non-uniform densities.
Previous learning-based attempts mainly focus on regularizing input point
cloud shapes, so as to draw on the experience of 2D semantic segmentation
networks. For example, points are ﬁrst voxelized by volumetric occupancy grid
representation and 3D Convolutional Neural Networks (CNN) are employed to

2

X. Ye et al.

learn voxel-level semantics. Due to the sparsity of point clouds, the voxelization
is ineﬃcient and ﬁne-details are missed to avoid high computation cost. Besides,
the accuracy is limited because all points within the same voxel are assigned with
the same semantic label. To make use of 2D frameworks, snapshots of 2D images
taken at multi-views of 3D space are also learned, however, the repro jection back
to 3D space is also a nontrivial problem.
The ﬁrst pioneer work PointNet that directly operates on 3D point cloud
is recently proposed [4]. With
3D Scene Flow from 4D Light Field Gradients

Sizhuo Ma, Brandon M. Smith, and Mohit Gupta

Department of Computer Sciences, University of Wisconsin-Madison, USA

{sizhuoma,bmsmith,mohitg}@cs.wisc.edu

Abstract. This paper presents novel techniques for recovering 3D dense scene
ﬂow, based on differential analysis of 4D light ﬁelds. The key enabling result is a
per-ray linear equation, called the ray ﬂow equation, that relates 3D scene ﬂow to
4D light ﬁeld gradients. The ray ﬂow equation is invariant to 3D scene structure
and applicable to a general class of scenes, but is underconstrained (3 unknowns
per equation). Thus, additional constraints must be imposed to recover motion.
We develop two families of scene ﬂow algorithms by leveraging the structural
similarity between ray ﬂow and optical ﬂow equations: local ‘Lucas-Kanade’ ray
ﬂow and global ‘Horn-Schunck’ ray ﬂow, inspired by corresponding optical ﬂow
methods. We also develop a combined local-global method by utilizing the corre-
spondence structure in the light ﬁelds. We demonstrate high precision 3D scene
ﬂow recovery for a wide range of scenarios, including rotation and non-rigid
motion. We analyze the theoretical and practical performance limits of the pro-
posed techniques via the light ﬁeld structure tensor, a 3 × 3 matrix that encodes
the local structure of light ﬁelds. We envision that the proposed analysis and al-
gorithms will lead to design of future light-ﬁeld cameras that are optimized for
motion sensing, in addition to depth sensing.

1

Introduction

The ability to measure dense 3D scene motion has numerous applications, including
robot navigation, human-computer interfaces and augmented reality. Imagine a head-
mounted camera tracking the 3D motion of hands for manipulation of objects in a virtual
environment, or a social robot trying to determine a person’s level of engagement from
subtle body movements. These applications require precise measurement of per-pixel
3D scene motion, also known as scene ﬂow [31]. In this paper, we present a novel
approach for measuring 3D scene ﬂow with light ﬁeld sensors [1, 24]. This approach is
based on the derivation of a new constraint, the ray ﬂow equation, which relates dense
3D motion ﬁeld of a scene to gradients of the measured light ﬁeld, as follows:

LX VX + LY VY + LZ VZ + Lt = 0 ,

where VX ,VY , VZ are per-pixel 3D scene ﬂow components, LX , LY , LZ are spatio-angular
gradients of the 4D light ﬁeld, and Lt is the temporal light ﬁeld derivative. This simple,
linear equation describes the ray ﬂow, deﬁned as local changes in the 4D light ﬁeld,
due to small, differential, 3D scene motion. The ray ﬂow equation is independent of the
scene depth, and is broadly applicable to a general class of scenes.
The ray ﬂow equation is an under-constrained linear equation with three unknowns
(VX ,VY ,VZ ) per equation. Therefore, it is impossible to recover the full 3D scene ﬂow

2

S. Ma, B. M. Smith and M. Gupta

without imposing further constraint
3D Vehicle Tra jectory Reconstruction in
Monocular Video Data Using Environment
Structure Constraints

Sebastian Bullinger1 [0000−0002−1584−5319] , Christoph Bodensteiner1 , Michael
Arens1 , and Rainer Stiefelhagen2

1 Fraunhofer IOSB, Ettlingen, Germany

{sebastian.bullinger,christoph.bodensteiner,michael.arens}
@iosb.fraunhofer.de

2 Karlsruhe Institute of Technology, Karlsruhe, Germany

rainer.stiefelhagen@kit.edu

Abstract. We present a framework to reconstruct three-dimensional ve-
hicle tra jectories using monocular video data. We track two-dimensional
vehicle shapes on pixel level exploiting instance-aware semantic segmen-
tation techniques and optical ﬂow cues. We apply Structure from Motion
techniques to vehicle and background images to determine for each frame
camera poses relative to vehicle instances and background structures. By
combining vehicle and background camera pose information, we restrict
the vehicle tra jectory to a one-parameter family of possible solutions.
We compute a ground representation by fusing background structures
and corresponding semantic segmentations. We propose a novel method
to determine vehicle tra jectories consistent to image observations and
reconstructed environment structures as well as a criterion to identify
frames suitable for scale ratio estimation. We show qualitative results
using drone imagery as well as driving sequences from the Cityscape
dataset. Due to the lack of suitable benchmark datasets we present a
new dataset to evaluate the quality of reconstructed three-dimensional
vehicle tra jectories. The video sequences show vehicles in urban areas and
are rendered using the path-tracing render engine Cycles. In contrast to
previous work, we perform a quantitative evaluation of the presented
approach. Our algorithm achieves an average reconstruction-to-ground-
truth-tra jectory distance of 0.31 meter using this dataset. The dataset
including evaluation scripts will be publicly available on our website3 .

Keywords: Vehicle Tra jectory Reconstruction, Instance-aware Seman-
tic Segmentation, Structure-from-Motion

1

Introduction

1.1 Tra jectory Reconstruction

Three-dimensional vehicle tra jectory reconstruction has many relevant use cases
in the domain of autonomous systems and augmented reality applications. There

3 Pro ject page: http://s.fhg.de/tra jectory

2

S. Bullinger, C. Bodensteiner, M.Arens and R. Stiefelhagen

are diﬀerent platforms like drones or wearable systems where one wants to
achieve this task with a minimal number of devices in order to reduce weight
or lower production costs. We propose a novel approach to reconstruct three-
dimensional vehicle motion tra jectories using a single camera as sensor.
The reconstruction of ob ject motion tra jectories in monocular video data cap-
tured by moving cameras is a challenging task, since in general it cannot be solely
solved exploiting image observations. Each observed ob ject motion tra jectory is
scale ambiguous. Additional const
3D-CODED : 3D Correspondences by Deep
Deformation

Thibault Groueix1 , Matthew Fisher2 , Vladimir G. Kim2 , Bryan C. Russell2 ,
and Mathieu Aubry1

1 LIGM (UMR 8049), ´Ecole des Ponts, UPE
2 Adobe Research

http://imagine.enpc.fr/~groueixt/3D- CODED/

Abstract. We present a new deep learning approach for matching de-
formable shapes by introducing Shape Deformation Networks which jointly
encode 3D shapes and correspondences. This is achieved by factoring
the surface representation into (i) a template, that parameterizes the
surface, and (ii) a learnt global feature vector that parameterizes the
transformation of the template into the input surface. By predicting this
feature for a new shape, we implicitly predict correspondences between
this shape and the template. We show that these correspondences can
be improved by an additional step which improves the shape feature by
minimizing the Chamfer distance between the input and transformed
template. We demonstrate that our simple approach improves on state-
of-the-art results on the diﬃcult FAUST-inter challenge, with an average
correspondence error of 2.88cm. We show, on the TOSCA dataset, that
our method is robust to many types of perturbations, and generalizes
to non-human shapes. This robustness allows it to perform well on real
unclean, meshes from the the SCAPE dataset.

Keywords: 3D deep learning, computational geometry, shape matching

(a) Input Shape

(b) Template

(c) Deformed template

Fig. 1: Our approach predicts shape correspondences by learning a consistent
mesh parameterization with a shared template. Colors show correspondences.

2

1

T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, M. Aubry

Introduction

There is a growing demand for techniques that make use of the large amount
of 3D content generated by modern sensor technology. An essential task is to
establish reliable 3D shape correspondences between scans from raw sensor data
or between scans and a template 3D shape. This process is challenging due to
low sensor resolution and high sensor noise, especially for articulated shapes,
such as humans or animals, that exhibit signiﬁcant non-rigid deformations and
shape variations.
Traditional approaches to estimating shape correspondences for articulated
ob jects typically rely on intrinsic surface analysis either optimizing for an isomet-
ric map or leveraging intrinsic point descriptors [40]. To improve correspondence
quality, these methods have been extended to take advantage of category-speciﬁc
data priors [9]. Eﬀective human-speciﬁc templates and registration techniques
have been developed over the last decade [46], but these methods require signif-
icant eﬀort and domain-speciﬁc knowledge to design the parametric deformable
template, create an ob jective function that ensures alignment of salient regions
and is not prone to being stuck in local minima, and develop an optimization
strategy that eﬀectively combines a global search for a good heuristic initializa-
tion and a loc
3DFeat-Net: Weakly Supervised Local 3D
Features for Point Cloud Registration

Zi Jian Yew[0000−0002−8094−7370] ( ) and Gim Hee Lee

Department of Computer Science, National University of Singapore

{zijian.yew, gimhee.lee}@comp.nus.edu.sg

Abstract. In this paper, we propose the 3DFeat-Net which learns both
3D feature detector and descriptor for point cloud matching using weak
supervision. Unlike many existing works, we do not require manual anno-
tation of matching point clusters. Instead, we leverage on alignment and
attention mechanisms to learn feature correspondences from GPS/INS
tagged 3D point clouds without explicitly specifying them. We create
training and benchmark outdoor Lidar datasets, and experiments show
that 3DFeat-Net obtains state-of-the-art performance on these gravity-
aligned datasets.

Keywords: point cloud, registration, deep learning, weak supervision

1

Introduction

3D point cloud registration plays an important role in many real-world applica-
tions such as 3D Lidar-based mapping and localization for autonomous robots,
and 3D model acquisition for archaeological studies, geo-surveying and architec-
tural inspections etc. Compared to images, point clouds exhibit less variation and
can be matched under strong lighting changes, i.e. day and night, or summer and
winter (Fig. 1). A two-step process is commonly used to solve the point cloud
registration problem - (1) establishing 3D-3D point correspondences between
the source and target point clouds, and (2) ﬁnding the optimal rigid transforma-
tion between the two point clouds that minimizes the total Euclidean distance
between all point correspondences. Unfortunately, the critical step of establish-
ing 3D-3D point correspondences is non-trivial. Even though many handcrafted
3D feature detectors [35,5] and descriptors [26,25,13,30,28] have been proposed
over the years, the performance of establishing 3D-3D point correspondences
remains unsatisfactory. As a result, iterative algorithms, e.g. Iterative Closest
Point (ICP) [3], that circumvent the need for wide-baseline 3D-3D point corre-
spondences with good initialization and nearest neighbors, are often used. This
severely limits usage in applications such as global localization / pose estimation
[16] and loop-closures [7] that require wide-baseline correspondences.
Inspired by the success of deep learning for computer vision tasks such as
image-based ob ject recognition [21], several deep learning based works that learn
3D feature descriptors for ﬁnding wide-baseline 3D-3D point matches have been

2

Zi Jian Yew and Gim Hee Lee

proposed in the recent years. Regardless of the improvements of these deep
learning based 3D descriptors over the traditional handcrafted 3D descriptors,
none of them proposed a full pipeline that uses deep learning to concurrently
learn both the 3D feature detector and descriptor. This is because the existing
deep learning approaches are mostly based on supervised learning that requires
huge amou
3DMV: Joint 3D-Multi-View Prediction for 3D
Semantic Scene Segmentation

Angela Dai1 and Matthias Nießner2

1 Stanford University
2 Technical University of Munich

Fig. 1. 3DMV takes as input a reconstruction of an RGB-D scan along with its color
images (left), and predicts a 3D semantic segmentation in the form of per-voxel la-
bels (mapped to the mesh, right). The core of our approach is a joint 3D-multi-view
prediction network that leverages the synergies between geometric and color features.

Abstract. We present 3DMV, a novel method for 3D semantic scene
segmentation of RGB-D scans in indoor environments using a joint 3D-
multi-view prediction network. In contrast to existing methods that ei-
ther use geometry or RGB data as input for this task, we combine both
data modalities in a joint, end-to-end network architecture. Rather than
simply pro jecting color data into a volumetric grid and operating solely
in 3D – which would result in insuﬃcient detail – we ﬁrst extract feature
maps from associated RGB images. These features are then mapped into
the volumetric feature grid of a 3D network using a diﬀerentiable back-
pro jection layer. Since our target is 3D scanning scenarios with possibly
many frames, we use a multi-view pooling approach in order to handle a
varying number of RGB input views. This learned combination of RGB
and geometric features with our joint 2D-3D architecture achieves signiﬁ-
cantly better results than existing baselines. For instance, our ﬁnal result
on the ScanNet 3D segmentation benchmark increases from 52.8% to
75% accuracy compared to existing volumetric architectures.

https://github.com/angeladai/3DMV

2

1

A. Dai and M. Nießner

Introduction

Semantic scene segmentation is important for a large variety of applications
as it enables understanding of visual data. In particular, deep learning-based
approaches have led to remarkable results in this context, allowing prediction of
accurate per-pixel labels in images [22, 14]. Typically, these approaches operate
on a single RGB image; however, one can easily formulate the analogous task in
3D on a per-voxel basis [5, 13, 21, 34, 40, 41], which is a common scenario in the
context of 3D scene reconstruction. In contrast to 2D, the third dimension oﬀers
a unique opportunity as it not only predicts semantics, but also provides a spatial
semantic map of the scene content based on the underlying 3D representation.
This is particularly relevant for robotics applications since a robot relies not only
on information of what is in a scene but also needs to know where things are.
In 3D, the representation of a scene is typically obtained from RGB-D surface
reconstruction methods [26, 27, 17, 6] which often store scanned geometry in a 3D
voxel grid where the surface is encoded by an implicit surface function such as a
signed distance ﬁeld [4]. One approach towards analyzing these reconstructions
is to leverage a CNN with 3D convolutions, which has been used for shape
classiﬁcati
A Closed-form Solution to
Photorealistic Image Stylization

Yijun Li1 , Ming-Yu Liu2 , Xueting Li1 , Ming-Hsuan Yang1,2 , Jan Kautz2

1University of California, Merced 2NVIDIA

{yli62,xli75,mhyang}@ucmerced.edu

{mingyul,jkautz}@nvidia.com

Abstract. Photorealistic image stylization concerns transferring style of
a reference photo to a content photo with the constraint that the stylized
photo should remain photorealistic. While several photorealistic image
stylization methods exist, they tend to generate spatially inconsistent
stylizations with noticeable artifacts. In this paper, we propose a method
to address these issues. The proposed method consists of a stylization
step and a smoothing step. While the stylization step transfers the
style of the reference photo to the content photo, the smoothing step
ensures spatially consistent stylizations. Each of the steps has a closed-
form solution and can be computed eﬃciently. We conduct extensive
experimental validations. The results show that the proposed method
generates photorealistic stylization outputs that are more preferred by
human sub jects as compared to those by the competing methods while
running much faster. Source code and additional results are available at

https://github.com/NVIDIA/FastPhotoStyle.

Keywords: Image stylization, photorealism, closed-form solution.

1

Introduction

Photorealistic image stylization aims at changing style of a photo to that of a
reference photo. For a faithful stylization, content of the photo should remain
the same. Furthermore, the output photo should look like a real photo as it
were captured by a camera. Figure 1 shows two photorealistic image stylization
examples. In one example, we transfer a summery photo to a snowy one, while
in the other, we transfer a day-time photo to a night-time photo.
Classical photorealistic stylization methods are mostly based on color/tone
matching [1,2,3,4] and are often limited to speciﬁc scenarios (e.g., seasons [5]
and headshot portraits [6]). Recently, Gatys et al. [7,8] show that the correla-
tions between deep features encode the visual style of an image and propose
an optimization-based method, the neural style transfer algorithm, for image
stylization. While the method shows impressive performance for artistic styl-
ization (converting images to paintings), it often introduces structural artifacts
and distortions when applied to photorealistic image stylization as shown in
Figure 1(c). In a follow-up work, Luan et al. [9] propose adding a regularization
term to the optimization ob jective function of the neural style transfer algorithm

2

Y. Li, M.-Y. Liu, X. Li, M.-H. Yang, J. Kautz

(a) Style

(b) Content

(c) Gatys et al. [8] (d) Luan et al. [9]

(e) Ours

Fig. 1: Given a style photo (a) and a content photo (b), photorealistic image
stylization aims at transferring style of the style photo to the content photo as
shown in (c), (d) and (e). Comparing with existing methods [8,9], the output
photos computed by
A Dataset of Flash and Ambient Illumination
Pairs from the Crowd

Ya˘gız Aksoy1,2 , Changil Kim1 , Petr Kellnhofer1 , Sylvain Paris3 , Mohamed
Elgharib4 , Marc Pollefeys2,5 , and Wo jciech Matusik1

1 MIT CSAIL, Cambridge MA, USA
2 ETH Z¨urich, Z¨urich, Switzerland, ya@inf.ethz.ch
3 Adobe Research, Cambridge MA, USA
4 QCRI, Doha, Qatar
5 Microsoft, Redmond WA, USA

Abstract. Illumination is a critical element of photography and is es-
sential for many computer vision tasks. Flash light is unique in the sense
that it is a widely available tool for easily manipulating the scene illumi-
nation. We present a dataset of thousands of ambient and ﬂash illumina-
tion pairs to enable studying ﬂash photography and other applications
that can beneﬁt from having separate illuminations. Diﬀerent than the
typical use of crowdsourcing in generating computer vision datasets, we
make use of the crowd to directly take the photographs that make up our
dataset. As a result, our dataset covers a wide variety of scenes captured
by many casual photographers. We detail the advantages and challenges
of our approach to crowdsourcing as well as the computational eﬀort to
generate completely separate ﬂash illuminations from the ambient light
in an uncontrolled setup. We present a brief examination of illumina-
tion decomposition, a challenging and underconstrained problem in ﬂash
photography, to demonstrate the use of our dataset in a data-driven
approach.

Keywords: ﬂash photography · dataset collection · crowdsourcing · il-
lumination decomposition

1

Introduction

Crowdsourcing has been a driving force for computer vision datasets especially
with the rise of data-driven approaches. The typical use of crowdsourcing in this
ﬁeld has been obtaining answers to high-level questions about photographs [7] or
obtaining ground truth annotations [21] for simple tasks such as segmentation in
a scalable and economical manner. However, commonplace strategies that rely on
user interaction do not apply to scenarios where complex physical processes are
involved, such as ﬂash/no-ﬂash, short/long exposure, high/low dynamic range, or
shallow/deep depth of ﬁeld. With the wide availability and high quality of current
mobile cameras, crowdsourcing has a larger potential that includes the collection

2

Y. Aksoy et al.

Fig. 1. We introduce a diverse dataset of thousands of photograph pairs with ﬂash-only
and ambient-only illuminations, collected via crowdsourcing.

of photographs directly. With the motivation of scalability and diversity, we
tackle the challenge of crowdsourcing a computational photography dataset. We
introduce a new approach where the crowd captures the images that make up
the dataset directly, and illustrate our strategy on the ﬂash/no-ﬂash task.
Illumination is one of the most critical aspects of photography. The scene
illumination determines the dominant aesthetic aspect of a photograph, as well
as control of the visibility of and attention drawn to the ob jects in the s
A Framework for Evaluating
6-DOF Ob ject Trackers

Mathieu Garon[0000−0003−1811−4156] , Denis Laurendeau[0000−0003−2858−5955] and

Jean-Fran¸cois Lalonde[0000−0002−6583−2364]

Universit´e Laval⋆⋆

Abstract. We present a challenging and realistic novel dataset for eval-
uating 6-DOF ob ject tracking algorithms. Existing datasets show serious
limitations—notably, unrealistic synthetic data, or real data with large
ﬁducial markers—preventing the community from obtaining an accurate
picture of the state-of-the-art. Using a data acquisition pipeline based
on a commercial motion capture system for acquiring accurate ground
truth poses of real ob jects with respect to a Kinect V2 camera, we build
a dataset which contains a total of 297 calibrated sequences. They are ac-
quired in three diﬀerent scenarios to evaluate the performance of trackers:
stability, robustness to occlusion and accuracy during challenging inter-
actions between a person and the ob ject. We conduct an extensive study
of a deep 6-DOF tracking architecture and determine a set of optimal
parameters. We enhance the architecture and the training methodology
to train a 6-DOF tracker that can robustly generalize to ob jects never
seen during training, and demonstrate favorable performance compared
to previous approaches trained speciﬁcally on the ob jects to track.

Keywords: 3D ob ject tracking, databases, deep learning

1

Introduction

With the recent emergence of 3D-enabled augmented reality devices, tracking 3D
ob jects in 6 degrees of freedom (DOF) is a problem that has received increased
attention in the past few years. As opposed to SLAM-based camera localization
techniques—now robustly implemented on-board various commercial devices—
that can use features from the entire scene, 6-DOF ob ject tracking approaches
have to rely on features present on a (typically small) ob ject, making it a chal-
lenging problem. Despite this, recent approaches have demonstrated tremendous
performance both in terms of speed and accuracy [1–3].
Unfortunately, obtaining an accurate assessment of the performance of 6-
DOF ob ject tracking approaches is becoming increasingly diﬃcult since accuracy
on the main dataset used for this purpose has now reached closed to 100%.
Introduced in 2013 by Choi and Christensen [4], their dataset consists of 4 short
sequences of purely synthetic scenes. The scenes are made of unrealistic, texture-
less backgrounds with a single colored ob ject to track, resulting in noiseless

⋆⋆

mathieu.garon.2@ulaval.ca, denis.laurendeau@gel.ulaval.ca, jflalonde@gel.ulaval.ca

2

Mathieu Garon, Denis Laurendeau and Jean-Fran¸cois Lalonde

(a) Choi-Christensen [4]

(b) Garon-Lalonde [3]

(c) Ours

Fig. 1. Comparison of datasets for evaluating 6-DOF tracking algorithms. Typical
RGB (top) and depth (bottom) frames for (a) the synthetic dataset of Choi and Chris-
tensen [4], (b) the real dataset of Garon and Lalonde [3], and (c) ours. Compared to
the previous work, our dataset contains re
A Geometric Perspective on Structured Light Coding

Mohit Gupta and Nikhil Nakhate

Department of Computer Sciences, University of Wisconsin-Madison, USA

mohitg@cs.wisc.edu, nakhate@wisc.edu

Abstract. We present a mathematical framework for analysis and design of high-
performance structured light (SL) coding schemes. Using this framework, we
design Hamiltonian SL coding, a novel family of SL coding schemes that can
recover 3D shape with high precision, with only a small number (as few as three)
of images. We establish structural similarity between popular discrete (binary)
SL coding methods, and Hamiltonian coding, which is a continuous coding ap-
proach. Based on this similarity, and by leveraging design principles from several
different SL coding families, we propose a general recipe for designing Hamilto-
nian coding patterns with speci ﬁc desirable properties, su ch as patterns with high
spatial frequencies for dealing with global illumination. We perform several ex-
periments to evaluate the proposed approach, and demonstrate that Hamiltonian
coding based SL approaches outperform existing methods in challenging scenar-
ios, including scenes with dark albedos, strong ambient light, and interreﬂections.

1 Introduction

Structured light (SL) is a widely used 3D imaging technique in several applications,
including industrial automation, augmented reality, and robot navigation. Laser scan-
ning [9] based SL systems can recover 3D shape with extreme precision (10 − 100
microns), albeit at the cost of a large acquisition time. Applications such as industrial
inspection require high precision, but with a limited acquisition time budget. While
single-shot SL approaches [33] can recover depths with only a single image, the depths
are spatially smoothed, resulting in loss of detail. Multi-pattern SL approaches project
a series of patterns so that each projector pixel is assigned a unique temporal intensity
code. These codes are used to establish per-pixel correspondence for each camera pixel,
thereby achieving high spatial resolution. However, unfortunately, their depth precision
in demanding scenarios (small time budget, low signal-to-noise ratio) remains low, and
is often the bottleneck in widespread adoption of SL 3D imaging in key applications.
The depth precision of a multi-pattern SL system is determined by the coding
scheme, i.e., the set of patterns that the light source projects. The problem of designing
optimal patterns that achieve high depth precision was ﬁrst
formulated by Horn and
Kiryati [18]. However, ﬁnding a closed form (or even a numeri cal) solution was consid-
ered infeasible. Instead, a family of patterns based on intuitions from digital commu-
nications literature was proposed. These patterns, designed using Hilbert space ﬁlling
curves, belonged to the class of discrete coding schemes (intensities of the patterns
are from a discrete set). While these patterns perform well in high signal-to-noise ratio
(SNR) settings, their pe
A Hybrid Model for Identity Obfuscation by
Face Replacement

Qianru Sun Ayush Tewari Weipeng Xu
Mario Fritz Christian Theobalt Bernt Schiele

Max Planck Institute for Informatics, Saarland Informatics Campus

{qsun, atewari, wxu, mfritz, theobalt, schiele}@mpi-inf.mpg.de

Abstract. As more and more personal photos are shared and tagged
in social media, avoiding privacy risks such as unintended recognition,
becomes increasingly challenging. We propose a new hybrid approach to
obfuscate identities in photos by head replacement. Our approach com-
bines state of the art parametric face synthesis with latest advances in
Generative Adversarial Networks (GAN) for data-driven image synthe-
sis. On the one hand, the parametric part of our method gives us control
over the facial parameters and allows for explicit manipulation of the
identity. On the other hand, the data-driven aspects allow for adding
ﬁne details and overall realism as well as seamless blending into the
scene context. In our experiments we show highly realistic output of our
system that improves over the previous state of the art in obfuscation
rate while preserving a higher similarity to the original image content.

1

Introduction

Visual data is shared publicly at unprecedented scales through social media.
At the same time, however, advanced image retrieval and face recognition algo-
rithms, enabled by deep neural networks and large-scale training datasets, allow
to index and recognize privacy relevant information more reliably than ever. To
address this exploding privacy threat, methods for reliable identity obfuscation
are crucial. Ideally, such a method should not only eﬀectively hide the identity
information but also preserve the realism of the visual data, i.e., make obfuscated
people look realistic.
Existing techniques for identity obfuscation have evolved from simply cov-
ering the face with often unpleasant occluders, such as black boxes or mosaics,
to more advanced methods that produce natural images [1–3]. These methods
either perturb the imagery in an imperceptible way to confuse speciﬁc recogni-
tion algorithms [2, 3], or substantially modify the appearance of the people in
the images, thus making them unrecognizable even for generic recognition al-
gorithms and humans [1]. Among the latter category, recent work [1] leverages
a generative adversarial network (GAN) to inpaint the head region conditioned
on facial landmarks. It achieves state-of-the-art performance in terms of both
recognition rate and image quality. However, due to the lack of controllability of
the image generation process, the results of such a purely data-driven method

2

Sun et al.

inevitably exhibit artifacts by inpainting faces of unﬁtting face pose, expression
or implausible shape. In contrast, parametric face models [4] give us complete
control of facial attributes and have demonstrated compelling results for appli-
cations such as face reconstruction, expression transfer and visual dubbing [4–6].
I
A Joint Sequence Fusion Model for Video
Question Answering and Retrieval

Youngjae Yu

Jongseok Kim Gunhee Kim

Department of Computer Science and Engineering,
Seoul National University, Seoul, Korea

{yj.yu,js.kim}@vision.snu.ac.kr, gunhee@snu.ac.kr
http://vision.snu.ac.kr/projects/jsfusion/

Abstract. We present an approach named JSFusion (Joint Sequence
Fusion) that can measure semantic similarity between any pairs of mul-
timodal sequence data (e.g . a video clip and a language sentence). Our
multimodal matching network consists of two key components. First, the
Joint Semantic Tensor composes a dense pairwise representation of two
sequence data into a 3D tensor. Then, the Convolutional Hierarchical De-
coder computes their similarity score by discovering hidden hierarchical
matches between the two sequence modalities. Both modules leverage
hierarchical attention mechanisms that learn to promote well-matched
representation patterns while prune out misaligned ones in a bottom-up
manner. Although the JSFusion is a universal model to be applicable
to any multimodal sequence data, this work focuses on video-language
tasks including multimodal retrieval and video QA. We evaluate the JS-
Fusion model in three retrieval and VQA tasks in LSMDC, for which our
model achieves the best performance reported so far. We also perform
multiple-choice and movie retrieval tasks for the MSR-VTT dataset, on
which our approach outperforms many state-of-the-art methods.

Keywords: Multimodal Retrieval; Video Question and Answering

1 Introduction

Recently, various video-language tasks have drawn a lot of interests in computer
vision research [1,2,3], including video captioning [4,5,6,7,8,9], video question an-
swering (QA) [10,11], and video retrieval for a natural language query [8,12,13].
To solve such challenging tasks, it is important to learn a hidden join representa-
tion between word and frame sequences, for correctly measuring their semantic
similarity. Video classiﬁcation [14,15,16,17,18] can be a candidate solution, but
tagging only a few labels to a video may be insuﬃcient to fully relate multiple
latent events in the video to a language description. Thanks to recent advance of
deep representation learning, many methods for multimodal semantic embedding
(e.g . [19,20,21]) have been proposed. However, most of existing methods embed
each of visual and language information into a single vector, which is often in-
suﬃcient especially for a video and a natural sentence. With single vectors for

2

Y. Yu , J. Kim and G. Kim

Fig. 1. The intuition of the Joint Sequence Fusion (JSFusion) model. Given a pair of a
video clip and a language query, Joint Semantic Tensor (in purple) encodes a pairwise
joint embedding between the two sequence data, and Convolutional Hierarchical De-
coder (in blue) discovers hierarchical matching relations from JST. Our model is easily
adaptable to many video QA and retrieval tasks.

the two sequence modalities, it is hard to directly
A Minimal Closed-Form Solution for
Multi-Perspective Pose Estimation
using Points and Lines

Pedro Miraldo1 [0000−0002−8551−2448] , Tiago Dias1 [0000−0002−7826−7421] , and
Srikumar Ramalingam2 [0000−0002−2844−4119]

1 Instituto Superior T´ecnico, Lisboa

{pedro.miraldo,tiagojdias}@tecnico.ulisboa.pt

2 School of Computing, University of Utah

srikumar@cs.utah.edu

Abstract. We propose a minimal solution for pose estimation using
both points and lines for a multi-perspective camera. In this paper, we
treat the multi-perspective camera as a collection of rigidly attached
perspective cameras. These type of imaging devices are useful for several
computer vision applications that require a large coverage such as surveil-
lance, self-driving cars, and motion-capture studios. While prior methods
have considered the cases using solely points or lines, the hybrid case in-
volving both points and lines has not been solved for multi-perspective
cameras. We present the solutions for two cases. In the ﬁrst case, we are
given 2D to 3D correspondences for two points and one line. In the later
case, we are given 2D to 3D correspondences for one point and two lines.
We show that the solution for the case of two points and one line can be
formulated as a fourth degree equation. This is interesting because we
can get a closed-form solution and thereby achieve high computational
eﬃciency. The later case involving two lines and one point can be mapped
to an eighth degree equation. We show simulations and real experiments
to demonstrate the advantages and beneﬁts over existing methods.

Keywords: multi-perspective camera, pose estimation, points, lines.

1

Introduction

Pose estimation is a fundamental problem that is used in a wide variety of appli-
cations such as image-based localization (complementary to global positioning
units that are prone to suﬀer from multi-path eﬀects), augmented/virtual reality,
surround-view and birds-eye view synthesis from a car-mounted multi-camera
system, and telemanipulation of robotic arms. The basic idea of pose estimation
is to recover the camera position and orientation with respect to some known 3D
ob ject in the world. We typically associate a world coordinate frame to the 3D
ob ject and pose estimation denotes the computation of the rigid transformation
between world frame and the camera coordinate frame. The problem setting for
pose estimation is generally the same. We are given the correspondences between

2

P. Miraldo, T. Dias, and S. Ramalingam

Table 1: List of minimal pose problems, for perspective, multi-perspective, and
general camera models, using both points and/or lines.

Minimal Problem #Points/Lines #Solutions Closed-Form Papers
Persepective with points
3/0
4
Yes
[3–7]
Perspective with lines
0/3
8
No
[8, 9]
Perspective with points and lines
2/1
4
Yes
[1]
Perspective with points and lines
1/2
8
No
[1]
Multi-Perspective with points
3/0
8
No
[10]
Multi-Perspective with lines
0/3
8
No
[11]
Multi-Perspective with poi
A Modulation Module for Multi-task Learning with
Applications in Image Retrieval

Xiangyun Zhao1 ⋆ Haoxiang Li2 Xiaohui Shen3 Xiaodan Liang4 Ying Wu1

1 EECS Department, Northwestern University
2 AIBee
3 Bytedance AI Lab
4 Carnegie Mellon University

Abstract. Multi-task learning has been widely adopted in many computer vision
tasks to improve overall computation efﬁciency or boost the performance of indi-
vidual tasks, under the assumption that those tasks are correlated and complemen-
tary to each other. However, the relationships between the tasks are complicated
in practice, especially when the number of involved tasks scales up. When two
tasks are of weak relevance, they may compete or even distract each other during
joint training of shared parameters, and as a consequence undermine the learning
of all the tasks. This will raise destructive interference which decreases learning
efﬁciency of shared parameters and lead to low quality loss local optimum w.r.t.
shared parameters. To address the this problem, we propose a general modulation
module, which can be inserted into any convolutional neural network architec-
ture, to encourage the coupling and feature sharing of relevant tasks while disen-
tangling the learning of irrelevant tasks with minor parameters addition. Equipped
with this module, gradient directions from different tasks can be enforced to be
consistent for those shared parameters, which beneﬁts multi-task joint training.
The module is end-to-end learnable without ad-hoc design for speciﬁc tasks, and
can naturally handle many tasks at the same time. We apply our approach on
two retrieval tasks, face retrieval on the CelebA dataset [12] and product retrieval
on the UT-Zappos50K dataset [34, 35], and demonstrate its advantage over other
multi-task learning methods in both accuracy and storage efﬁciency.

1 Introduction

Multi-task learning aims to improve learning efﬁciency and boost the performance of
individual tasks by jointly learning multiple tasks at the same time. With the recent
prevalence of deep learning-based approaches in various computer vision tasks, multi-
task learning is often implemented as parameter sharing in certain intermediate layers
in a uniﬁed convolutional neural network architecture [33, 19]. However, such feature
sharing only works when the tasks are correlated and complementary to each other.
When two tasks are irrelevant, they may provide competing or even contradicting gra-
dient directions during feature learning. For example, learning to predict face attributes

⋆ Part of the work is done when Xiangyun Zhao was an intern at Adobe Research advised by
Haoxiang Li and Xiaohui Shen.

2

X. Zhao, H. Li, X. Shen, X. Liang, Y. Wu

Smile

Open Mouth

Young

Fig. 1. Conﬂicting training signals in multi-task learning: when jointly learning discriminative
features for multiple face attributes, some samples may introduce conﬂicting training signals in
updating shared model parameters, such as “Smile” vs. “Youn
A New Large Scale Dynamic Texture Dataset
with Application to ConvNet Understanding

Isma Hadji and Richard P. Wildes

York University, Toronto, Ontario, Canada

{hadjisma,wildes}@cse.yorku.ca

Abstract. We introduce a new large scale dynamic texture dataset.
With over 10,000 videos, our Dynamic Texture DataBase (DTDB) is two
orders of magnitude larger than any previously available dynamic tex-
ture dataset. DTDB comes with two complementary organizations, one
based on dynamics independent of spatial appearance and one based on
spatial appearance independent of dynamics. The complementary orga-
nizations allow for uniquely insightful experiments regarding the abilities
of ma jor classes of spatiotemporal ConvNet architectures to exploit ap-
pearance vs. dynamic information. We also present a new two-stream
ConvNet that provides an alternative to the standard optical-ﬂow-based
motion stream to broaden the range of dynamic patterns that can be
encompassed. The resulting motion stream is shown to outperform the
traditional optical ﬂow stream by considerable margins. Finally, the util-
ity of DTDB as a pretraining substrate is demonstrated via transfer
learning on a diﬀerent dynamic texture dataset as well as the companion
task of dynamic scene recognition resulting in a new state-of-the-art.

1

Introduction

Visual texture, be it static or dynamic, is an important scene characteristic that
provides vital information for segmentation into coherent regions and identiﬁca-
tion of material properties. Moreover, it can support subsequent operations in-
volving background modeling, change detection and indexing. Correspondingly,
much research has addressed static texture analysis for single images (e.g. [21,
6, 5, 36, 35]). In comparison, research concerned with dynamic texture analysis
from temporal image streams (e.g. video) has been limited (e.g. [15, 26, 38, 27]).
The relative state of dynamic vs. static texture research is unsatisfying be-
cause the former is as prevalent in the real world as the latter and it provides
similar descriptive power. Many commonly encountered patterns are better de-
scribed by global dynamics of the signal rather than individual constituent ele-
ments. For example, it is more perspicuous to describe the global motion of the
leaves on a tree as windblown foliage rather than in terms of individual leaf mo-
tion. Further, given the onslaught of video available via on-line and other sources,
applications of dynamic texture analysis may eclipse those of static texture.
Dynamic texture research is hindered by a number of factors. A ma jor issue
is lack of clarity on what constitutes a dynamic texture. Typically, dynamic tex-
tures are deﬁned as temporal sequences exhibiting certain temporal statistics or

2

I. Hadji and R. P. Wildes

stationary properties in time [30]. In practice, however, the term dynamic tex-
ture is usually used to describe the case of image sequences exhibiting stochastic
dynamics (e.g. turbulent water a
A Segmentation-aware Deep Fusion Network for
Compressed Sensing MRI

Zhiwen Fan⋆ 1 , Liyan Sun⋆ 1 , Xinghao Ding(B)1 , Yue Huang1 , Congbo Cai1 ,
and John Paisley2

1 Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen
University, Fujian, China

dxh@xmu.edu.cn

2 Department of Electrical Engineering, Columbia University, New York, NY, USA

Abstract. Compressed sensing MRI is a classic inverse problem in the
ﬁeld of computational imaging, accelerating the MR imaging by mea-
suring less k-space data. The deep neural network models provide the
stronger representation ability and faster reconstruction compared with
”shallow” optimization-based methods. However, in the existing deep-
based CS-MRI models, the high-level semantic supervision information
from massive segmentation-labels in MRI dataset is overlooked. In this
paper, we proposed a segmentation-aware deep fusion network called
SADFN for compressed sensing MRI. The multilayer feature aggrega-
tion (MLFA) method is introduced here to fuse all the features from
diﬀerent layers in the segmentation network. Then, the aggregated fea-
ture maps containing semantic information are provided to each layer in
the reconstruction network with a feature fusion strategy. This guaran-
tees the reconstruction network is aware of the diﬀerent regions in the
image it reconstructs, simplifying the function mapping. We prove the
utility of the cross-layer and cross-task information fusion strategy by
comparative study. Extensive experiments on brain segmentation bench-
mark MRBrainS and BratS15 validated that the proposed SADFN model
achieves state-of-the-art accuracy in compressed sensing MRI. This pa-
per provides a novel approach to guide the low-level visual task using
the information from mid- or high-level task.

Keywords: Compressed Sensing · Magnetic Resonance Imaging · Med-
ical Image Segmentation · Deep Neural Network

1

Introduction

Magnetic resonance imaging (MRI) is a medical imaging technique used in ra-
diology to produce the anatomical images in human body with the advantages
of low radiation, high resolution in soft tissues and multiple imaging modalities.
However, the ma jor limitation in MRI is the slow imaging speed which causes
motion artifacts [1] when the imaging sub ject moves consciously or unconscious-
ly. The high resolution in k-t space is also diﬃcult to be achieved in dynamic

⋆ The co-ﬁrst authors contributed equally.

2

W. Fan et al.

(a) Full-sampled

(b) Under-sampled

(c) Seg Label

(d) Whole

(e) BG

(f ) GM

(g) WM

(h) CSF

(i) Whole

(j) BG

(k) GM

(l) WM

(m) CSF

Fig. 1. A full-sampled MR image in Figure 1(a), its under-sampled counterpart in
Figure 1(b) and segmentation labels in Figure 1(c). We plot the histograms of under-
sampled MRI (second row) and full-sampled MRI (third row) on training MRI datasets.

MRI because of long imaging period [2]. Thus compressed sensing technique is
introduced to accelerate the MRI by measuring less k-space samples
A Style-Aware Content Loss for
Real-time HD Style Transfer

Artsiom Sanakoyeu⋆ , Dmytro Kotovenko⋆ , Sabine Lang, and Bj¨orn Ommer

Heidelberg Collaboratory for Image Processing,
IWR, Heidelberg University, Germany

firstname.lastname@iwr.uni-heidelberg.de

Abstract. Recently, style transfer has received a lot of attention. While
much of this research has aimed at speeding up processing, the ap-
proaches are still lacking from a principled, art historical standpoint:
a style is more than just a single image or an artist, but previous work
is limited to only a single instance of a style or shows no beneﬁt from
more images. Moreover, previous work has relied on a direct comparison
of art in the domain of RGB images or on CNNs pre-trained on Ima-
geNet, which requires millions of labeled ob ject bounding boxes and can
introduce an extra bias, since it has been assembled without artistic con-
sideration. To circumvent these issues, we propose a style-aware content
loss, which is trained jointly with a deep encoder-decoder network for
real-time, high-resolution stylization of images and videos. We propose a
quantitative measure for evaluating the quality of a stylized image and
also have art historians rank patches from our approach against those
from previous work. These and our qualitative results ranging from small
image patches to megapixel stylistic images and videos show that our ap-
proach better captures the subtle nature in which a style aﬀects content.

Keywords: Style transfer, generative network, deep learning

1

Introduction

A picture may be worth a thousand words, but at least it contains a lot of very
diverse information. This not only comprises what is portrayed, e.g., composition

⋆ Both authors contributed equally to this work.

Fig. 1. Evaluating the ﬁne details preserved by our approach. Can you guess which of
the cut-outs are from Monet’s artworks and which are generated? Solution is on p. 14.

2

A. Sanakoyeu, D. Kotovenko, S. Lang, and B. Ommer

[12]

[12]

[12] on collection [22] on collection Ours on collection

Content

(a)

(b)

(c)

(d)

(e)

Fig. 2. Style transfer using diﬀerent approaches on 1 and a collection of reference
style images. (a) [12] using van Gogh’s ”Road with Cypress and Star” as reference
style image; (b) [12] using van Gogh’s ”Starry night”; (c) [12] using the average Gram
matrix computed across the collection of Vincent van Gogh’s artworks; (d) [22] trained
on the collection of van Gogh’s artworks alternating target style images every SGD
mini-batch; (e) our approach trained on the same collection of van Gogh’s artworks.
Stylizations (a) and (b) depend signiﬁcantly on the particular style image, but using a
collection of the style images (c), (d) does not produce visually plausible results, due
to oversmoothing over the numerous Gram matrices. In contrast, our approach (e) has
learned how van Gogh is altering particular content in a speciﬁc manner (edges around
ob jects also stylized, cf. bell tower)

o
A Systematic DNN Weight Pruning Framework
using Alternating Direction Method of
Multipliers

Tianyun Zhang1∗ [0000−0002−2475−6414] , Shaokai Ye1∗ , Kaiqi Zhang1 , Jian Tang1 ,
Wujie Wen2 , Makan Fardad1 , and Yanzhi Wang3

1 Syracuse University, Syracuse, NY 13244, USA

{tzhan120,sye106,kzhang17,jtang02,makan}@syr.edu

2 Florida International University, Miami, FL 33199, USA
3 Northeastern University, Boston, MA 02115, USA
∗Equal Contribution

Abstract. Weight pruning methods for deep neural networks (DNNs)
have been investigated recently, but prior work in this area is mainly
heuristic, iterative pruning, thereby lacking guarantees on the weight re-
duction ratio and convergence time. To mitigate these limitations, we
present a systematic weight pruning framework of DNNs using the alter-
nating direction method of multipliers (ADMM). We ﬁrst formulate the
weight pruning problem of DNNs as a nonconvex optimization problem
with combinatorial constraints specifying the sparsity requirements, and
then adopt the ADMM framework for systematic weight pruning. By us-
ing ADMM, the original nonconvex optimization problem is decomposed
into two subproblems that are solved iteratively. One of these subprob-
lems can be solved using stochastic gradient descent, the other can be
solved analytically. Besides, our method achieves a fast convergence rate.
The weight pruning results are very promising and consistently outper-
form the prior work. On the LeNet-5 model for the MNIST data set, we
achieve 71.2× weight reduction without accuracy loss. On the AlexNet
model for the ImageNet data set, we achieve 21× weight reduction with-
out accuracy loss. When we focus on the convolutional layer pruning for
computation reductions, we can reduce the total computation by ﬁve
times compared with the prior work (achieving a total of 13.4× weight
reduction in convolutional layers). Our models and codes are released at

https://github.com/KaiqiZhang/admm- pruning.

Keywords: systematic weight pruning · deep neural networks (DNNs)
· alternating direction method of multipliers (ADMM)

1

Introduction

Large-scale deep neural networks or DNNs have made breakthroughs in many
ﬁelds, such as image recognition [16,17,12], speech recognition [13,4], game play-
ing [24], and driver-less cars [23]. Despite the huge success, their large model size

2

T. Zhang et al.

and computational requirements will add signiﬁcant burden to state-of-the-art
computing systems [16,27,10], especially for embedded and IoT systems. As a
result, a number of prior works are dedicated to model compression in order to si-
multaneously reduce the computation and model storage requirements of DNNs,
with minor eﬀect on the overall accuracy. These model compression techniques
include weight pruning [10,11,5,31,25,9,29,21], sparsity regularization [30,19,32],
weight clustering [10,3,26], and low rank approximation [6,7], etc.
A simple but eﬀective weight pruning method has been proposed in [11],
which prunes th
A Trilateral Weighted Sparse Coding Scheme
for Real-World Image Denoising

Jun Xu1 , Lei Zhang1 ⋆ , David Zhang1,2

1The Hong Kong Polytechnic University, Hong Kong SAR, China
2School of Science and Engineering, The Chinese University of Hong Kong
(Shenzhen), Shenzhen, China

{csjunxu, cslzhang, csdzhang}@comp.polyu.edu.hk

Abstract. Most of existing image denoising methods assume the cor-
rupted noise to be additive white Gaussian noise (AWGN). However,
the realistic noise in real-world noisy images is much more complex than
AWGN, and is hard to be modeled by simple analytical distributions. As
a result, many state-of-the-art denoising methods in literature become
much less eﬀective when applied to real-world noisy images captured by
CCD or CMOS cameras. In this paper, we develop a trilateral weighted
sparse coding (TWSC) scheme for robust real-world image denoising.
Speciﬁcally, we introduce three weight matrices into the data and regu-
larization terms of the sparse coding framework to characterize the statis-
tics of realistic noise and image priors. TWSC can be reformulated as a
linear equality-constrained problem and can be solved by the alternating
direction method of multipliers. The existence and uniqueness of the solu-
tion and convergence of the proposed algorithm are analyzed. Extensive
experiments demonstrate that the proposed TWSC scheme outperforms
state-of-the-art denoising methods on removing realistic noise.

Keywords: real-world image denoising, sparse coding

1

Introduction

Noise will be inevitably introduced in imaging systems and may severely damage
the quality of acquired images. Removing noise from the acquired image is an
essential step in photography and various computer vision tasks such as segmen-
tation [1], HDR imaging [2], and recognition [3], etc. Image denoising aims to
recover the clean image x from its noisy observation y = x + n, where n is the
corrupted noise. This problem has been extensively studied in literature, and
numerous statistical image modeling and learning methods have been proposed
in the past decades [4–26].
Most of the existing methods [4–20] focus on additive white Gaussian noise
(AWGN), and they can be categorized into dictionary learning based meth-
ods [4, 5], nonlocal self-similarity based methods [6–14], sparsity based meth-
ods [4, 5, 7–11], low-rankness based methods [12, 13], generative learning based

⋆ This pro ject is supported by Hong Kong RGC GRF pro ject (PolyU 152124/15E).

2

J. Xu, L. Zhang, and D. Zhang

Fig. 1: Comparison of noisy image patches in real-world noisy image (left) and syn-
thetic noisy image with additive white Gaussian noise (right).

(a)

(b)

(c)

(d)

(e)

(f )

Fig. 2: An example of realistic noise. (a) A real-world noisy image captured by a Nikon

Trilateral Weighted Sparse Coding Scheme for Real-World Image Denoising

3

performance and generate artifacts [5]. The methods [22, 23] perform image de-
noising by concatenating the patches of RGB channels int
A Uniﬁed Framework for Multi-View Multi-Class
Object Pose Estimation

Chi Li[0000−0002−5957−5680] , Jin Bai[0000−0002−0653−0542] , and Gregory D.
Hager[0000−0002−6662−9763]

Department of Computer Science, Johns Hopkins University

{chi li,jbai12,hager}@jhu.edu

Abstract. One core challenge in object pose estimation is to ensure accurate
and robust performance for large numbers of diverse foreground objects amidst
complex background clutter. In this work, we present a scalable framework for
accurately inferring six Degree-of-Freedom (6-DoF) pose for a large number of
object classes from single or multiple views. To learn discriminative pose features,
we integrate three new capabilities into a deep Convolutional Neural Network
(CNN): an inference scheme that combines both classiﬁcation and pose regression
based on a uniform tessellation of the Special Euclidean group in three dimensions
(SE(3)), the fusion of class priors into the training process via a tiled class map,
and an additional regularization using deep supervision with an object mask.
Further, an efﬁcient multi-view framework is formulated to address single-view
ambiguity. We show that this framework consistently improves the performance of
the single-view network. We evaluate our method on three large-scale benchmarks:
YCB-Video, JHUScene-50 and ObjectNet-3D. Our approach achieves competitive
or superior performance over the current state-of-the-art methods.

Keywords: Object pose estimation, multi-view recognition, deep learning

1

Introduction

Estimating 6-DoF object pose from images is a core problem for a wide range of appli-
cations including robotic manipulation, navigation, augmented reality and autonomous
driving. While numerous methods appear in the literature [12, 41, 1, 39, 2, 6, 17, 26],
scalability (to large numbers of objects) and accuracy continue to be critical issues that
limit existing methods. Recent work has attempted to leverage the power of deep CNNs
to surmount these limitations [35, 25, 42, 27, 38, 16, 44, 30]. One naive approach is to
train a network to estimate the pose of each object of interest (Fig. 1 (a)). More recent
approaches follow the principle of “object per output branch” (Fig. 1 (b)) whereby each
object class1 is associated with an output stream connected to a shared feature basis [44,
16, 35, 25, 30]. In both cases, the size of the network increases with the number of objects,
which implies that large amounts of data are needed for each class to avoid overﬁtting.
In this work, we present a multi-class pose estimation architecture (Fig. 1 (c)) which
receives object images and class labels provided by a detection system and which has

1 An object class may refer to either an object instance or an object category.

2

C. Li, J. Bai and G. Hager

Fig. 1: Illustration of different learning architectures for single-view object pose estimation: (a)
each object is trained on an independent network; (b) each object is associated with one output
branch of
A Zero-Shot Framework for Sketch Based Image
Retrieval.

Sasi Kiran Yelamarthi∗ , Shiva Krishna Reddy∗ , Ashish Mishra, and Anurag
Mittal

Indian Institute of Technology Madras, India

{sasikiran1996, shivakrishnam912}@gmail.com, {mishra,
amittal}@cse.iitm.ac.in

Abstract. Sketch-based image retrieval (SBIR) is the task of retriev-
ing images from a natural image database that correspond to a given
hand-drawn sketch. Ideally, an SBIR model should learn to associate
components in the sketch (say, feet, tail, etc.) with the corresponding
components in the image having similar shape characteristics. However,
current evaluation methods simply focus only on coarse-grained evalua-
tion where the focus is on retrieving images which belong to the same
class as the sketch but not necessarily having the same shape charac-
teristics as in the sketch. As a result, existing methods simply learn to
associate sketches with classes seen during training and hence fail to gen-
eralize to unseen classes. In this paper, we propose a new benchmark for
zero-shot SBIR where the model is evaluated on novel classes that are
not seen during training. We show through extensive experiments that
existing models for SBIR that are trained in a discriminative setting
learn only class speciﬁc mappings and fail to generalize to the proposed
zero-shot setting. To circumvent this, we propose a generative approach
for the SBIR task by proposing deep conditional generative models that
take the sketch as an input and ﬁll the missing information stochastically.
Experiments on this new benchmark created from the ”Sketchy” dataset,
which is a large-scale database of sketch-photo pairs demonstrate that
the performance of these generative models is signiﬁcantly better than
several state-of-the-art approaches in the proposed zero-shot framework
of the coarse-grained SBIR task.

Keywords: Image Retrieval, Zero-Shot Learning

1

Introduction

The rise in the number of internet users coupled with increased storage capacity,
better internet connectivity and higher bandwidths has resulted in an exponen-
tial growth in multimedia content on the Web. In particular, image content has
become ubiquitous and plays an important role in engaging users on social me-
dia as well as customers on various e-commerce sites. With this growth in image
content, the information needs and search patterns of users have also evolved.
Speciﬁcally, it is now common for users to search for images (instead of docu-
ments) either by providing a textual description of the image or by providing

*: Equal Contribution

2

Sasi Kiran Yelamarthi et al

another image which is similar to the desired image. The former is known as
text based image retrieval and the latter as content based image retrieval [18].
The motivation for content based image retrieval can be easily understood by
taking an example from online fashion. Here, it is often hard to provide a textual
description of the desired product but easier to provide a visu
A-Contrario Horizon-First Vanishing Point
Detection Using Second-Order Grouping Laws

Gilles Simon, Antoine Fond and Marie-Odile Berger

Loria, CNRS, Inria Nancy Grand Est, Universit´e de Lorraine

{gsimon,afond,berger}@loria.fr

Abstract. We show that, in images of man-made environments, the
horizon line can usually be hypothesized based on a-contrario detections
of second-order grouping events. This allows constraining the extraction
of the horizontal vanishing points on that line, thus reducing false detec-
tions. Experiments made on three datasets show that our method, not
only achieves state-of-the-art performance w.r.t. horizon line detection
on two datasets, but also yields much less spurious vanishing points than
the previous top-ranked methods.

Keywords: Horizon line · Vanishing point detection · A-contrario model
· Perceptual grouping · Gestalt theory · Man-made environments

1

Introduction

Accurate detection of vanishing points (VPs) is a prerequisite for many computer
vision problems such as camera self-calibration[16], single view structure recovery
[7], video compass [6], robot navigation [10] and augmented reality [4], among
many others. Under the pinhole camera model, a VP is an abstract point on
the image plane where 2-D pro jections of a set of parallel line segments in 3-D
space appear to converge. In the Gestalt theory of perception [3], such a spatial
arrangement of perceived ob jects is called a grouping law, or a gestalt. More
speciﬁcally, as a 2-D line segment (LS) is in itself a gestalt (grouping of aligned
points), a VP is qualiﬁed as a second-order gestalt [3].
In this paper, we are interested in VP detection from uncalibrated monocular
images. As any two parallel lines intersect in a VP, LSs grouping is a diﬃcult
problem that often yields a large number of spurious VPs. However, many tasks
in computer vision, including the examples mentioned above, only require that
the vertical (so-called zenith) VP and two or more horizontal VPs (hVPs) are
detected. In that case, a lot of spurious VPs may be avoided by ﬁrst detecting
the zenith and the horizon line (HL), and then constraining the hVPs on the HL.
The zenith is generally easy to detect, as many lines converge towards that point
in man-made environments. However, until recently, the HL was detected as an
alignment of VPs, in other words, a third-order gestalt. This led to a “chicken-
and-egg” situation, that motivated e.g. the authors of [14], to minimize an overall
energy across the VPs and the HL, at the expense of a high computational cost.

2

G. Simon, A. Fond and M.-O. Berger

Fig. 1. The horizon line can be detected as a meaningful alignment of image line
segments orthogonal to the zenith line.

Following [12], we show that, as soon as the HL is inside the image boundaries,
this line can usually be detected as an alignment of oriented LSs, that is, a
second-order gestalt (at the same perceptive level as the VPs). This comes from
a simple observation, that any 
Accurate Scene Text Detection through Border
Semantics Awareness and Bootstrapping

Chuhui Xue[0000−0002−3562−3094] , Shijian Lu[0000−0002−6766−2506] , and Fangneng
Zhan[0000−0003−1502−6847]

School of Computer Science and Engineering, Nanyang Technological University

xuec0003@e.ntu.edu.sg, {shijian.lu,fnzhan}@ntu.edu.sg

Abstract. This paper presents a scene text detection technique that ex-
ploits bootstrapping and text border semantics for accurate localization
of texts in scenes. A novel bootstrapping technique is designed which
samples multiple ‘subsections’ of a word or text line and accordingly re-
lieves the constraint of limited training data eﬀectively. At the same time,
the repeated sampling of text ‘subsections’ improves the consistency of
the predicted text feature maps which is critical in predicting a single
complete instead of multiple broken boxes for long words or text lines. In
addition, a semantics-aware text border detection technique is designed
which produces four types of text border segments for each scene text.
With semantics-aware text borders, scene texts can be localized more
accurately by regressing text pixels around the ends of words or text
lines instead of all text pixels which often leads to inaccurate localiza-
tion while dealing with long words or text lines. Extensive experiments
demonstrate the eﬀectiveness of the proposed techniques, and superior
performance is obtained over several public datasets, e. g. 80.1 f-score
for the MSRA-TD500, 67.1 f-score for the ICDAR2017-RCTW, etc.

Keywords: Scene text detection, data augmentation, semantics-aware
detection, deep network models

1

Introduction

Scene text detection and recognition has attracted increasing interests in recent
years in both computer vision and deep learning research communities due to
its wide range of applications in multilingual translation, autonomous driving,
etc. As a prerequisite of scene text recognition, detecting text in scenes plays an
essential role in the whole chain of scene text understanding processes. Though
studied for years, accurate and robust detection of texts in scenes is still a very
open research challenge as witnessed by increasing benchmarking competitions
in recent years such as ICDAR2015-Incidental [19], ICDAR2017-MLT [30], etc.
With the fast development of convolutional neural networks (CNN) in rep-
resentation learning and ob ject detection, two CNN-based scene text detection
approaches have been investigated in recent years which treat words or text
lines as generic ob jects and adapt generic ob ject detection techniques for the

2

Chuhui Xue, Shijian Lu, Fangneng Zhan

Fig. 1: Overview of proposed scene text detection technique: For each training
image, a set of augmented images and semantics-aware text borders are extracted
and fed to a multi-channel fully convolutional network to train a scene text
detector (as shown above the dotted line). Given a test image, the scene text
detector predicts a text feature 
Acquisition of Localization Conﬁdence for
Accurate Ob ject Detection

Borui Jiang∗ 1,3 , Ruixuan Luo∗ 1,3 , Jiayuan Mao∗ 2,4 ,
Tete Xiao1,3 , and Yuning Jiang4

1 School of Electronics Engineering and Computer Science, Peking University
2 ITCS, Institute for Interdisciplinary Information Sciences, Tsinghua University
3 Megvii Inc. (Face++)
4 Toutiao AI Lab

{jbr, luoruixuan97, jasonhsiao97}@pku.edu.cn,
mjy14@mails.tsinghua.edu.cn, jiangyuning@bytedance.com

Abstract. Modern CNN-based ob ject detectors rely on bounding box
regression and non-maximum suppression to localize ob jects. While the
probabilities for class labels naturally reﬂect classiﬁcation conﬁdence,
localization conﬁdence is absent. This makes properly localized bounding
boxes degenerate during iterative regression or even suppressed during
NMS. In the paper we propose IoU-Net learning to predict the IoU
between each detected bounding box and the matched ground-truth.
The network acquires this conﬁdence of localization, which improves
the NMS procedure by preserving accurately localized bounding boxes.
Furthermore, an optimization-based bounding box reﬁnement method
is proposed, where the predicted IoU is formulated as the ob jective.
Extensive experiments on the MS-COCO dataset show the eﬀectiveness
of IoU-Net, as well as its compatibility with and adaptivity to several
state-of-the-art ob ject detectors.

Keywords: ob ject localization, bounding box regression, non-maximum
suppression

1

Introduction

Ob ject detection serves as a prerequisite for a broad set of downstream vision
applications, such as instance segmentation [19, 20], human skeleton [27], face
recognition [26] and high-level ob ject-based reasoning [30]. Ob ject detection
combines both ob ject classiﬁcation and ob ject localization. A ma jority of modern
ob ject detectors are based on two-stage frameworks [9, 8, 22, 16, 10], in which
ob ject detection is formulated as a multi-task learning problem: 1) distinguish
foreground ob ject proposals from background and assign them with proper class
labels; 2) regress a set of coeﬃcients which localize the ob ject by maximizing
intersection-over-union (IoU) or other metrics between detection results and the
ground-truth. Finally, redundant bounding boxes (duplicated detections on the
same ob ject) are removed by a non-maximum suppression (NMS) procedure.

∗ indicates equal contribution.

2

B. Jiang, R. Luo, J. Mao, T. Xiao, and Y. Jiang

(a) Demonstrative cases of the misalignment between classiﬁcation conﬁdence and localiza-
tion accuracy. The yellow bounding boxes denote the ground-truth, while the red and green
bounding boxes are both detection results yielded by FPN [16]. Localization conﬁdence is
computed by the proposed IoU-Net. Using classiﬁcation conﬁdence as the ranking metric
will cause accurately localized bounding boxes (in green) being incorrectly eliminated in
the traditional NMS procedure. Quantitative analysis is provided in Section 2.1

Iterations

IOU: 0.
Action Anticipation with RBF Kernelized
Feature Mapping RNN

Yuge Shi[0000−0003−1905−9320] , Basura Fernando[0000−0002−6920−9916] , and Richard
Hartley[0000−0002−5005−0191]

The Australian National University, Australia

Abstract. We introduce a novel Recurrent Neural Network-based algorithm for
future video feature generation and action anticipation called feature mapping RNN .
Our novel RNN architecture builds upon three effective principles of machine
learning, namely parameter sharing, Radial Basis Function kernels and adversar-
ial training. Using only some of the earliest frames of a video, the feature map-
ping RNN is able to generate future features with a fraction of the parameters
needed in traditional RNN. By feeding these future features into a simple multi-
layer perceptron facilitated with an RBF kernel layer, we are able to accurately
predict the action in the video.
In our experiments, we obtain 18% improvement on JHMDB-21 dataset, 6% on
UCF101-24 and 13% improvement on UT-Interaction datasets over prior state-
of-the-art for action anticipation.

Keywords: Human action prediction, novel Recurrent Neural Network, Radial
Basis Function kernel, Adversarial training

1

Introduction

Action anticipation (sometimes referred to as action prediction) is gaining a lot of
attention due to its many real world applications such as human-computer interac-
tion [2,33,30], sports analysis [3,4,56] and pedestrian movement prediction [9,21,18,5,46]
especially in the autonomous driving scenarios.
In contrast to most widely studied human action recognition methods, in action
anticipation, we aim to recognize human action as early as possible [39,23,28,42,49].
This is a challenging task due to the complex nature of video data. Although a video
containing a human action consists of a large number of frames, many of them are not
representative of the action being performed; large amount of visual data also tend to
contain entangled information about variations in camera position, background, relative
movements and occlusions. This results in cluttered temporal information and makes
recognition of the human action a lot harder. The issue becomes even more signiﬁcant
for action anticipation methods, as the algorithm has to make a decision using only a
fraction of the video at the very start. Therefore, ﬁnding a good video representation that
extracts temporal information relevant to human action is crucial for the anticipation
model.
To over come some of these issues, we resort to use deep convolutional neural net-
works (CNNs) and take the deep feature on the penultimate layer of CNN as video

2

Y. Shi et al.

Fig. 1: Overview of Proposed feature mapping RNN : Given a frame extracted from
video data, the algorithm ﬁrst passes the RGB image I (t) through a deep CNN to ac-
quire high level features of the image xt . The vector is then split into smaller segments
xi
t of equal length. Each scalar element in the segmented vector is used as input to a sin
Action Search: Spotting Actions in Videos and
Its Application to Temporal Action Localization

Humam Alwassel, Fabian Caba Heilbron, and Bernard Ghanem

King Abdullah University of Science and Technology (KAUST), Saudi Arabia
http://www.humamalwassel.com/publication/action- search/

{humam.alwassel,fabian.caba,bernard.ghanem}@kaust.edu.sa

Abstract. State-of-the-art temporal action detectors ineﬃciently search
the entire video for speciﬁc actions. Despite the encouraging progress
these methods achieve, it is crucial to design automated approaches that
only explore parts of the video which are the most relevant to the actions
being searched for. To address this need, we propose the new problem
of action spotting in video, which we deﬁne as ﬁnding a speciﬁc action
in a video while observing a small portion of that video. Inspired by the
observation that humans are extremely eﬃcient and accurate in spotting
and ﬁnding action instances in video, we propose Action Search, a novel
Recurrent Neural Network approach that mimics the way humans spot
actions. Moreover, to address the absence of data recording the behavior
of human annotators, we put forward the Human Searches dataset, which
compiles the search sequences employed by human annotators spotting
actions in the AVA and THUMOS14 datasets. We consider temporal
action localization as an application of the action spotting problem. Ex-
periments on the THUMOS14 dataset reveal that our model is not only
able to explore the video eﬃciently (observing on average 17.3% of the
video) but it also accurately ﬁnds human activities with 30.8% mAP.

Keywords: Video understanding · Action localization · Action spotting

1

Introduction

Similar to many video-related applications, such as video ob ject detection and
video surveillance, temporal action localization requires an eﬃcient search for
diﬀerent visual targets in videos. With the recent exponential growth in the
number videos online (e.g. over 400 video hours are uploaded to YouTube every
minute), it is crucial today to develop methods that can simultaneously search
this large volume of videos eﬃciently and spot actions accurately. Thus, we
propose the new problem of action spotting in video, which we deﬁne as ﬁnding
a speciﬁc action in a video sequence while observing a small portion of that video.
Since the computational cost is directly impacted by the number of observations
made in a video, this spotting problem brings search eﬃciency to the forefront.

The ﬁrst two authors contributed equally to this work. Authors ordering was de-
termined by three coin ﬂips.

2

H. Alwassel et al.

Fig. 1: Left: A partial search sequence a human performed to ﬁnd the start of a
Long Jump action (the shaded green area). Notably, humans are eﬃcient in spot-
ting actions without observing a large portion of the video. Right: An eﬃciency
comparison between humans, Action Search, and other detection/proposal meth-
ods on THUMOS14 [24]. Our model is 5.8x more eﬃcient than 
ActiveStereoNet: End-to-End Self-Supervised
Learning for Active Stereo Systems

Yinda Zhang1,2 , Sameh Khamis1 , Christoph Rhemann1 , Julien Valentin1 ,
Adarsh Kowdle1 , Vladimir Tankovich1 , Michael Schoenberg1 ,
Shahram Izadi1 , Thomas Funkhouser1,2 , Sean Fanello1

1Google Inc., 2Princeton University

Abstract. In this paper we present ActiveStereoNet, the ﬁrst deep
learning solution for active stereo systems. Due to the lack of ground
truth, our method is fully self-supervised, yet it produces precise depth
with a subpixel precision of 1/30th of a pixel; it does not suﬀer from the
common over-smoothing issues; it preserves the edges; and it explicitly
handles occlusions. We introduce a novel reconstruction loss that is more
robust to noise and texture-less patches, and is invariant to illumination
changes. The proposed loss is optimized using a window-based cost ag-
gregation with an adaptive support weight scheme. This cost aggregation
is edge-preserving and smooths the loss function, which is key to allow
the network to reach compelling results. Finally we show how the task
of predicting invalid regions, such as occlusions, can be trained end-to-
end without ground-truth. This component is crucial to reduce blur and
particularly improves predictions along depth discontinuities. Extensive
quantitatively and qualitatively evaluations on real and synthetic data
demonstrate state of the art results in many challenging scenes.

Keywords: Active Stereo, Depth Estimation, Self-supervised Learning,
Neural Network, Occlusion Handling, Deep Learning

1

Introduction

Depth sensors are revolutionizing computer vision by providing additional 3D
information for many hard problems, such as non-rigid reconstruction [9, 8], ac-
tion recognition [10, 15] and parametric tracking [47, 48] . Although there are
many types of depth sensor technologies, they all have signiﬁcant limitations.
Time of ﬂight systems suﬀer from motion artifacts and multi-path interference
[5, 4, 39]. Structured light is vulnerable to ambient illumination and multi-device
interference [14, 12]. Passive stereo struggles in texture-less regions, where ex-
pensive global optimization techniques are required - especially in traditional
non-learning based methods.
Active stereo oﬀers a potential solution: an infrared stereo camera pair is
used, with a pseudorandom pattern pro jectively texturing the scene via a pat-
terned IR light source. (Fig. 1). With a proper selection of sensing wavelength,
the camera pair captures a combination of active illumination and passive light,

2

Y. Zhang et al.

Fig. 1. ActiveStereoNet (ASN) produces smooth, detailed, quantization free results
using a pair of rectiﬁed IR images acquired with an Intel Realsense D435 camera. In
particular, notice how the jacket is almost indiscernible using the sensor output, and
in contrast, how it is clearly observable in our results.

improving quality above that of structured light while providing a robust solution
in bot
Actor-Centric Relation Network

Chen Sun, Abhinav Shrivastava, Carl Vondrick,
Kevin Murphy, Rahul Sukthankar, and Cordelia Schmid

Google Research

Abstract. Current state-of-the-art approaches for spatio-temporal ac-
tion localization rely on detections at the frame level and model tempo-
ral context with 3D ConvNets. Here, we go one step further and model
spatio-temporal relations to capture the interactions between human ac-
tors, relevant ob jects and scene elements essential to diﬀerentiate similar
human actions. Our approach is weakly supervised and mines the rel-
evant elements automatically with an actor-centric relational network
(ACRN). ACRN computes and accumulates pair-wise relation informa-
tion from actor and global scene features, and generates relation features
for action classiﬁcation. It is implemented as neural networks and can be
trained jointly with an existing action detection system. We show that
ACRN outperforms alternative approaches which capture relation infor-
mation, and that the proposed framework improves upon the state-of-
the-art performance on JHMDB and AVA. A visualization of the learned
relation features conﬁrms that our approach is able to attend to the rel-
evant relations for each action.

Keywords: spatio-temporal action detection, relation networks

1

Introduction

Robust human action understanding will have a large impact in applications
across robotics, security, and health. However, despite signiﬁcant progress in
visual recognition for ob jects and scenes [16, 27, 41, 64], performance on action
recognition remains relatively low. Now that we have large, diverse, and real-
istic datasets such as AVA [13], SLAC [62], and Charades [49], why has action
recognition performance not caught up?
Models for spatio-temporal action localization from the last few years have
been mainly based on architectures for recognizing ob jects [12, 37, 58], building
on the success of R-CNN style architectures [9, 10, 40]. However, unlike ob jects
which can be identiﬁed solely by their visual appearance, in many cases actions
can not be identiﬁed by the visual appearance of actors alone. Rather, action
recognition often requires reasoning about the actor’s relationship with ob jects
and other actors, both spatially and temporally. To make this point, Figure 1
shows two actors performing diﬀerent actions. Even for humans, by just looking
at the cropped boxes, it is diﬃcult to tell what actions are being performed. It is
from the actors’ interactions with a ball in the scene that we can tell that these

2

C. Sun et al.

Fig. 1: Action detection is challenging even for humans without relation reasoning
from the context. Only by extracting the relationship between the actor and the
ob ject (ball), and understanding how this relationship evolves over time, can one
tell that the ﬁrst action is catching a ball, while the second action is shooting
a ball. The last column visualizes the relational heat maps generated by our
algorithm.
Adaptive Afﬁnity Fields for Semantic Segmentation

Tsung-Wei Ke*[0000−0003−1315−3834] , Jyh-Jing Hwang*[0000−0001−7892−0657]
, Ziwei Liu[0000−0002−4220−5958] , and Stella X. Yu[0000−0002−3507−5761]

{twke,jyh,zwliu,stellayu}@berkeley.edu

UC Berkeley / ICSI

Abstract. Semantic segmentation has made much progress with increasingly
powerful pixel-wise classiﬁers and incorporating structural priors via Conditional
Random Fields (CRF) or Generative Adversarial Networks (GAN). We propose
a simpler alternative that learns to verify the spatial structure of segmentation
during training only. Unlike existing approaches that enforce semantic labels on
individual pixels and match labels between neighbouring pixels, we propose the
concept of Adaptive Afﬁnity Fields (AAF) to capture and match the semantic
relations between neighbouring pixels in the label space. We use adversarial
learning to select the optimal afﬁnity ﬁeld size for each semantic category. It
is formulated as a minimax problem, optimizing our segmentation neural net-
work in a best worst-case learning scenario. AAF is versatile for representing
structures as a collection of pixel-centric relations, easier to train than GAN and
more efﬁcient than CRF without run-time inference. Our extensive evaluations on
PASCAL VOC 2012, Cityscapes, and GTA5 datasets demonstrate its above-par
segmentation performance and robust generalization across domains.

Keywords: semantic segmentation; afﬁnity ﬁeld; adversarial learning

1

Introduction

Semantic segmentation of an image refers to the challenging task of assigning each pixel
a categorical label, e.g., motorcycle or person. Segmentation performance is often mea-
sured in a pixel-wise fashion, in terms of mean Intersection over Union (mIoU) across
categories between the ground-truth (Fig. 1b) and the predicted label map (Fig. 1c).

Much progress has been made on segmentation with convolutional neural nets (CNN),
mostly due to increasingly powerful pixel-wise classiﬁers, e.g., VGG-16 [32, 21] and
ResNet [14, 33], with the convolutional ﬁlters optimized by minimizing the average
pixel-wise classiﬁcation error over the image.

Even with big training data and with deeper and more complex network architec-
tures, pixel-wise classiﬁcation based approaches fundamentally lack the spatial dis-
crimination power when foreground pixels and background pixels are close or mixed
together: Segmentation is poor when the visual evidence for the foreground is weak,

* Equal contributors. Jyh-Jing is a visiting student from the University of Pennsylvania.

2

T.-W. Ke, J.-J. Hwang, Z. Liu, and S. X. Yu

Fig. 1. We propose new pairwise pixel loss functions that capture the spatial structure of segmen-
tation. Given an image (a), the task is to predict the ground-truth labeling (b). When a deep neural
net is trained with conventional softmax cross-entropy loss on individual pixels, the predicted seg-
mentation (c) is often based on visual appearance and oblivious of the spa
Adding Attentiveness to the Neurons in
Recurrent Neural Networks

Pengfei Zhang1 [0000−0002−8303−8930] , Jianru Xue1 ⋆ [0000−0002−4994−9343] , Cuiling
Lan2 ⋆ [0000−0001−9145−9957] , Wenjun Zeng2 [0000−0003−2531−3137] , Zhanning
Gao1 [0000−0003−2031−2805] , and Nanning Zheng1 [0000−0003−1608−8257]

1 Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong University
2 Microsoft Reserach Asia

zpengfei@stu.xjtu.edu.cn, {culan,wezeng}@microsoft.com,
{jrxue,nnzheng}@mail.xjtu.edu.cn, zhanninggao@gmail.com

Abstract. Recurrent neural networks (RNNs) are capable of model-
ing the temporal dynamics of complex sequential information. However,
the structures of existing RNN neurons mainly focus on controlling the
contributions of current and historical information but do not explore
the diﬀerent importance levels of diﬀerent elements in an input vector
of a time slot. We propose adding a simple yet eﬀective Element-wise-
Attention Gate (EleAttG) to an RNN block (e.g., all RNN neurons in a
network layer) that empowers the RNN neurons to have the attentive-
ness capability. For an RNN block, an EleAttG is added to adaptively
modulate the input by assigning diﬀerent levels of importance, i.e., at-
tention, to each element/dimension of the input. We refer to an RNN
block equipped with an EleAttG as an EleAtt-RNN block. Speciﬁcal-
ly, the modulation of the input is content adaptive and is performed
at ﬁne granularity, being element-wise rather than input-wise. The pro-
posed EleAttG, as an additional fundamental unit, is general and can be
applied to any RNN structures, e.g., standard RNN, Long Short-Term
Memory (LSTM), or Gated Recurrent Unit (GRU). We demonstrate the
eﬀectiveness of the proposed EleAtt-RNN by applying it to the action
recognition tasks on both 3D human skeleton data and RGB videos.
Experiments show that adding attentiveness through EleAttGs to RNN
blocks signiﬁcantly boosts the power of RNNs.

Keywords: Element-wise-Attention Gate (EleAttG) · recurrent neural
networks · action recognition · skeleton · RGB video

1

Introduction

In recent years, recurrent neural networks [25], such as standard RNN (sRNN),
its variant Long Short-Term Memory (LSTM) [15], and Gated Recurrent Unit
(GRU) [3], have been adopted to address many challenging problems with se-
quential time-series data, such as action recognition [9], machine translation [2],

⋆ Corresponding author.

2

Zhang et al.

(a)

(b)

Fig. 1: Illustration of Element-wise-Attention Gate (EleAttG) (marked in red)
for (a) a generic RNN block, where the RNN structure could be the standard
RNN, LSTM, or GRU and (b) a GRU block which consists of a group of (e.g., N )
GRU neurons. In the diagram, each line carries a vector. The brown circles denote
element-wise operation, e.g., element-wise vector product or vector addition. The
yellow boxes denote the units of the original GRU with the output dimension of
N . The red box denotes the EleAttG with an output dimension of D , which is
the sam
Adversarial Open-World Person
Re-Identiication

Xiang Li1 , Ancong Wu1 , and Wei-Shi Zheng1,2,3 ⋆ [0000−0001−8327−0003]

1 Sun Yat-sen University

{lixiang47,wuancong}@mail2.sysu.edu.cn
wszheng@ieee.org

2 Inception Institute of Artiicial Intelligence, United Arab Emirates
3 Key Laboratory of Machine Intelligence and Advanced Computing, MOE

Abstract. In a typical real-world application of re-id, a watch-list (gallery
set) of a handful of target people (e.g. suspects) to track around a large
volume of non-target people are demanded across camera views, and this
is called the open-world person re-id. Diferent from conventional (closed-
world) person re-id, a large portion of probe samples are not from target
people in the open-world setting. And, it always happens that a non-
target person would look similar to a target one and therefore would
seriously challenge a re-id system. In this work, we introduce a deep
open-world group-based person re-id model based on adversarial learn-
ing to alleviate the attack problem caused by similar non-target people.
The main idea is learning to attack feature extractor on the target peo-
ple by using GAN to generate very target-like images (imposters), and in
the meantime the model will make the feature extractor learn to tolerate
the attack by discriminative learning so as to realize group-based verii-
cation. The framework we proposed is called the adversarial open-world
person re-identiication, and this is realized by our Adversarial PersonNet
(APN) that jointly learns a generator, a person discriminator, a target
discriminator and a feature extractor, where the feature extractor and
target discriminator share the same weights so as to makes the feature
extractor learn to tolerate the attack by imposters for better group-based
veriication. While open-world person re-id is challenging, we show for
the irst time that the adversarial-based approach helps stabilize person
re-id system under imposter attack more efectively.

1 Introduction

Person re-identiication (re-id), which is to match a pedestrian across disjoint
camera views in diverse scenes, is practical and useful for many ields, such as
public security applications and has gained increasing interests in recent years
[3, 4, 6, 10, 11, 22, 34, 36, 37, 40, 42, 45]. Rather than re-identifying every person in
a multiple camera network, a typical real-world application is to re-identify or
track only a handful of target people on a watch list (gallery set), which is called

⋆ corresponding author

2

X. Li, A. Wu and W.-S. Zheng

Adversarial Open-World Person Re-Identification

Real Person 
Images

Feature 
Extractor

A

t
t

a
c
k

Person 
Discriminator

Weight 
Sharing

Target 
Discriminator

Learn to Discriminate between 
Target Images and Imposters

Generator

Adversarial Training

D

e

f

e
n

s
e

Source

Non-
Human-like

Target

Non-Target

D

i

s
c

r
i

m

i

n
a

t

e

D

i

s
c

r
i

m

i

n
a

t

e

Fig. 1: Overview of adversarial open-world pe
ADVIO: An Authentic Dataset for
Visual-Inertial Odometry

Santiago Cortés1 [0000−0001−7886−7841] , Arno Solin1 [0000−0002−0958−7886] , Esa
Rahtu2 [0000−0001−8767−0864] , and Juho Kannala1 [0000−0001−5088−4041]

1 Department of Computer Science, Aalto University, Espoo, Finland

{santiago.cortesreina,arno.solin,juho.kannala}@aalto.fi

2 Tampere University of Technology, Tampere, Finland

esa.rahtu@tut.fi

Abstract. The lack of realistic and open benchmarking datasets for
pedestrian visual-inertial odometry has made it hard to pinpoint dif-
ferences in published methods. Existing datasets either lack a full six
degree-of-freedom ground-truth or are limited to small spaces with op-
tical tracking systems. We take advantage of advances in pure inertial
navigation, and develop a set of versatile and challenging real-world com-
puter vision benchmark sets for visual-inertial odometry. For this pur-
pose, we have built a test rig equipped with an iPhone, a Google Pixel
Android phone, and a Google Tango device. We provide a wide range of
raw sensor data that is accessible on almost any modern-day smartphone
together with a high-quality ground-truth track. We also compare result-
ing visual-inertial tracks from Google Tango, ARCore, and Apple ARKit
with two recent methods published in academic forums. The data sets
cover both indoor and outdoor cases, with stairs, escalators, elevators,
oﬃce environments, a shopping mall, and metro station.

Keywords: Visual-inertial odometry · Navigation · Benchmarking

Access data and documentation at:
https://github.com/AaltoVision/ADVIO

1

Introduction

Various systems and approaches have recently emerged for tracking the mo-
tion of hand-held or wearable mobile devices based on video cameras and iner-
tial measurement units (IMUs). There exist both open published methods (e.g.
[14,16,2,12,21]) and closed proprietary systems. Recent examples of the latter
are ARCore by Google and ARKit by Apple which run on the respective man-
ufacturers’ ﬂagship smartphone models. Other examples of mobile devices with
built-in visual-inertial odometry are the Google Tango tablet device and Mi-
crosoft Hololens augmented reality glasses. The main motivation for developing

2

Cortés, Solin, Rahtu, and Kannala

Google Pixel
(ARCore pose)

Google Tango
(raw pose, area learning pose,
ﬁsheye video, point cloud)

Apple iPhone 6s
(ARKit pose)

Raw sensor data:
• Video
• Accelerometer
• Gyroscope
• Magnetometer
• Barometer
• GNSS

Free hand-
held motion

Ground-truth
(6-DoF pose)

Fig. 1. The custom-built capture rig with a Google Pixel smartphone on the left, a
Google Tango device in the middle, and an Apple iPhone 6s on the right.

odometry methods for smart mobile devices is to enable augmented reality appli-
cations which require precise real-time tracking of ego-motion. Such applications
could have signiﬁcant value in many areas, like architecture and design, games
and entertainment, telepresence, and education and training.
Despite th
ADVISE: Symbolism and External Knowledge
for Decoding Advertisements

Keren Ye[0000−0002−7349−7762] and Adriana Kovashka[0000−0003−1901−9660]

University of Pittsburgh, Pittsburgh PA 15260, USA

{yekeren,kovashka}@cs.pitt.edu

Abstract. In order to convey the most content in their limited space,
advertisements embed references to outside knowledge via symbolism.
For example, a motorcycle stands for adventure (a positive property the
ad wants associated with the product being sold), and a gun stands for
danger (a negative property to dissuade viewers from undesirable be-
haviors). We show how to use symbolic references to better understand
the meaning of an ad. We further show how anchoring ad understand-
ing in general-purpose ob ject recognition and image captioning improves
results. We formulate the ad understanding task as matching the ad im-
age to human-generated statements that describe the action that the ad
prompts, and the rationale it provides for taking this action. Our pro-
posed method outperforms the state of the art on this task, and on an
alternative formulation of question-answering on ads. We show additional
applications of our learned representations for matching ads to slogans,
and clustering ads according to their topic, without extra training.

Keywords: advertisements · symbolism · question answering · external
knowledge · vision and language · representation learning

1

Introduction

Advertisements are a powerful tool for aﬀecting human behavior. Product ads
convince us to make large purchases, e.g. for cars and home appliances, or small
but recurrent purchases, e.g. for laundry detergent. Public service announce-
ments (PSAs) encourage socially beneﬁcial behaviors, e.g. combating domestic
violence or driving safely. To stand out from the rest, ads have to be both eye-
catching and memorable [71], while also conveying the information that the ad
designer wants to impart. All this must be done in a limited space (one image)
and time (however many seconds the viewer spends looking at the ad).
How can ads get the most “bang for their buck”? One technique is to make
references to knowledge viewers already have, e.g. cultural knowledge, associa-
tions, and symbolic mappings humans have learned [54, 35, 57, 34]. These sym-
bolic references might come from literature (e.g. a snake symbolizes evil or dan-
ger), movies (a motorcycle symbolizes adventure or coolness), common sense (a
ﬂexed arm symbolizes strength), or pop culture (Usain Bolt symbolizes speed).
In this paper, we describe how to use symbolic mappings to predict the
messages of advertisements. On one hand, we model how components of the ad

2

Keren Ye, Adriana Kovashka

N

I

A
R

T

T
S
E
T

danger

gun

cool

bottle

A

danger

cool

motorbike

cool

B

C

bottle

D

I should (cid:271)u(cid:455) this drink (cid:271)e(cid:272)ause it ’s e(cid:454)(cid:272)iting.

Fig. 1. Our key idea: Use symbolic associations shown in yellow (a gun symbolizes
danger; a motorcycle s
Aﬃnity Derivation and Graph Merge for
Instance Segmentation

Yiding Liu1 , Siyu Yang2 , Bin Li3 , Wengang Zhou1 ,
Jizheng Xu3 , Houqiang Li1 , and Yan Lu3

1 Department of Electronic Engineering and Information Science
University of Science and Technology of China

liuyd123@mail.ustc.edu.cn {zhwg,lihq}@ustc.edu.cn

2 Beihang University

yangsiyu@buaa.edu.cn

3 Microsoft Research

{libin,jzxu,yanlu}@microsoft.com

Abstract. We present an instance segmentation scheme based on pixel
aﬃnity information, which is the relationship of two pixels belonging to
the same instance. In our scheme, we use two neural networks with sim-
ilar structures. One predicts the pixel level semantic score and the other
is designed to derive pixel aﬃnities. Regarding pixels as the vertexes and
aﬃnities as edges, we then propose a simple yet eﬀective graph merge
algorithm to cluster pixels into instances. Experiments show that our
scheme generates ﬁne grained instance masks. With Cityscape training
data, the proposed scheme achieves 27.3 AP on test set.

Keywords: instance segmentation, pixel aﬃnity, graph merge, proposal-
free

1

Introduction

With the fast development of Convolutional Neural Networks (CNN), re-
cent years have witnessed breakthroughs in various computer vision tasks. For
example, CNN based methods have surpassed humans in image classiﬁcation
[24]. Rapid progress has been made in the areas of ob ject detection [14, 26, 43],
semantic segmentation [17], and even instance segmentation [19, 21].
Semantic segmentation and instance segmentation try to label every pixel in
images. Instance segmentation is more challenging as it also tells which ob ject
one pixel belongs to. Basically, there are two categories of methods for instance
segmentation. The ﬁrst one is developed from ob ject detection. If one already has
results of ob ject detection, i.e. a bounding box for each ob ject, one can move
one step further to reﬁne the bounding box semantic information to generate

This work was done when Yiding Liu and Siyu Yang took internship at Microsoft
Research Asia.

2

Y. Liu, S. Yang, B. Li, W. Zhou, J. Xu, H. Li and Y. Lu

instance results. Since the results rely on the proposals from ob ject detection,
such a category can be regarded as proposal-based methods. The other one is to
cluster pixels into instances based on semantic segmentation results. We refer to
this category as proposal-free methods.

Recent instance segmentation methods have advanced in both directions
above. Proposal-based methods are usually extensions of ob ject detection frame-
works [42, 38, 18]. Fully Convolutional Instance-aware Semantic Segmentation
(FCIS) [33] produces position-sensitive feature maps [12] and generates masks
by merging features in corresponding areas. Mask RCNN (Mask Region CNN)
[23] extends Faster RCNN [44] with another branch to generate masks with diﬀer-
ent classes. Proposal-based methods produce instance-level results in the region
of interest (ROI) to make the mask
AGIL: Learning Attention from Human for
Visuomotor Tasks

Ruohan Zhang1 [0000−0001−6681−3360] , Zhuode Liu2 ,

Luxin Zhang3 , Jake A. Whritner4 , Karl S. Muller4 ,
Mary M. Hayhoe4 , and Dana H. Ballard1

1 Department of Computer Science, University of Texas at Austin, Austin, USA

{zharu,danab}@utexas.edu

2 Google Inc., Mountain View, USA

zhuodel@google.com

3 Department of Intelligence Science, Peking University, Beijing, China

4 Center for Perceptual Systems, University of Texas at Austin, Austin, USA

{jake.whritner,karl.muller,hayhoe}@utexas.edu

zhangluxin@pku.edu.cn

Abstract. When intelligent agents learn visuomotor behaviors from hu-
man demonstrations, they may beneﬁt from knowing where the human
is allocating visual attention, which can be inferred from their gaze. A
wealth of information regarding intelligent decision making is conveyed
by human gaze allocation; hence, exploiting such information has the
potential to improve the agents’ performance. With this motivation, we
propose the AGIL (Attention Guided Imitation Learning) framework.
We collect high-quality human action and gaze data while playing Atari
games in a carefully controlled experimental setting. Using these data, we
ﬁrst train a deep neural network that can predict human gaze positions
and visual attention with high accuracy (the gaze network) and then
train another network to predict human actions (the policy network).
Incorporating the learned attention model from the gaze network into
the policy network signiﬁcantly improves the action prediction accuracy
and task performance.

Keywords: Visual Attention · Eye Tracking · Imitation Learning

1

Introduction

In end-to-end learning of visuomotor behaviors, algorithms such as imitation
learning, reinforcement learning (RL), or a combination of both, have achieved
remarkable successes in video games [28], board games [37, 38], and robot manip-
ulation tasks [24, 30]. One ma jor issue of using RL alone is its sample eﬃciency,
hence in practice human demonstration can be used to speedup learning [37, 6,
15].
Imitation learning, or learning from demonstration, follows a student-teacher
paradigm, in which a learning agent learns from the demonstration of human

2

R. Zhang et al.

teachers [1]. A popular approach is behavior cloning, i.e., training an agent to
predict (imitate) demonstrated behaviors with supervised learning methods. Im-
itation learning research mainly focuses on the student–advancing our under-
standing of the learning agent–while very little eﬀort is made to understand the
human teacher. In this work, we argue that understanding and modeling the
human teacher is also an important research issue in this paradigm. Speciﬁcally,
in visuomotor learning tasks, a key component of human intelligence–the visual
attention mechanism–encodes a wealth of information that can be exploited by
a learning algorithm. Modeling human visual attention and guiding the learning
agent with a learned attention model could lead
AMC: AutoML for Model Compression
and Acceleration on Mobile Devices

Yihui He‡∗ , Ji Lin†∗ , Zhijian Liu† , Hanrui Wang† , Li-Jia Lil , and Song Han†

{jilin, songhan}@mit.edu

†Massachusetts Institute of Technology
‡Carnegie Mellon University
lGoogle

Abstract. Model compression is an eﬀective technique to eﬃciently
deploy neural network models on mobile devices which have limited
computation resources and tight power budgets. Conventional model
compression techniques rely on hand-crafted features and require domain
experts to explore the large design space trading oﬀ among model size,
speed, and accuracy, which is usually sub-optimal and time-consuming.
In this paper, we propose AutoML for Model Compression (AMC) which
leverages reinforcement learning to eﬃciently sample the design space
and can improve the model compression quality. We achieved state-of-
the-art model compression results in a fully automated way without any
human eﬀorts. Under 4× FLOPs reduction, we achieved 2.7% better
accuracy than the hand-crafted model compression method for VGG-16
on ImageNet. We applied this automated, push-the-button compression
pipeline to MobileNet-V1 and achieved a speedup of 1.53× on the GPU
(Titan Xp) and 1.95× on an Android phone (Google Pixel 1), with
negligible loss of accuracy.

Keywords: AutoML · Reinforcement learning · Model compression ·
CNN acceleration · Mobile vision.

1

Introduction

In many machine learning applications (e.g ., robotics, self-driving cars, and
advertisement ranking), deep neural networks are constrained by latency, energy
and model size budget. Many works have been proposed to improve the hardware
eﬃciency of neural networks by model compression [26, 19, 22]. The core of model
compression technique is to determine the compression policy for each layer
as they have diﬀerent redundancy, which conventionally requires hand-crafted
heuristics and domain experts to explore the large design space trading oﬀ among
model size, speed, and accuracy. The design space is so large that human heuristic
is usually sub-optimal, and manual model compression is time-consuming. To
this end, we aim to automatically ﬁnd the compression policy for an arbitrary
network to achieve even better performance than human strategy.

∗ indicates equal contributions.

2

Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li and Song Han

Model Compression by Human: 
Labor Consuming, Sub-optimal

Critic

Reward= -Error*log(FLOP)

Original NN

Compressed NN

AMC Engine

Actor

Action: Compress with 
Sparsity ratio at (e.g. 50%)

Embedding

Embedding st=[N,C,H,W,i…]

Layer t+1
? %

Layer t
50%

Layer t-1
30%

Original NN

Compressed NN

Agent: DDPG

Model Compression by AI: 
Automated, Higher Compression Rate, Faster

Environment: Channel Pruning

Fig. 1. Overview of AutoML for Model Compression (AMC) engine. Left: AMC replaces
human and makes model compression fully automated while performing better than
human. Right: Form AMC as a reinforcement learni
An Adversarial Approach to Hard Triplet
Generation

Yiru Zhao1,2 ⋆ , Zhongming Jin2 , Guo-jun Qi3,2 ,
Hongtao Lu1 ⋆⋆ , Xian-sheng Hua2 ⋆⋆

1 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and
Cognitive Engineering, Shanghai Jiao Tong University
2 Alibaba Damo Academy, Alibaba Group
3 Laboratory for MAchine Perception and LEarning, University of Central Florida

{yiru.zhao,htlu}@sjtu.edu.cn, guojun.qi@ucf.edu,
{zhongming.jinzm,xiansheng.hxs}@alibaba-inc.com,

Abstract. While deep neural networks have demonstrated competitive
results for many visual recognition and image retrieval tasks, the ma jor
challenge lies in distinguishing similar images from diﬀerent categories
(i.e., hard negative examples) while clustering images with large varia-
tions from the same category (i.e., hard positive examples). The current
state-of-the-art is to mine the most hard triplet examples from the mini-
batch to train the network. However, mining-based methods tend to look
into these triplets that are hard in terms of the current estimated net-
work, rather than deliberately generating those hard triplets that really
matter in globally optimizing the network. For this purpose, we pro-
pose an adversarial network for Hard Triplet Generation (HTG) to op-
timize the network ability in distinguishing similar examples of diﬀerent
categories as well as grouping varied examples of the same categories.
We evaluate our method on the real-world challenging datasets, such
as CUB200-2011, CARS196, DeepFashion and VehicleID datasets, and
show that our method outperforms the state-of-the-art methods signiﬁ-
cantly.

Keywords: image retrieval · hard examples · adversarial nets

1

Introduction

Deep metric learning is of great practical importance and has shown promising
results in many tasks, such as image retrieval [20, 32, 44, 31], face recognition [26,
34, 40], person re-identiﬁcation [1, 28, 46], etc. In spite of various forms of deep
metric learning in diﬀerent tasks, it shares a common goal of learning an optimal
image representation that pulls semantically similar images close to each other
while pushing dissimilar ones apart in a learned feature space.

⋆ This work was done when the author was visiting Alibaba as a research intern.
⋆⋆ Corresponding author.

2

Yiru Zhao, Zhongming Jin, Guo-jun Qi, Hongtao Lu, Xian-sheng Hua

Deep metric learning often considers images in triplets as its training units
inside a mini-batch. A triplet contains a query along with a relevant and an
irrelevant example. Then a deep metric learning algorithm seeks to push the
relevant (irrelevant) example towards (away from) the query in the underlying
embedding space. It is obvious that randomly choosing triplets can be very
ineﬃcient to train a deep embedding network, as not all triplets are equally
informative
[44]. Some triplets contain harder examples that cannot be well
handled by the current embedding network, where the irrelevant example is
closer to the query
Analyzing Clothing Layer Deformation Statistics
of 3D Human Motions

Jinlong Yang1 , Jean-S´ebastien Franco1 , Franck H´etroy-Wheeler2 , and Stefanie
Wuhrer1

1 Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP*, LJK, 38000 Grenoble, France
* Institute of Engineering Univ. Grenoble Alpes

{jinlong.yang,jean-sebastien.franco,stefanie.wuhrer}@inria.fr

2 ICube, University of Strasbourg, France

hetroywheeler@unistra.fr

Abstract. Recent capture technologies and methods allow not only to
retrieve 3D model sequence of moving people in clothing, but also to
separate and extract the underlying body geometry, motion component
and the clothing as a geometric layer. So far this clothing layer has only
been used as raw oﬀsets for individual applications such as retargeting
a diﬀerent body capture sequence with the clothing layer of another se-
quence, with limited scope, e.g. using identical or similar motions. The
structured, semantics and motion-correlated nature of the information
contained in this layer has yet to be fully understood and exploited. To
this purpose we propose a comprehensive analysis of the statistics of
this layer with a simple two-component model, based on PCA subspace
reduction of the layer information on one hand, and a generic parame-
ter regression model using neural networks on the other hand, designed
to regress from any semantic parameter whose variation is observed in
a training set, to the layer parameterization space. We show that this
model not only allows to reproduce previous retargeting works, but gen-
eralizes the data generation capabilities to other semantic parameters
such as clothing variation and size, or physical material parameters with
synthetically generated training sequence, paving the way for many kinds
of capture data-driven creation and augmentation applications.

Keywords: Clothing Motion Analysis, 3D Garment Capture, Capture
Data Augmentation and Retargeting

1

Introduction

Sequences showing the dense 3D geometry of dressed humans in motion are used
in many 3D content creation applications. Two main approaches may be used
to generate them, namely through physical simulation and by dense 3D motion
capture. Physical simulators allow to generate realistic motions and cloth folding
patterns based on given 3D models of the actor and the clothing, along with
actor motion and cloth material parameters [7, 9, 17]. Dense 3D motion capture
of human models has recently become possible at high spatial and temporal

2

J. Yang et al.

resolution, e.g. using multi-camera systems [4, 10, 23]. While the captured data is
unstructured, recent processing algorithms allow to track the captured geometry
over time and to separate the actor’s body from the clothing [21, 38, 40].
While these works allow for the generation of accurate dense 3D motion
sequences of human models with clothing, the content creation is expensive.
Motion capture of the dense 3D geometry requires calibrated acquisition set-
ups and the processing of th
Appearance-Based Gaze Estimation via
Evaluation-Guided Asymmetric Regression

Yihua Cheng1 , Feng Lu1,2 and Xucong Zhang3

1State Key Laboratory of Virtual Reality Technology and Systems, School of
Computer Science and Engineering, Beihang University, Beijing, China.
2Beijing Advanced Innovation Center for Big Data-Based Precision Medicine,
Beihang University, Beijing, China.
3Max Planck Institute for Informatics, Saarland Informatics Campus, Germany.

{yihua c,lufeng}@buaa.edu.cn, xczhang@mpi-inf.mpg.de

Abstract. Eye gaze estimation has been increasingly demanded by re-
cent intelligent systems to accomplish a range of interaction-related tasks,
by using simple eye images as input. However, learning the highly com-
plex regression between eye images and gaze directions is nontrivial, and
thus the problem is yet to be solved eﬃciently. In this paper, we pro-
pose the Asymmetric Regression-Evaluation Network (ARE-Net), and
try to improve the gaze estimation performance to its full extent. At

2

Y. Cheng, F. Lu and X. Zhang

limitations such as 1) requirement on speciﬁc hardware for illumination and
capture, 2) high failure rate when used in the uncontrolled environment, and 3)
limited working distance (typically within 60cm).

Diﬀerent with model-based methods, appearance-based methods do not rely
on small eye feature extraction under special illumination. Instead, they can work
with just a single ordinary camera to capture the eye appearance, then learn a
mapping function to predict the gaze direction from the eye appearance directly.
Whereas this greatly enlarges the applicability, the challenge part is that human
eye appearance can be heavily aﬀected by various factors, such as the head pose,
the illumination, and the individual diﬀerence, making the mapping function
diﬃcult to learn. In recent years, the Convolutional Neural Network (CNN) has
shown to be able to learn very complex functions given suﬃcient training data.
Consequently, the CNN-based methods have been reported to outperform the
conventional methods [6].

The goal of this work is to further exploit the power of CNNs and improve the
performance of appearance-based gaze estimation to a higher level. At the core
of our method is the notion of asymmetric regression for the left and the right
eyes. It is based on our key observation that 1) the gaze directions of two eyes
should be consistent physically, however, 2) even if we apply the same regression
method, the gaze estimation performance on two eyes can be very diﬀerent. Such
“two eye asymmetry” implys a new gaze regression strategy that no longer treats
both eyes equally but tends to rely on the “high quality eye” to train a more
eﬃcient and robust regression model.

In order to do so, we consider the following technical issues, i.e., how to design
a network that processes both eyes simultaneously and asymmetrically, and how
to control the asymmetry to optimize the network by using the high quality data.

Our idea is to guide th
ArticulatedFusion: Real-time Reconstruction of
Motion, Geometry and Segmentation Using a
Single Depth Camera

Chao Li1 [0000−0002−8046−9110] , Zheheng Zhao1 , and Xiaohu Guo1

Department of Computer Science,
The University of Texas at Dallas

{Chao.Li2, Zheheng.Zhao, xguo}@utdallas.edu

Abstract. This paper proposes a real-time dynamic scene reconstruc-
tion method capable of reproducing the motion, geometry, and segmenta-
tion simultaneously given live depth stream from a single RGB-D camera.
Our approach fuses geometry frame by frame and uses a segmentation-
enhanced node graph structure to drive the deformation of geometry in
registration step. A two-level node motion optimization is proposed. The
optimization space of node motions and the range of physically-plausible
deformations are largely reduced by taking advantage of the articulated
motion prior, which is solved by an eﬃcient node graph segmentation
method. Compared to previous fusion-based dynamic scene reconstruc-
tion methods, our experiments show robust and improved reconstruction
results for tangential and occluded motions.

Keywords: Fusion, Articulated, Motion, Segmentation

1

Introduction

Dynamic scene reconstruction is a very important topic for digital world build-
ing. It includes capturing and reproducing geometry, appearance, motion, and
skeleton, which enables more realistic rendering for VR/AR scenarios like Holo-
portation [5]. An example is that the reconstructed geometry can be directly
used for a virtual scene, and the articulated motion can be retargeted to new
models to generate new animations, making scene production more eﬃcient.
Although many eﬀorts have been devoted to this research ﬁeld, the problem
remains challenging due to extraordinarily large solution space but real-time
rendering requirements for VR/AR applications. Recently, volumetric depth fu-
sion methods for dynamic scene reconstruction, such as DynamicFusion [17],
VolumeDeform [10], Fusion4D [5] and albedo based fusion [8] open a new gate
for people in this ﬁeld. This type of method enables quality improvements over
temporal reconstruction models in terms of both accuracy and completeness of
the surface geometry. Among all these works, fusion methods by a single depth
camera [17, 10] are more promising for popularization, because of their low cost
and easy setup. However, this group of methods still faces some challenging is-
sues, like high occlusion from the single view, limited computational resource to

2

C. Li, Z. Zhao and X. Guo

achieve real-time performance, and no geometry/skeleton prior knowledge, and
thus are restricted to limited motions. DoubleFusion [30] can reconstruct both
the inner body and outer surface for faster motions by adding body template as
prior knowledge. Later, KillingFusion [21] and SobolevFusion [22] is proposed to
reconstruct dynamic scenes with topology changes and fast inter-frame motions.
DynamicFusion is the pioneering work acheiving template-less non-rigid re-
Ask, Acquire, and Attack: Data-free UAP
Generation using Class Impressions

Konda Reddy Mopuri*[0000−0001−8894−7212] , Phani Krishna

Uppala*[0000−0003−0413−5685] , and R. Venkatesh Babu[0000−0002−1926−1804]

Video Analytics Lab, Indian Institute of Science, Bangalore, India

Abstract. Deep learning models are susceptible to input speciﬁc noise,
called adversarial perturbations. Moreover, there exist input-agnostic
noise, called Universal Adversarial Perturbations (UAP) that can aﬀect
inference of the models over most input samples. Given a model, there
exist broadly two approaches to craft UAPs: (i) data-driven: that require
data, and (ii) data-free: that do not require data samples. Data-driven
approaches require actual samples from the underlying data distribution
and craft UAPs with high success (fooling) rate. However, data-free ap-
proaches craft UAPs without utilizing any data samples and therefore
result in lesser success rates. In this paper, for data-free scenarios, we
propose a novel approach that emulates the eﬀect of data samples with
class impressions in order to craft UAPs using data-driven ob jectives.
Class impression for a given pair of category and model is a generic
representation (in the input space) of the samples belonging to that cat-
egory. Further, we present a neural network based generative model that
utilizes the acquired class impressions to learn crafting UAPs. Experi-
mental evaluation demonstrates that the learned generative model, (i)
readily crafts UAPs via simple feed-forwarding through neural network
layers, and (ii) achieves state-of-the-art success rates for data-free sce-
nario and closer to that for data-driven setting without actually utilizing
any data samples.

Keywords: adversarial attacks · attacks on ML systems · data-free at-
tacks · image-agnostic perturbations · class impressions

1

Introduction

Machine learning models are pregnable (e.g. [4,3,9]) at test time to specially
learned, mild noise in the input space, commonly known as adversarial pertur-
bations. Data samples created via adding these perturbations to clean samples
are known as adversarial samples. Lately, the Deep Neural Networks (DNN)
based ob ject classiﬁers are also observed [28,7,14,11] to be drastically aﬀected
by the adversarial attacks with quasi imperceptible perturbations. Further, it
is observed (e.g. [28]) that these adversarial perturbations exhibit cross model
generalizability (transferability). This means, often same adversarial sample gets

*Equal contribution

2

K.R. Mopuri, P.K. Uppala and R.V. Babu

Fig. 1.

Ask, Acquire, and Attack

3

Approaches that craft UAPs can be broadly categorized into two classes: (i)
data-driven, and (ii) data-free approaches. Data-driven approaches such as [13]
require access to samples of the underlying data distribution to craft UAPs using
a fooling ob jective (e.g. conﬁdence reduction as in eq (2)). Thus, UAPs crafted
via data-driven approaches typically result in higher success rate
Associating Inter-Image Salient Instances for Weakly
Supervised Semantic Segmentation

Ruochen Fan1 [0000−0003−1991−0146] , Qibin Hou2 [0000−0002−8388−8708] , Ming-Ming
Cheng2 [0000−0001−5550−8758] Gang Yu3 [0000−0001−5570−2710] , Ralph R. Martin4 , and
Shi-Min Hu1 [0000−0001−7507−6542]

1 Tsinghua University, Beijing, China {frc16@mails.,shimin@}tsinghua.edu.cn
2 Nankai University, Tianjin, China cmm@nankai.edu.cn, andrewhoux@gmail.com

3 Megvii Inc., Beijing, China yugang@megvii.com
4 Cardiff University, Cardiff CF243AA, U.K. ralph@cs.cardiff.ac.uk

Abstract. Effectively bridging between image level keyword annotations and
corresponding image pixels is one of the main challenges in weakly supervised
semantic segmentation. In this paper, we use an instance-level salient object de-
tector to automatically generate salient instances (candidate objects) for train-
ing images. Using similarity features extracted from each salient instance in the
whole training set, we build a similarity graph, then use a graph partitioning al-
gorithm to separate it into multiple subgraphs, each of which is associated with
a single keyword (tag). Our graph-partitioning-based clustering algorithm allows
us to consider the relationships between all salient instances in the training set
as well as the information within them. We further show that with the help of at-
tention information, our clustering algorithm is able to correct certain wrong as-
signments, leading to more accurate results. The proposed framework is general,
and any state-of-the-art fully-supervised network structure can be incorporated
to learn the segmentation network. When working with DeepLab for semantic
segmentation, our method outperforms state-of-the-art weakly supervised alter-
natives by a large margin, achieving 65.6% mIoU on the PASCAL VOC 2012
dataset. We also combine our method with Mask R-CNN for instance segmenta-
tion, and demonstrated for the ﬁrst time the ability of weakly supervised instance
segmentation using only keyword annotations.

Keywords: Semantic segmentation, weak supervision, graph partitioning.

1

Introduction

Semantic segmentation, providing rich pixel level labeling of a scene, is one of the most
important tasks in computer vision. The strong learning ability of convolutional neural
networks (CNNs) has enabled signiﬁcant progress in this ﬁeld recently [5, 27, 29, 46, 47].
However, the performance of such CNN-based methods requires a large amount of
training data annotated to pixel-level, e.g., PASCAL VOC [11] and MS COCO [28];
such data are very expensive to collect. As an approach to alleviate the demand for
pixel-accurate annotations, weakly supervised semantic segmentation has drawn great
attention recently. Such methods merely require supervisions of one or more of the

2

Ruochen Fan, Qibin Hou and Ming-Ming Cheng

(a) input images

(b) salient instances

(c) proxy GT

(d) output results

Fig. 1: Input images (a) are fed into a salient instance detection metho
Attend and Rectify: a Gated Attention
Mechanism for Fine-Grained Recovery.

Pau Rodr´ıguez1 , Josep M. Gonfaus2 , Guillem Cucurull1 ,, F. Xavier Roca1 , and
Jordi Gonz`alez1

1 Computer Vision Center and Universitat Aut`onoma de Barcelona (UAB),
Campus UAB, 08193 Bellaterra, Catalonia Spain.

2 Visual Tagging Services, Parc de Recerca, Campus UAB

pau.rodriguez@cvc.uab.cat

Abstract. We propose a novel attention mechanism to enhance Convo-
lutional Neural Networks for ﬁne-grained recognition. It learns to attend
to lower-level feature activations without requiring part annotations and
uses these activations to update and rectify the output likelihood distri-
bution. In contrast to other approaches, the proposed mechanism is mod-
ular, architecture-independent and eﬃcient both in terms of parameters
and computation required. Experiments show that networks augmented
with our approach systematically improve their classiﬁcation accuracy
and become more robust to clutter. As a result, Wide Residual Networks
augmented with our proposal surpasses the state of the art classiﬁcation
accuracies in CIFAR-10, the Adience gender recognition task, Stanford
dogs, and UEC Food-100.

Keywords: Deep Learning · Convolutional Neural Networks · Attention

1

Introduction

Humans and animals process vasts amounts of information with limited com-
putational resources thanks to attention mechanisms which allow them to focus
resources on the most informative chunks of information [1,3,29]
This work is inspired by the advantages of visual and biological attention
mechanisms, for tackling ﬁne-grained visual recognition with Convolutional Neu-
ral Networks (CNN) [17]. This is a particularly diﬃcult task since it involves
looking for details in large amounts of data (images) while remaining robust to
deformation and clutter. In this sense, diﬀerent attention mechanisms for ﬁne-
grained recognition exist in the literature: (i) iterative methods that process im-
ages using ”glimpses” with recurrent neural networks (RNN) or long short-term
memory (LSTM) [26,38], (ii) feed-forward attention mechanisms that augment
vanilla CNNs, such as the Spatial Transformer Networks (STN) [11], or top-
down feed-forward attention mechanisms (FAM) [23]. Although it is not applied
to ﬁne-grained recognition, the Residual Attention introduced by [32] is another
example of feed-forward attention mechanism that takes advantage of residual

2

P. Rodr´ıguez et al.

Fig. 1: The proposed mechanism. The original CNN is augmented with N atten-
tion modules at N diﬀerent depths. Each attention module applies K attention
heads to the network feature maps to make a class prediction based on local in-
formation. The original network outputnet is then corrected based on the local
features by means of the global attention gates, resulting in the ﬁnal output.

connections [8] to enhance or dampen certain regions of the feature maps in an
incremental manner.
Thus, most of the existing attention mechanisms are e
Attention-aware Deep Adversarial Hashing for
Cross-Modal Retrieval

Xi Zhang1,2 [0000−0002−9173−4119] , Hanjiang Lai1,2 ⋆ [0000−0001−8057−6744] , and
Jiashi Feng3 [0000−0001−6843−0064]

1 School of Data and Computer Science, Sun Yat-Sen University, GuangZhou, China
2 Guangdong Key Laboratory of Big Data Analysis and Processing

zhangx368@mail2.sysu.edu.cn, laihanj3@mail.sysu.edu.cn

3 Department of Electrical and Computer Engineering,
National University of Singapore, Singapore, Singapore

elefjia@nus.edu.sg

Abstract. Due to the rapid growth of multi-modal data, hashing meth-
ods for cross-modal retrieval have received considerable attention. How-
ever, ﬁnding content similarities between diﬀerent modalities of data is
still challenging due to an existing heterogeneity gap. To further address
this problem, we propose an adversarial hashing network with an atten-
tion mechanism to enhance the measurement of content similarities by
selectively focusing on the informative parts of multi-modal data. The
proposed new deep adversarial network consists of three building blocks:
1) the feature learning module to obtain the feature representations; 2)
the attention module to generate an attention mask, which is used to
divide the feature representations into the attended and unattended fea-
ture representations; and 3) the hashing module to learn hash functions
that preserve the similarities between diﬀerent modalities. In our frame-
work, the attention and hashing modules are trained in an adversarial
way: the attention module attempts to make the hashing module un-
able to preserve the similarities of multi-modal data w.r.t. the unattend-
ed feature representations, while the hashing module aims to preserve
the similarities of multi-modal data w.r.t. the attended and unattend-
ed feature representations. Extensive evaluations on several benchmark
datasets demonstrate that the proposed method brings substantial im-
provements over other state-of-the-art cross-modal hashing methods.

Keywords: Hashing, Adversarial Learning, Attention Mechanism, Cross
Modal Retrieval

1

Introduction

Due to the rapid development of the Internet, diﬀerent types of media data
are also growing rapidly, e.g., texts, images, and videos. Cross-modal retrieval,
which takes one type of data as the query and returns the relevant data of

⋆ Corresponding author: Hanjiang Lai

2

Zhang et al.

Query: “a girl is sitting on a donkey”

masks

attended

unattended

(I) The attention module

(cid:1830)(cid:1861)(cid:1871)
(cid:1830)(cid:1861)(cid:1871)

a girl is 
sitting 
on a 
donkey

,

a girl is 
sitting 
on a 
donkey

,

(cid:3407)
(cid:3410)

(cid:1830)(cid:1861)(cid:1871)
(cid:1830)(cid:1861)(cid:1871)

a girl is 
sitting 
on a 
donkey

,

a girl is 
sitting 
on a 
donkey

,

(1) Learning hash module and attention module fixed 

(2) Learning attention module and hash module fixed 

(cid:1830)is: Distance in deep binary space

(II) Adversarial learning 

Fig. 1. Attention-aware de
Attention-based Ensemble for
Deep Metric Learning

Wonsik Kim, Bhavya Goyal, Kunal Chawla, Jungmin Lee, Keunjoo Kwon

Samsung Research,
Samsung Electronics

{wonsik16.kim, bhavya.goyal, kunal.chawla, jm411.lee,
keunjoo.kwon}@samsung.com

Abstract. Deep metric learning aims to learn an embedding function,
modeled as deep neural network. This embedding function usually puts
semantically similar images close while dissimilar images far from each
other in the learned embedding space. Recently, ensemble has been ap-
plied to deep metric learning to yield state-of-the-art results. As one
important aspect of ensemble, the learners should be diverse in their fea-
ture embeddings. To this end, we propose an attention-based ensemble,
which uses multiple attention masks, so that each learner can attend to
diﬀerent parts of the ob ject. We also propose a divergence loss, which en-
courages diversity among the learners. The proposed method is applied
to the standard benchmarks of deep metric learning and experimental
results show that it outperforms the state-of-the-art methods by a sig-
niﬁcant margin on image retrieval tasks.
Keywords: attention, ensemble, deep metric learning

1

Introduction

Deep metric learning has been actively researched recently. In deep metric learn-
ing, feature embedding function is modeled as a deep neural network. This fea-
ture embedding function embeds input images into feature embedding space
with a certain desired condition. In this condition, the feature embeddings of
similar images are required to be close to each other while those of dissimi-
lar images are required to be far from each other. To satisfy this condition,
many loss functions based on the distances between embeddings have been
proposed [3, 4, 6, 14, 25, 27–29, 33, 37]. Deep metric learning has been success-
fully applied in image retrieval task on popular benchmarks such as CARS-
196 [13], CUB-200-2011 [35], Stanford online products [29], and in-shop clothes
retrieval [18] datasets.
Ensemble is a widely used technique of training multiple learners to get a
combined model, which performs better than individual models. For deep met-
ric learning, ensemble concatenates the feature embeddings learned by multiple
learners which often leads to better embedding space under given constraints on
the distances between image pairs. The keys to success in ensemble are high per-
formance of individual learners as well as diversity among learners. To achieve
this ob jective, diﬀerent methods have been proposed [22, 39]. However, there
has not been much research on optimal architecture to yield diversity of feature
embeddings in deep metric learning.

2

W. Kim, B. Goyal, K. Chawla, J. Lee, K. Kwon

output1

output2

output3

output1

output2

output3

G

1

G

2

G

3

G

G

G

shared 
parameters

S

input

A

1

A

2

A

3

S

input

(a) 3-heads ensemble

(b) Attention-based ensemble (ABE-3)

Fig. 1. Diﬀerence between M -heads ensemble and attention-based ensemble. Both
a
Attention-GAN for Ob ject Transﬁguration in
Wild Images

Xinyuan Chen1,2,3 , Chang Xu3 , Xiaokang Yang1 , Dacheng Tao3

1MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University

{xychen91,xkyang}@sjtu.edu.cn

2Centre for Artiﬁcial Intelligence, SIT, FEIT, University of Technology Sydney
3UBTECH Sydney AI Centre, SIT, FEIT, University of Sydney

{c.xu,dacheng.tao}@sydney.edu.au

Abstract. This paper studies the ob ject transﬁguration problem in wild
images. The generative network in classical GANs for ob ject transﬁgura-
tion often undertakes a dual responsibility: to detect the ob jects of inter-
ests and to convert the ob ject from source domain to another domain. In
contrast, we decompose the generative network into two separated net-
works, each of which is only dedicated to one particular sub-task. The
attention network predicts spatial attention maps of images, and the
transformation network focuses on translating ob jects. Attention maps
produced by attention network are encouraged to be sparse, so that ma-
jor attention can be paid on ob jects of interests. No matter before or
after ob ject transﬁguration, attention maps should remain constant. In
addition, learning attention network can receive more instructions, given
the available segmentation annotations of images. Experimental results
demonstrate the necessity of investigating attention in ob ject transﬁgu-
ration, and that the proposed algorithm can learn accurate attention to
improve quality of generated images.

Keywords: Generative Adversarial Networks, Attention Mechanism

1

Introduction

The task of image-to-image translation aims to translate images from a source
domain to another target domain, e.g., greyscale to color and image to seman-
tic label. A lot of researches on image-to-image translation have been produced
in the supervised setting, where ground truths in the target domain are avail-
able. [1] learns a parametric translation function using CNNs by minimizing the
discrepancy between generated images and the corresponding target images. [2]
uses conditional GANs to learn a mapping from input to output images. Similar
ideas have been applied to various tasks such as generating photographs from
sketch or from semantic layout [3, 4], and image super-resolution [5].
To achieve image-to-image translation in the absence of paired examples,
a series of works has emerged by combining classical adversarial training [6]
with diﬀerent carefully designed constraints, e.g., circularity constraint [7–9], f -
consistency constraint [10], and distance constraints [11]. Although there is no

2

X. Chen et al.

Zebras

Horses

horse → zebra

zebra → horse

(a)

x ∈{X}

A(x)



x ∈{X}

A

D

real/fake

Sparse loss/MSE

y ∈{Y}

T(x)

T

(b)

Fig. 1. (a): Ob ject transﬁguration of horse ↔ zebra. (b): An illustration of Attention-
GAN. A, T , D respectively represent the attention network, the transformation network
and the discriminative network. Sparse loss d
Attribute-Guided Face Generation Using
Conditional CycleGAN

Yongyi Lu1 [0000−0003−1398−9965] , Yu-Wing Tai2 [0000−0002−3148−0380] , and
Chi-Keung Tang1 [0000−0001−6495−3685]

1 The Hong Kong University of Science and Technology
2 Tencent Youtu

{yluaw,cktang}@cse.ust.hk, yuwingtai@tencent.com

Abstract. We are interested in attribute-guided face generation: given
a low-res face input image, an attribute vector that can be extracted from
a high-res image (attribute image), our new method generates a high-
res face image for the low-res input that satisﬁes the given attributes.
To address this problem, we condition the CycleGAN and propose con-
ditional CycleGAN, which is designed to 1) handle unpaired training
data because the training low/high-res and high-res attribute images
may not necessarily align with each other, and to 2) allow easy con-
trol of the appearance of the generated face via the input attributes.
We demonstrate high-quality results on the attribute-guided conditional
CycleGAN, which can synthesize realistic face images with appearance
easily controlled by user-supplied attributes (e.g., gender, makeup, hair
color, eyeglasses). Using the attribute image as identity to produce the
corresponding conditional vector and by incorporating a face veriﬁca-
tion network, the attribute-guided network becomes the identity-guided
conditional CycleGAN which produces high-quality and interesting re-
sults on identity transfer. We demonstrate three applications on identity-
guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.

Keywords: Face Generation · Attribute · GAN.

1

Introduction

This paper proposes a practical approach, attribute-guided face generation, for
natural face image generation where facial appearance can be easily controlled
by user-supplied attributes. Figure 1 shows that by simply providing a high-res
image of Ivanka Trump, our face superresolution result preserves her identity
which is not necessarily guaranteed by conventional face superresolution (Fig-
ure 1: top row). When the input attribute/identity image is a diﬀerent person,
our method transfers the man’s identity to the high-res result, where the low-res
input is originally downsampled from a woman’s face (Figure 1: bottom row).

* This work was partially done when Yongyi Lu was an intern at Tencent Youtu.

2

Y. Lu, Y. W. Tai and C. K. Tang

(a)

(b)

(c)

(d)

(f)

(g)

(h)

(e)

(i)

Fig. 1. Identity-guided face generation. Top: identity-preserving face super-resolution
where (a) is the identity image; (b) input photo; (c) image crop from (b) in low resolu-
tion; (d) our generated high-res result; (e) ground truth image. Bottom: face transfer,
where (f ) is the identity image; (g) input low-res image of another person provides
overall shape constraint; (h) our generated high-res result where the man’s identity
is transferred. To produce the lo
Attributes as Operators:
Factorizing Unseen Attribute-Object Compositions

Tushar Nagarajan1 and Kristen Grauman2

1 The University of Texas at Austin
2 Facebook AI Research
tushar@cs.utexas.edu, grauman@fb.com∗

Abstract. We present a new approach to modeling visual attributes. Prior work
casts attributes in a similar role as objects, learning a latent representation where
properties (e.g., sliced) are recognized by classiﬁers much in the way objects
(e.g., apple) are. However, this common approach fails to separate the attributes
observed during training from the objects with which they are composed, mak-
ing it ineffectual when encountering new attribute-object compositions. Instead,
we propose to model attributes as operators. Our approach learns a semantic em-
bedding that explicitly factors out attributes from their accompanying objects,
and also beneﬁts from novel regularizers expressing attribute operators’ effects
(e.g., blunt should undo the effects of sharp). Not only does our approach align
conceptually with the linguistic role of attributes as modiﬁers, but it also general-
izes to recognize unseen compositions of objects and attributes. We validate our
approach on two challenging datasets and demonstrate signiﬁcant improvements
over the state of the art. In addition, we show that not only can our model recog-
nize unseen compositions robustly in an open-world setting, it can also generalize
to compositions where objects themselves were unseen during training.

1

Introduction

Attributes are semantic descriptions that convey an object’s properties—such as its ma-
terials, colors, patterns, styles, expressions, parts, or functions. Attributes have proven
to be an effective representation for faces and people [26, 36, 44, 49, 29, 45, 32], catalog
products [4, 24, 57, 17], and generic objects and scenes [28, 11, 27, 37, 19, 1]. Because
they are expressed in natural language, attributes facilitate human-machine communi-
cation about visual content, e.g., for applications in image search [26, 24], zero-shot
learning [1], narration [25], or image generation [55].
Attributes and objects are fundamentally different entities: objects are physical things
(nouns), whereas attributes are properties of those things (adjectives). Despite this fact,
existing methods for attributes largely proceed in the same manner as state-of-the-art
object recognition methods. Namely, image examples labeled according to the attributes
present are used to train discriminative models, e.g., with a convolutional neural net-
work [49, 29, 45, 32, 57, 47].
The latent vector encoding learned by such models is expected to capture an object-
agnostic attribute representation. Yet, achieving this is problematic, both in terms of

*On leave from University of Texas at Austin (grauman@cs.utexas.edu).

2

T. Nagarajan and K. Grauman

Sliced
Operator

prototypical 
"car" instance

prototypical 
"sliced" instance

Unseen Object

Fig. 1: Conceptual overview of our idea. Left: 
Audio-Visual Event Localization in
Unconstrained Videos

Yapeng Tian[0000−0003−1423−4513] , Jing Shi[0000−0002−4509−0535] , Bochen
Li[0000−0002−8304−6973] , Zhiyao Duan[0000−0002−8334−9974] , and Chenliang
Xu[0000−0002−2183−822X ]

University of Rochester, United States

Abstract. In this paper, we introduce a novel problem of audio-visual
event localization in unconstrained videos. We deﬁne an audio-visual
event as an event that is both visible and audible in a video segment.
We collect an Audio-Visual Event (AVE) dataset to systemically investi-
gate three temporal localization tasks: supervised and weakly-supervised
audio-visual event localization, and cross-modality localization. We de-
velop an audio-guided visual attention mechanism to explore audio-visual
correlations, propose a dual multimodal residual network (DMRN) to
fuse information over the two modalities, and introduce an audio-visual
distance learning network to handle the cross-modality localization. Our
experiments support the following ﬁndings: joint modeling of auditory
and visual modalities outperforms independent modeling, the learned at-
tention can capture semantics of sounding ob jects, temporal alignment
is important for audio-visual fusion, the proposed DMRN is eﬀective
in fusing audio-visual features, and strong correlations between the two
modalities enable cross-modality localization.

Keywords: audio-visual event, temporal localization, attention, fusion

1

Introduction

Studies in neurobiology suggest that the perceptual beneﬁts of integrating visual
and auditory information are extensive [9]. For computational models, they re-
ﬂect in lip reading [5,12], where correlations between speech and lip movements
provide a strong cue for linguistic understanding; in music performance [32],
where vibrato articulations and hand motions enable the association between
sound tracks and the performers; and in sound synthesis [41], where physical in-
teractions with diﬀerent types of material give rise to plausible sound patterns.
Albeit these advances, these models are limited in their constrained domains.
Indeed, our community has begun to explore marrying computer vision with
audition in-the-wild for learning a good representation [6,42,2]. For example, a
sound network is learned in [6] by a visual teacher network with a large amount
of unlabeled videos, which shows better performance than learning in a single
modality. However, they have all assumed that the audio and visual contents
in a video are matched (which is often not the case as we will show) and they

2

Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu

Fig. 1: (a) illustrates audio-visual event localization. The ﬁrst two rows show a 5s video
sequence with both audio and visual tracks for an audio-visual event chainsaw (event
is temporally labeled in yellow boxes). The third row shows our localization results
(in red boxes) and the generated audio-guided visual attention maps. (b) illustrates
cross-modality localization 
Audio-Visual Scene Analysis with
Self-Supervised Multisensory Features

Andrew Owens Alexei A. Efros

UC Berkeley

Abstract. The thud of a bouncing ball, the onset of speech as lips open — when
visual and audio events occur together, it suggests that there might be a com-
mon, underlying event that produced both signals. In this paper, we argue that
the visual and audio components of a video signal should be modeled jointly
using a fused multisensory representation. We propose to learn such a represen-
tation in a self-supervised way, by training a neural network to predict whether
video frames and audio are temporally aligned. We use this learned represen-
tation for three applications: (a) sound source localization, i.e. visualizing the
source of sound in a video; (b) audio-visual action recognition; and (c) on/off-
screen audio source separation, e.g. removing the off-screen translator’s voice
from a foreign ofﬁcial’s speech. Code, models, and video results are available on
our webpage: http://andrewowens.com/multisensory.

1

Introduction

As humans, we experience our world through a number of simultaneous sensory streams.
When we bite into an apple, not only do we taste it, but — as Smith and Gasser [1] point
out — we also hear it crunch, see its red skin, and feel the coolness of its core. The coin-
cidence of sensations gives us strong evidence that they were generated by a common,
underlying event [2], since it is unlikely that they co-occurred across multiple modal-
ities merely by chance. These cross-modal, temporal co-occurrences therefore provide
a useful learning signal: a model that is trained to detect them ought to discover multi-
modal structures that are useful for other tasks. In much of traditional computer vision
research, however, we have been avoiding the use of other, non-visual modalities, ar-
guably making the perception problem harder, not easier.
In this paper, we learn a temporal, multisensory representation that fuses the visual
and audio components of a video signal. We propose to train this model without using
any manually labeled data. That is, rather than explicitly telling the model that, e.g., it
should associate moving lips with speech or a thud with a bouncing ball, we have it dis-
cover these audio-visual associations through self-supervised training [3]. Speciﬁcally,
we train a neural network on a “pretext” task of detecting misalignment between audio
and visual streams in synthetically-shifted videos. The network observes raw audio and
video streams — some of which are aligned, and some that have been randomly shifted
by a few seconds — and we task it with distinguishing between the two. This turns out
to be a challenging training task that forces the network to fuse visual motion with audio
information and, in the process, learn a useful audio-visual feature representation.
We demonstrate the usefulness of our multisensory representation in three audio-
visual applications: (a) sound source localization, 
AugGAN: Cross Domain Adaptation with
GAN-based Data Augmentation

Sheng-Wei Huang1 ⋆ , Che-Tsung Lin1,2 [000−0002−5843−7294] ⋆ , Shu-Ping Chen1 ,
Yen-Yi Wu1 , Po-Hao Hsu1 , and Shang-Hong Lai1

1Department of Computer Science, National Tsing Hua University, Taiwan
2 Intelligent Mobility Division, Mechanical and Mechatronics Systems Research
Laboratories, Industrial Technology Research Institute, Taiwan

shengwei@mx.nthu.edu.tw, AlexLin@itri.org.tw, lai@cs.nthu.edu.tw

Abstract. Deep learning based image-to-image translation methods aim
at learning the joint distribution of the two domains and ﬁnding trans-
formations between them. Despite recent GAN (Generative Adversarial
Network) based methods have shown compelling results, they are prone
to fail at preserving image-ob jects and maintaining translation consis-
tency, which reduces their practicality on tasks such as generating large-
scale training data for diﬀerent domains. To address this problem, we
purpose a structure-aware image-to-image translation network, which is
composed of encoders, generators, discriminators and parsing nets for the
two domains, respectively, in a uniﬁed framework. The purposed network
generates more visually plausible images compared to competing meth-
ods on diﬀerent image-translation tasks. In addition, we quantitatively
evaluate diﬀerent methods by training Faster-RCNN and YOLO with
datasets generated from the image-translation results and demonstrate
signiﬁcant improvement on the detection accuracies by using the pro-
posed image-ob ject preserving network.

Keywords: Generative adversarial network, image-to-image translation,
semantic segmentation, ob ject detection, domain adaptation

1

Introduction

Deep learning pipelines have stimulated substantial progress for general ob ject
detection. Detectors kept pushing the boundaries on several detection datasets.
However, despite being able to eﬃciently detect ob jects seen by arbitrary view-
ing angles, CNN-based detectors are still limited in a way that they could not
function properly when faced with domains signiﬁcantly diﬀerent from those
in the original training dataset. The most common way to obtain performance
gain is to go through the troublesome data collection/annotation process. Nev-
ertheless, the recent successes of Generative Adversarial Networks (GANs) on
image-to-image translation have opened up possibilities in generating large-scale
detection training data without the need for ob ject annotation.

⋆ Indicates equal contribution

2

Huang et al.

Generative adversarial networks (Goodfellow et al. 2014)[1], which put two
networks (i.e., a generator and a discriminator) competing against each other,
have emerged as a powerful framework for learning generative models of random
data distributions. While expecting GANs to produce an RGB image and its
associated bounding boxes from a random noise vector still sounds like a fantasy,
training GANs to translate images from one scenario to another could h
AutoLoc: Weakly-supervised Temporal Action
Localization in Untrimmed Videos

Zheng Shou1 [0000−0002−7681−2166] , Hang Gao1 , Lei Zhang2 , Kazuyuki
Miyazawa3 , and Shih-Fu Chang1

1 Columbia University, New York, NY, USA
2 Microsoft Research, Redmond, Washington, USA
3 Mitsubishi Electric, Japan

Abstract. Temporal Action Localization (TAL) in untrimmed video is
important for many applications. But it is very expensive to annotate
the segment-level ground truth (action class and temporal boundary).
This raises the interest of addressing TAL with weak supervision, namely
only video-level annotations are available during training). However, the
state-of-the-art weakly-supervised TAL methods only focus on generat-
ing good Class Activation Sequence (CAS) over time but conduct simple
thresholding on CAS to localize actions. In this paper, we ﬁrst develop
a novel weakly-supervised TAL framework called AutoLoc to directly
predict the temporal boundary of each action instance. We propose a
novel Outer-Inner-Contrastive (OIC) loss to automatically discover the
needed segment-level supervision for training such a boundary predictor.
Our method achieves dramatically improved performance: under the IoU
threshold 0.5, our method improves mAP on THUMOS’14 from 13.7%
to 21.2% and mAP on ActivityNet from 7.4% to 27.3%. It is also very en-
couraging to see that our weakly-supervised method achieves comparable
results with some fully-supervised methods.

Keywords: Temporal Action Localization; Weak Supervision; Outer-
Inner-Contrastive; Class Activation Sequence

1

Introduction

Impressive improvement has been made in the past two years to address Tem-
poral Action Localization (TAL) in untrimmed videos [30, 19, 54, 76, 77, 60,
58, 44, 29, 82, 22, 75, 15, 14, 21, 7, 6, 78, 61, 20]. These methods were proposed for
the fully-supervised setting: the model training requires the full annotation
of the ground truth temporal boundary (start time and end time) for each ac-
tion instance. However, untrimmed videos are usually very long with substantial
background content in time. Therefore, manually annotating temporal bound-
aries for a new large-scale dataset is very expensive and time-consuming [81], and
thus might prohibit applying the fully-supervised methods to the new domains
that lack enough training data with full annotations.
This motivates us to develop TAL methods that require signiﬁcantly fewer
ground truth annotations for training. As illustrated in Fig. 1, in this paper

2

Z. Shou, H. Gao, L. Zhang, K. Miyazawa, S.-F. Chang

Fig. 1. We study the weakly-supervised temporal action localization problem: during
training we only have videos with the video-level labels, but during testing we need to
predict both (1) the action class and (2) the temporal boundary of each action instance.
In order to obtain the segment-level supervision for training the action localization
model to predict the boundary directly, we design a novel Outer-Inner-Contrastive
(OIC) loss
Bayesian Semantic Instance Segmentation
in Open Set World

Trung Pham, Vijay Kumar B G, Thanh-Toan Do,
Gustavo Carneiro, and Ian Reid

School of Computer Science, The University of Adelaide,

{trung.pham,vijay.kumar,thanh-toan.do,
gustavo.carneiro,ian.reid}@adelaide.edu.au

Abstract. This paper addresses the semantic instance segmentation
task in the open-set conditions, where input images can contain known
and unknown ob ject classes. The training process of existing semantic
instance segmentation methods requires annotation masks for all ob ject
instances, which is expensive to acquire or even infeasible in some realistic
scenarios, where the number of categories may increase boundlessly. In
this paper, we present a novel open-set semantic instance segmentation
approach capable of segmenting all known and unknown ob ject classes
in images, based on the output of an ob ject detector trained on known
ob ject classes. We formulate the problem using a Bayesian framework,
where the posterior distribution is approximated with a simulated anneal-
ing optimization equipped with an eﬃcient image partition sampler. We
show empirically that our method is competitive with state-of-the-art su-
pervised methods on known classes, but also performs well on unknown
classes when compared with unsupervised methods.

Keywords: Instance segmentation, Open-set conditions

1

Introduction

In recent years, scene understanding driven by multi-class semantic segmenta-
tion [10, 13, 16], ob ject detection [19] or instance segmentation [7] has progressed
signiﬁcantly thanks to the power of deep learning. However, a ma jor limitation
of these deep learning based approaches is that they only work for a set of known
ob ject classes that are used during supervised training. In contrast, autonomous
systems often operate under open-set conditions [23] in many application do-
mains, i.e. they will inevitably encounter ob ject classes that were not part of the
training dataset. For instance, state-of-the-art methods such as Mask-RCNN [7]
and YOLO9000 [19] fail to detect such unknown ob jects. This behavior is detri-
mental to the performance of autonomous systems that would ideally need to
understand scenes holistically, i.e., reasoning about all ob jects that appear in
the scene and their complex relations.
Semantic instance segmentation based scene understanding has recently at-
tracted the interest of the ﬁeld [3, 25]. The ultimate goal is to decompose the

2

T. Pham, V. Kumar B G, T-T Do, G. Carneiro, I. Reid

Fig. 1: Overview of semantic instance segmentation in a open-set environment. Our
method segments all image regions irrespective of whether they have been detected or
undetected, or are from a known or unknown class

input image into individual ob jects (e.g., car, human, chair) and stuﬀs (e.g., road,
ﬂoor) along with their semantic labels. Compared with semantic segmentation
and ob ject detection, the accuracy and robustness of semantic instance segmen-
tation lags s
Bi-box Regression for Pedestrian Detection and
Occlusion Estimation

Chunluan Zhou1,2 [0000−0003−0284−6256] and Junsong Yuan2 [0000−0002−7324−7034]

1 Nanyang Technological University, Singapore
2 The State University of New York at Buﬀalo, USA

czhou002@e.ntu.edu.sg, jsyuan@buffalo.edu

Abstract. Occlusions present a great challenge for pedestrian detection
in practical applications. In this paper, we propose a novel approach to
simultaneous pedestrian detection and occlusion estimation by regressing
two bounding boxes to localize the full body as well as the visible part of
a pedestrian respectively. For this purpose, we learn a deep convolutional
neural network (CNN) consisting of two branches, one for full body es-
timation and the other for visible part estimation. The two branches are
treated diﬀerently during training such that they are learned to produce
complementary outputs which can be further fused to improve detection
performance. The full body estimation branch is trained to regress full
body regions for positive pedestrian proposals, while the visible part esti-
mation branch is trained to regress visible part regions for both positive
and negative pedestrian proposals. The visible part region of a negative
pedestrian proposal is forced to shrink to its center. In addition, we in-
troduce a new criterion for selecting positive training examples, which
contributes largely to heavily occluded pedestrian detection. We validate
the eﬀectiveness of the proposed bi-box regression approach on the Cal-
tech and CityPersons datasets. Experimental results show that our ap-
proach achieves promising performance for detecting both non-occluded
and occluded pedestrians, especially heavily occluded ones.

Keywords: Pedestrian detection · Occlusion handling · Deep CNN

1

Introduction

Pedestrian detection has a wide range of applications including autonomous driv-
ing, robotics and video surveillance. Many eﬀorts have been made to improve its
performance in recent years [3, 8, 17, 6, 33, 40, 5, 39, 41, 37, 34, 4]. Although rea-
sonably good performance has been achieved on some benchmark datasets for
detecting non-occluded or slightly occluded pedestrians, the performance for de-
tecting heavily occluded pedestrians is still far from being satisfactory. Take the
Caltech dataset [9] for example. One of the top-performing approaches, SDS-
RCNN [4], achieves a miss rate of about 7.4% at 0.1 false positives per image
(FPPI) for non-occluded or slightly occluded pedestrian detection, but its miss
rate increases dramatically to about 58.5% at 0.1 FPPI for heavily occluded

2

C. Zhou and J. Yuan

Fig. 1. Detection examples of our approach. The red and blue boxes on each detec-
tion represent the estimated full body and visible part respectively. For a pedestrian
detection, its visible part is estimated normally as shown in columns 1 and 2. For a
non-pedestrian detection, its visible part is estimated to be the center of its correspond-
ing pedestrian 
Bi-Real Net: Enhancing the Performance of
1-bit CNNs With Improved Representational
Capability and Advanced Training Algorithm

Zechun Liu1 , Baoyuan Wu2 , Wenhan Luo2 , Xin Yang3 ⋆ , Wei Liu2 , and
Kwang-Ting Cheng1

1 Hong Kong University of Science and Technology
2 Tencent AI lab
3 Huazhong University of Science and Technology

zliubq@connect.ust.hk, {wubaoyuan1987, whluo.china}@gmail.com,
xinyang2014@hust.edu.cn, wliu@ee.columbia.edu, timcheng@ust.hk

Abstract. In this work, we study the 1-bit convolutional neural net-
works (CNNs), of which both the weights and activations are binary.
While being eﬃcient, the classiﬁcation accuracy of the current 1-bit
CNNs is much worse compared to their counterpart real-valued CNN
models on the large-scale dataset, like ImageNet. To minimize the per-
formance gap between the 1-bit and real-valued CNN models, we propose
a novel model, dubbed Bi-Real net, which connects the real activations
(after the 1-bit convolution and/or BatchNorm layer, before the sign
function) to activations of the consecutive block, through an identity
shortcut. Consequently, compared to the standard 1-bit CNN, the rep-
resentational capability of the Bi-Real net is signiﬁcantly enhanced and
the additional cost on computation is negligible. Moreover, we develop
a speciﬁc training algorithm including three technical novelties for 1-
bit CNNs. Firstly, we derive a tight approximation to the derivative of
the non-diﬀerentiable sign function with respect to activation. Secondly,
we propose a magnitude-aware gradient with respect to the weight for
updating the weight parameters. Thirdly, we pre-train the real-valued
CNN model with a clip function, rather than the ReLU function, to bet-
ter initialize the Bi-Real net. Experiments on ImageNet show that the
Bi-Real net with the proposed training algorithm achieves 56.4% and
62.2% top-1 accuracy with 18 layers and 34 layers, respectively. Com-
pared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net achieves
up to 10% higher top-1 accuracy with more memory saving and lower
computational cost.

1

Introduction

Deep Convolutional Neural Networks (CNNs) have achieved substantial ad-
vances in a wide range of vision tasks, such as ob ject detection and recognition
[12, 23, 25, 5, 3, 20], depth perception [2, 16], visual relation detection [29, 30], face

⋆ Corresponding author.

2

Zechun Liu et al.

tracking and alignment [24, 32, 34, 28, 27], ob ject tracking [17], etc. However, the
superior performance of CNNs usually requires powerful hardware with abun-
dant computing and memory resources. For example, high-end Graphics Process-
ing Units (GPUs). Meanwhile, there are growing demands to run vision tasks,
such as augmented reality and intelligent navigation, on mobile hand-held de-
vices and small drones. Most mobile devices are not equipped with a powerful
GPU neither an adequate amount of memory to run and store the expensive
CNN model. Consequently, the high demand for computation and
BiSeNet: Bilateral Segmentation Network for
Real-time Semantic Segmentation

Changqian Yu⋆ 1 [0000−0002−4488−4157] , Jingbo Wang⋆ 2 [0000−0001−9700−6262] ,
Chao Peng3 [0000−0003−4069−4775] , Changxin Gao⋆⋆ 1 [0000−0003−2736−3920] , Gang
Yu3 [0000−0001−5570−2710] , and Nong Sang1 [0000−0002−9167−1496]

1 National Key Laboratory of Science and Technology on Multispectral Information
Processing,School of Automation,Huazhong University of Science & Technology,China

{changqian yu,cgao,nsang}@hust.edu.cn

2 Key Laboratory of Machine Perception, Peking University, China

wangjingbo1219@pku.edu.cn

3 Megvii Inc. (Face++), China

{pengchao,yugang}@megvii.com

Abstract. Semantic segmentation requires both rich spatial informa-
tion and sizeable receptive ﬁeld. However, modern approaches usually
compromise spatial resolution to achieve real-time inference speed, which
leads to poor performance. In this paper, we address this dilemma with
a novel Bilateral Segmentation Network (BiSeNet). We ﬁrst design a
Spatial Path with a small stride to preserve the spatial information and
generate high-resolution features. Meanwhile, a Context Path with a fast
downsampling strategy is employed to obtain suﬃcient receptive ﬁeld.
On top of the two paths, we introduce a new Feature Fusion Module
to combine features eﬃciently. The proposed architecture makes a right
balance between the speed and segmentation performance on Cityscapes,
CamVid, and COCO-Stuﬀ datasets. Speciﬁcally, for a 2048×1024 input,
we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed
of 105 FPS on one NVIDIA Titan XP card, which is signiﬁcantly faster
than the existing methods with comparable performance.

Keywords: Real-time Semantic Segmentation · Bilateral Segmentation
Network

1

Introduction

The research of semantic segmentation, which amounts to assign semantic labels
to each pixel, is a fundamental task in computer vision. It can be broadly ap-
plied to the ﬁelds of augmented reality devices, autonomous driving, and video
surveillance. These applications have a high demand for eﬃcient inference speed
for fast interaction or response.

⋆ Equal Contribution
⋆⋆ Corresponding Author

2

C. Yu et al.

(a) Input and model

(b) U-shape

(c) Ours

Fig. 1. Illustration of the architectures to speed up and our proposed approach. (a)
presents the cropping or resizing operation on the input image and the lightweight
model with pruning channels or dropping stages. (b) indicates the U-shape structure.
(c) demonstrates our proposed Bilateral Segmentation Network (BiSeNet). The black
dash line represents the operations which damage the spatial information, while the red
dash line represents the operations which shrink the receptive ﬁeld. The green block is
our proposed Spatial Path (SP). In the network part, each block represents the feature
map of diﬀerent down-sampling size. And the length of the block represents the spatial
resolution, while the thickness is on behalf of the number of channels.
BodyNet: Volumetric Inference of
3D Human Body Shapes

G¨ul Varol1,*
Duygu Ceylan2
Bryan Russell2
Jimei Yang2
Ersin Yumer2,‡
Ivan Laptev1,*
Cordelia Schmid1,†

1 Inria, France

2Adobe Research, USA

Abstract. Human shape estimation is an important task for video edit-
ing, animation and fashion industry. Predicting 3D human body shape
from natural images, however, is highly challenging due to factors such
as variation in human bodies, clothing and viewpoint. Prior methods
addressing this problem typically attempt to ﬁt parametric body mod-
els with certain priors on pose and shape. In this work we argue for an
alternative representation and propose BodyNet, a neural network for
direct inference of volumetric body shape from a single image. BodyNet
is an end-to-end trainable network that beneﬁts from (i) a volumetric
3D loss, (ii) a multi-view re-pro jection loss, and (iii) intermediate su-
pervision of 2D pose, 2D body part segmentation, and 3D pose. Each of
them results in performance improvement as demonstrated by our exper-
iments. To evaluate the method, we ﬁt the SMPL model to our network
output and show state-of-the-art results on the SURREAL and Unite
the People datasets, outperforming recent approaches. Besides achieving
state-of-the-art performance, our method also enables volumetric body-
part segmentation.

1

Introduction

Parsing people in visual data is central to many applications including mixed-
reality interfaces, animation, video editing and human action recognition. To-
wards this goal, human 2D pose estimation has been signiﬁcantly advanced by
recent eﬀorts [1–4]. Such methods aim to recover 2D locations of body joints and
provide a simpliﬁed geometric representation of the human body. There has also
been signiﬁcant progress in 3D human pose estimation [5–8]. Many applications,
however, such as virtual clothes try-on, video editing and re-enactment require
accurate estimation of both 3D human pose and shape.
3D human shape estimation has been mostly studied in controlled settings
using speciﬁc sensors including multi-view capture [9], motion capture mark-
ers [10], inertial sensors [11], and 3D scanners [12]. In uncontrolled single-view
settings 3D human shape estimation, however, has received little attention so
far. The challenges include the lack of large-scale training data, the high dimen-
sionality of the output space, and the choice of suitable representations for 3D

∗ ´Ecole normale sup´erieure, Inria, CNRS, PSL Research University, Paris, France
†Univ. Grenoble Alpes, Inria, CNRS, INPG, LJK, Grenoble, France
‡Currently at Argo AI, USA. This work was performed while EY was at Adobe.

2

Varol, Ceylan, Russell, Yang, Yumer, Laptev, Schmid

Fig. 1: Our BodyNet predicts a volumetric 3D human body shape and 3D body
parts from a single image. We show the input image, the predicted human voxels,
and the predicted part voxels.

human shape. Bogo et al. [13] present the ﬁrst automatic method to ﬁt a de-
formable body model t
Boosted Attention: Leveraging Human
Attention for Image Captioning

Shi Chen[0000−0002−3749−4767] and Qi Zhao[0000−0003−3054−8934]

Department of Computer Science and Engineering,
University of Minnesota

{chen4595,qzhao}@umn.edu

Abstract. Visual attention has shown usefulness in image captioning,
with the goal of enabling a caption model to selectively focus on re-
gions of interest. Existing models typically rely on top-down language
information and learn attention implicitly by optimizing the captioning
ob jectives. While somewhat eﬀective, the learned top-down attention can
fail to focus on correct regions of interest without direct supervision of
attention. Inspired by the human visual system which is driven by not
only the task-speciﬁc top-down signals but also the visual stimuli, we in
this work propose to use both types of attention for image captioning.
In particular, we highlight the complementary nature of the two types
of attention and develop a model (Boosted Attention) to integrate them
for image captioning. We validate the proposed approach with state-of-
the-art performance across various evaluation metrics.

Keywords: Image Captioning, Visual Attention, Human Attention

1

Introduction

Image captioning aims at generating ﬂuent language descriptions on a given
image. Inspired by the human visual system, in the past few years, visual atten-
tion has been incorporated in various image captioning models [21, 26, 32, 33].
Attention mechanisms encourage models to selectively focus on speciﬁc regions
while generating captions instead of scanning through the whole image, avoiding
information overﬂow as well as highlighting visual regions related to the task.
Following the success made in [32], visual attention in most conventional
image captioning models is developed in a top-down fashion on a word basis.
That is, visual attention is computed for each generated word based on visual
information from the image and the partially generated natural language de-
scription. While such mechanism (i.e., top-down attention) aims at connecting
natural language and visual content, without prior knowledge on the visual con-
tent in terms of salient regions (i.e., stimulus-based attention), the computed
visual attention can fail to concentrate on ob jects of interest and attend to ir-
relevant regions. As shown in Figure 1, a model with only top-down attention
focuses on non-salient regions in the background (Figure 1(c)) and does not cap-
ture salient ob jects in the image, i.e., bul ldog and teddy bear according to the
human-generated caption.

2

S. Chen and Q. Zhao

Fig. 1. Top-down attention may fail to focus on ob jects of interest. (a): original im-
age with human-generated caption, (b-c) two top-down attention maps and their cor-
responding model-generated captions, and (d) stimulus-based attention map for the
image. Words related to the top-down attention maps are colored in red.

Human attention is driven by both task-speciﬁc top-down sign
BOP: Benchmark for 6D Ob ject Pose Estimation

Tom´aˇs Hodaˇn1∗ , Frank Michel2∗ , Eric Brachmann3 , Wadim Kehl4
Anders Glent Buch5 , Dirk Kraft5 , Bertram Drost6 , Joel Vidal7 , Stephan Ihrke2
Xenophon Zabulis8 , Caner Sahin9 , Fabian Manhardt10 , Federico Tombari10
Tae-Kyun Kim9 , Jiˇr´ı Matas1 , Carsten Rother3

1CTU in Prague, 2TU Dresden, 3Heidelberg University, 4Toyota Research Institute
5University of Southern Denmark, 6MVTec Software, 7Taiwan Tech
8FORTH Heraklion, 9 Imperial College London, 10TU Munich

Abstract. We propose a benchmark for 6D pose estimation of a rigid
ob ject from a single RGB-D input image. The training data consists of
a texture-mapped 3D ob ject model or images of the ob ject in known 6D
poses. The benchmark comprises of: i) eight datasets in a uniﬁed format
that cover diﬀerent practical scenarios, including two new datasets focus-
ing on varying lighting conditions, ii) an evaluation methodology with a
pose-error function that deals with pose ambiguities, iii) a comprehensive
evaluation of 15 diverse recent methods that captures the status quo of
the ﬁeld, and iv) an online evaluation system that is open for continu-
ous submission of new results. The evaluation shows that methods based
on point-pair features currently perform best, outperforming template
matching methods, learning-based methods and methods based on 3D
local features. The pro ject website is available at bop.felk.cvut.cz.

1

Introduction

Estimating the 6D pose, i.e. 3D translation and 3D rotation, of a rigid ob ject
has become an accessible task with the introduction of consumer-grade RGB-D
sensors. An accurate, fast and robust method that solves this task will have a
big impact in application ﬁelds such as robotics or augmented reality.
Many methods for 6D ob ject pose estimation have been published recently,
e.g. [34,24,18,2,36,21,27,25], but it is unclear which methods perform well and
in which scenarios. The most commonly used dataset for evaluation was created
by Hinterstoisser et al. [14], which was not intended as a general benchmark and
has several limitations: the lighting conditions are constant and the ob jects are
easy to distinguish, unoccluded and located around the image center. Since then,
some of the limitations have been addressed. Brachmann et al. [1] added ground-
truth annotation for occluded ob jects in the dataset of [14]. Hodaˇn et al. [16]
created a dataset that features industry-relevant ob jects with symmetries and
similarities, and Drost et al. [8] introduced a dataset containing ob jects with
reﬂective surfaces. However, the datasets have diﬀerent formats and no standard
evaluation methodology has emerged. New methods are usually compared with
only a few competitors on a small subset of datasets.

∗Authors have been leading the pro ject jointly.

2

Hodaˇn, Michel et al.

LM/LM-O [14,1]

IC-MI [34]

IC-BIN [7]

T-LESS [16]

RU-APC [28]

TUD-L - new

TYO-L - new

BOP: Benchmark for 6D Ob ject Pose Estimation

3

1.1 Rel
Broadcasting Convolutional Network
for Visual Relational Reasoning

Simyung Chang1,2 , John Yang1 , SeongUk Park1 , and No jun Kwak1

1 Seoul National University, Seoul, South Korea

{timelighter, yjohn, swpark0703, nojunk}@snu.ac.kr

2 Samsung Electronics, Suwon, South Korea

Abstract. In this paper, we propose the Broadcasting Convolutional
Network (BCN) that extracts key ob ject features from the global ﬁeld
of an entire input image and recognizes their relationship with local fea-
tures. BCN is a simple network module that collects eﬀective spatial
features, embeds location information and broadcasts them to the entire
feature maps. We further introduce the Multi-Relational Network (mul-
tiRN) that improves the existing Relation Network (RN) by utilizing the
BCN module. In pixel-based relation reasoning problems, with the help
of BCN, multiRN extends the concept of ‘pairwise relations’ in conven-
tional RNs to ‘multiwise relations’ by relating each ob ject with multiple
ob jects at once. This yields in O(n) complexity for n ob jects, which is
2 ). Through experi-
a vast computational gain from RNs that take O(n
ments, multiRN has achieved a state-of-the-art performance on CLEVR
dataset, which proves the usability of BCN on relation reasoning prob-
lems.

Keywords: Visual Relational Reasoning, BCN, Broadcast, CLEVR,
Multi-RN, Visuo-spatial features

1

Introduction

A complete cognizance of a visual scene is achieved by relational reasonings of
a set of detected entities in an attempt to discover the underlying structure
[18]. Reasoning comparative relationships allows artiﬁcial intelligence to infer
semantic similarities or transitive orders among ob jects in scenes with various
perspectives and scales [5]. While the core of relational reasoning instrumentally
depends on spatial learning [3, 28], the relational networks (RNs) [27, 29] have
fostered the performance vastly on related tasks based on their spatial grid fea-
tures. However, the number of ob jects in conventional RNs upsurges as their
method assumes that each grid represents an ob ject at the corresponding posi-
tion within the scene regardless of the existence of an ob ject at a grid position.
Moreover, the computational cost increases quadratically as RNs are based on
pairwise computation of ob jects’ relations for relational reasoning.
This computational burden is inevitable for visual reasoning problems if con-
ventional architectures of convolutional neural networks (CNNs) [21] are used.

2

Simyung Chang, John Yang, SeongUk Park, No jun Kwak

Although, CNNs have allowed success in many computer vision problems [6, 7, 9,
12, 19, 24], yet they still suﬀer from diﬃculties in generalization over geometric
variations of scenes. This is mainly due to the receptive ﬁelds that are mapped
with convolution ﬁlters at ﬁxed areas, which derives CNNs to disregard spatial
locations in the process of searching for optimal features. Either bigger size of
ﬁlters that embrace multiple input ent
BSN: Boundary Sensitive Network for Temporal Action
Proposal Generation

Tianwei Lin1 [0000−0001−5535−279X ] , Xu Zhao⋆ 1 [0000−0002−8176−623X ] , Haisheng

Su1 [0000−0002−4228−7439] , Chongjing Wang2 , and Ming Yang1

1 Department of Automation, Shanghai Jiao Tong University, China
2 China Academy of Information and Communications Technology, China

wzmsltw,zhaoxu,suhaisheng,mingyang@sjtu.edu.cn

wangchongjing@caict.ac.cn

Abstract. Temporal action proposal generation is an important yet challeng-
ing problem, since temporal proposals with rich action content are indispens-
able for analysing real-world videos with long duration and high proportion ir-
relevant content. This problem requires methods not only generating proposals
with precise temporal boundaries, but also retrieving proposals to cover truth
action instances with high recall and high overlap using relatively fewer pro-
posals. To address these difﬁculties, we introduce an effective proposal genera-
tion method, named Boundary-Sensitive Network (BSN), which adopts “local to
global” fashion. Locally, BSN ﬁrst locates temporal boundaries with high prob-
abilities, then directly combines these boundaries as proposals. Globally, with
Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the
conﬁdence of whether a proposal contains an action within its region. We con-
duct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14,
where BSN outperforms other state-of-the-art temporal action proposal genera-
tion methods with high recall and high temporal precision. Finally, further ex-
periments demonstrate that by combining existing action classiﬁers, our method
signiﬁcantly improves the state-of-the-art temporal action detection performance.

Keywords: Temporal action proposal generation · Temporal action detection ·
Temporal convolution · Untrimmed video

1

Introduction

Nowadays, with fast development of digital cameras and Internet, the number of videos
is continuously booming, making automatic video content analysis methods widely re-
quired. One major branch of video analysis is action recognition, which aims to classify
manually trimmed video clips containing only one action instance. However, videos in
real scenarios are usually long, untrimmed and contain multiple action instances along
with irrelevant contents. This problem requires algorithms for another challenging task:
temporal action detection, which aims to detect action instances in untrimmed video
including both temporal boundaries and action classes. It can be applied in many areas
such as video recommendation and smart surveillance.

⋆ Corresponding author. This research has been supported by the funding from NSFC
(61673269, 61273285) and the Cooperative Medianet Innovation Center (CMIC).

2

Tianwei Lin et al.

Fig. 1: Overview of our approach. Given an untrimmed video, (1) we evaluate bound-
aries and actionness probabilities of each temporal location and generate proposals
based 
Burst Image Deblurring Using Permutation
Invariant Convolutional Neural Networks

Miika Aittala[0000−0003−0988−5397] and Fr´edo Durand

Massachusetts Institute of Technology, Cambridge MA 02139, USA

miika@csail.mit.edu, fredo@mit.edu

Abstract. We propose a neural approach for fusing an arbitrary-length
burst of photographs suﬀering from severe camera shake and noise into
a sharp and noise-free image. Our novel convolutional architecture has a
simultaneous view of all frames in the burst, and by construction treats
them in an order-independent manner. This enables it to eﬀectively de-
tect and leverage subtle cues scattered across diﬀerent frames, while en-
suring that each frame gets a full and equal consideration regardless
of its position in the sequence. We train the network with richly varied
synthetic data consisting of camera shake, realistic noise, and other com-
mon imaging defects. The method demonstrates consistent state of the
art burst image restoration performance for highly degraded sequences
of real-world images, and extracts accurate detail that is not discernible
from any of the individual frames in isolation.

Keywords: burst imaging · image processing · deblurring · denoising ·
convolutional neural networks

1

Introduction

Motion blur and noise remain a signiﬁcant problem in photography despite ad-
vances in light eﬃciency of digital imaging devices. Mobile phone cameras are
particularly suspect to handshake and noise due to the small optics and the typ-
ical unsupported free-hand shooting position. Shortcomings of optical systems
can be in part ameliorated by computational procedures such as denoising and
sharpening. One line of work that has recently had signiﬁcant impact relies on
burst imaging. A notable example is the imaging pipeline supplied in Android
mobile phones: transparently to the user, the camera shoots a sequence of low-
quality frames and fuses them computationally into a higher-quality photograph
than could be achieved with a conventional exposure in same time [12].
We address the problem of burst deblurring, where one is presented with a set
of images depicting the same target, each suﬀering from a diﬀerent realization of
camera shake. While each frame might be hopelessly blurred in isolation, they
still retain pieces of partial information about the underlying sharp image. The
aim is to recover it by fusing whatever information is available.
Convolutional neural networks (CNN’s) have led to breakthroughs in a wide
range of image processing tasks, and have also been applied to burst deblurring

2

M. Aittala and F. Durand

[34, 33]. Observing that bursts can have arbitrarily varying lengths, the recent
work of Wieschollek et al. [33] maintains an estimate of the sharp image, and up-
dates it in a recurrent manner by feeding in the frames one at a time. While this
is shown to produce good results, it is well known that recurrent architectures
struggle with learning to fuse information they receive over a n
C-WSL: Count-guided Weakly Supervised
Localization

Mingfei Gao1 , Ang Li2 ⋆ , Ruichi Yu1 , Vlad I. Morariu3⋆ , and Larry S. Davis1

1University of Maryland, College Park 2DeepMind

3Adobe Research

{mgao,richyu,lsd}@umiacs.umd.edu anglili@google.com

morariu@adobe.com

Abstract. We introduce count-guided weakly supervised localization
(C-WSL), an approach that uses per-class ob ject count as a new form
of supervision to improve weakly supervised localization (WSL). C-WSL
uses a simple count-based region selection algorithm to select high-quality
regions, each of which covers a single ob ject instance during training,
and improves existing WSL methods by training with the selected re-
gions. To demonstrate the eﬀectiveness of C-WSL, we integrate it into
two WSL architectures and conduct extensive experiments on VOC2007
and VOC2012. Experimental results show that C-WSL leads to large
improvements in WSL and that the proposed approach signiﬁcantly out-
performs the state-of-the-art methods. The results of annotation experi-
ments on VOC2007 suggest that a modest extra time is needed to obtain
per-class ob ject counts compared to labeling only ob ject categories in an
image. Furthermore, we reduce the annotation time by more than 2×
and 38× compared to center-click and bounding-box annotations.

Keywords: Weakly supervised localization · Count supervision.

1

Introduction

Convolutional neural networks (CNN) have achieved state-of-the-art performance
on the ob ject detection task [29, 23, 27, 28, 32, 21, 12, 20, 37, 33, 38, 39]. However,
these detectors are trained in a strongly supervised setting, requiring a large
number of bounding box annotations and huge amounts of human labor.
To ease the burden of human annotation, weakly supervised localization
(WSL) methods train a detector using weak supervision, e.g., image-level su-
pervision, instead of tight ob ject bounding boxes. The presence of an ob ject
category in an image can be obtained on the Internet nearly for free, so most
existing WSL architectures require only ob ject categories as supervision.
Existing methods [1, 3, 5, 15, 24, 36, 35, 19, 14, 40, 16, 30, 34] have proposed dif-
ferent architectures to address the WSL problem. However, there is still a large
performance gap between weakly and strongly supervised detectors [29, 28, 23]
on standard ob ject detection benchmarks [9, 10, 22]. Often, this is due to the lim-
ited information provided by ob ject-category supervision. One ma jor unsolved

⋆ The work was done while the author was at the University of Maryland

2

M. Gao, A. Li, R. Yu, V. I. Morariu and L. S. Davis

Training	phase

Testing	phase

Dog

WSL	Detector

(2,	Dog)

Proposals Detections

Region	
Selection

Training

Testing

Fig. 1. Given a set of ob ject proposals and the per-class ob ject count label, we select
high-quality positive regions (that tightly cover a single ob ject) to train a WSL detector.
Count information signiﬁcantly reduces detected bounding boxes that a
CAR-Net: Clairvoyant Attentive Recurrent Network

Amir Sadeghian1 , Ferdinand Legros1 ⋆ , Maxime Voisin1 ⋆ , Ricky Vesel2 ,
Alexandre Alahi3 , Silvio Savarese1

1 Stanford University, 2 Race Optimal,
3Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland

{amirabs,flegros,maxime.voisin,ssilvio}@stanford.edu;
vesel.rw@gmail.com; alexandre.alahi@epfl.ch

Abstract. We present an interpretable framework for path prediction that lever-
ages dependencies between agents’ behaviors and their spatial navigation envi-
ronment. We exploit two sources of information: the past motion trajectory of
the agent of interest and a wide top-view image of the navigation scene. We pro-
pose a Clairvoyant Attentive Recurrent Network (CAR-Net) that learns where to
look in a large image of the scene when solving the path prediction task. Our
method can attend to any area, or combination of areas, within the raw image
(e.g., road intersections) when predicting the trajectory of the agent. This allows
us to visualize ﬁne-grained semantic elements of navigation scenes that inﬂuence
the prediction of trajectories. To study the impact of space on agents’ trajectories,
we build a new dataset made of top-view images of hundreds of scenes (Formula
One racing tracks) where agents’ behaviors are heavily inﬂuenced by known ar-
eas in the images (e.g., upcoming turns). CAR-Net successfully attends to these
salient regions. Additionally, CAR-Net reaches state-of-the-art accuracy on the
standard trajectory forecasting benchmark, Stanford Drone Dataset (SDD). Fi-
nally, we show CAR-Net’s ability to generalize to unseen scenes.

1 Introduction

Path prediction consists in predicting the future positions of agents (e.g., humans or ve-
hicles) within an environment. It applies to a wide range of domains from autonomous
driving vehicles [1] and social robot navigation [2–4], to abnormal behavior detection
in surveillance [5–10]. Observable cues relevant to path prediction can be grouped into
dynamic and static information. The former captures the previous motion of all agents
within the scene (past trajectories). The latter consists of the static scene surrounding
agents [11–13]. In this work, we want to leverage the static scene context to perform
path prediction. The task is formulated as follows: given the past trajectory of an agent
(x-y coordinates of past few seconds) and a large visual image of the environment (top-
view of the scene), we want to forecast the trajectory of the agent over the next few
seconds. Our model should learn where to look within a large visual input to enhance
its prediction performance (see Fig. 1).
Predicting agents’ trajectories while taking into account the static scene context is
a challenging problem. It requires understanding complex interactions between agents
and space, and encoding these interactions into the path prediction model. Moreover,

⋆ indicates equal contribution

2

A. Sadeghian et al.

Sing le-source	
Attention

Multi-source	
Atte
CBAM: Convolutional Block Attention Module

Sanghyun Woo* 1 , Jongchan Park*†2 , Joon-Young Lee3 , and In So Kweon1

1 Korea Advanced Institute of Science and Technology, Daejeon, Korea

{shwoo93, iskweon77}@kaist.ac.kr

2 Lunit Inc., Seoul, Korea

jcpark@lunit.io

3 Adobe Research, San Jose, CA, USA

jolee@adobe.com

Abstract. We propose Convolutional Block Attention Module (CBAM),
a simple yet eﬀective attention module for feed-forward convolutional
neural networks. Given an intermediate feature map, our module se-
quentially infers attention maps along two separate dimensions, channel
and spatial, then the attention maps are multiplied to the input feature
map for adaptive feature reﬁnement. Because CBAM is a lightweight and
general module, it can be integrated into any CNN architectures seam-
lessly with negligible overheads and is end-to-end trainable along with
base CNNs. We validate our CBAM through extensive experiments on
ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets.
Our experiments show consistent improvements in classiﬁcation and de-
tection performances with various models, demonstrating the wide ap-
plicability of CBAM. The code and models will be publicly available.

Keywords: Ob ject recognition, attention mechanism, gated convolu-
tion

1

Introduction

Convolutional neural networks (CNNs) have signiﬁcantly pushed the perfor-
mance of vision tasks [1,2,3] based on their rich representation power. To en-
hance performance of CNNs, recent researches have mainly investigated three
important factors of networks: depth, width, and cardinality.
From the LeNet architecture [4] to Residual-style Networks [5,6,7,8] so far,
the network has become deeper for rich representation. VGGNet [9] shows that
stacking blocks with the same shape gives fair results. Following the same spirit,
ResNet [5] stacks the same topology of residual blocks along with skip connec-
tion to build an extremely deep architecture. GoogLeNet [10] shows that width
is another important factor to improve the performance of a model. Zagoruyko
and Komodakis [6] propose to increase the width of a network based on the
ResNet architecture. They have shown that a 28-layer ResNet with increased

*Both authors have equally contributed.
†The work was done while the author was at KAIST.

2

Woo, Park, Lee, Kweon

width can outperform an extremely deep ResNet with 1001 layers on the CI-
FAR benchmarks. Xception [11] and ResNeXt [7] come up with to increase the
cardinality of a network. They empirically show that cardinality not only saves
the total number of parameters but also results in stronger representation power
than the other two factors: depth and width.

Apart from these factors, we investigate a diﬀerent aspect of the architec-
ture design, attention. The signiﬁcance of attention has been studied extensively
in the previous literature [12,13,14,15,16,17]. Attention not only tells where to
focus, it also improves the representation of interests. Our goal 
CGIntrinsics: Better Intrinsic Image Decomposition
through Physically-Based Rendering

Zhengqi Li[0000−0003−2929−8149] and Noah Snavely[0000−0002−6921−6833]

Department of Computer Science & Cornell Tech, Cornell University

Abstract. Intrinsic image decomposition is a challenging, long-standing com-
puter vision problem for which ground truth data is very difﬁcult to acquire. We
explore the use of synthetic data for training CNN-based intrinsic image decompo-
sition models, then applying these learned models to real-world images. To that
end, we present CG IN TR IN S IC S, a new, large-scale dataset of physically-based
rendered images of scenes with full ground truth decompositions. The rendering
process we use is carefully designed to yield high-quality, realistic images, which
we ﬁnd to be crucial for this problem domain. We also propose a new end-to-end
training method that learns better decompositions by leveraging CG IN TR IN S IC S,
and optionally IIW and SAW, two recent datasets of sparse annotations on real-
world images. Surprisingly, we ﬁnd that a decomposition network trained solely
on our synthetic data outperforms the state-of-the-art on both IIW and SAW, and
performance improves even further when IIW and SAW data is added during
training. Our work demonstrates the suprising effectiveness of carefully-rendered
synthetic data for the intrinsic images task.

1

Introduction

Intrinsic images is a classic vision problem involving decomposing an input image I
into a product of reﬂectance (albedo) and shading images R · S . Recent years have seen
remarkable progress on this problem, but it remains challenging due to its ill-posedness.
An attractive proposition has been to replace traditional hand-crafted priors with learned,
CNN-based models. For such learning methods data is key, but collecting ground truth
data for intrinsic images is extremely difﬁcult, especially for images of real-world scenes.
One way to generate large amounts of training data for intrinsic images is to render
synthetic scenes. However, existing synthetic datasets are limited to images of single
objects [1, 2] (e.g., via ShapeNet [3]) or images of CG animation that utilize simpliﬁed,
unrealistic illumination (e.g., via Sintel [4]). An alternative is to collect ground truth
for real images using crowdsourcing, as in the Intrinsic Images in the Wild (IIW) and
Shading Annotations in the Wild (SAW) datasets [5, 6]. However, the annotations in
such datasets are sparse and difﬁcult to collect accurately at scale.
Inspired by recent efforts to use synthetic images of scenes as training data for indoor
and outdoor scene understanding [7–10], we present the ﬁrst large-scale scene-level
intrinsic images dataset based on high-quality physically-based rendering, which we call
CG IN TR IN S IC S (CG I). CG I consists of over 20,000 images of indoor scenes, based on
the SUNCG dataset [11]. Our aim with CG I is to help drive signiﬁcant progress towards
solving the intrinsic image
Characterizing Adversarial Examples Based on
Spatial Consistency Information for Semantic
Segmentation

Chaowei Xiao1 , Ruizhi Deng2 , Bo Li3,4 , Fisher Yu4 , Mingyan Liu1 , and Dawn Song4

1University of Michigan

2Simon Fraser University

3UIUC 4UC Berkeley

Abstract. Deep Neural Networks (DNNs) have been widely applied in
various recognition tasks. However, recently DNNs have been shown to
be vulnerable against adversarial examples, which can mislead DNNs to
make arbitrary incorrect predictions. While adversarial examples are well
studied in classiﬁcation tasks, other learning problems may have diﬀer-
ent properties. For instance, semantic segmentation requires additional
components such as dilated convolutions and multiscale processing. In
this paper, we aim to characterize adversarial examples based on spatial
context information in semantic segmentation. We observe that spatial
consistency information can be potentially leveraged to detect adversar-
ial examples robustly even when a strong adaptive attacker has access
to the model and detection strategies. We also show that adversarial
examples based on attacks considered within the paper barely transfer
among models, even though transferability is common in classiﬁcation.
Our observations shed new light on developing adversarial attacks and
defenses to better understand the vulnerabilities of DNNs.

Keywords: Semantic segmentation, adversarial example, spatial con-
sistency

1

Introduction

Deep Neural Networks (DNNs) have been shown to be highly expressive and have
achieved state-of-the-art performance on a wide range of tasks, such as speech
recognition [20], image classiﬁcation [24], natural language understanding [54],
and robotics [32]. However, recent studies have found that DNNs are vulnerable
to adversarial examples [38,17,31,47,45,40,9,8,7]. Such examples are intentionally
perturbed inputs with small magnitude adversarial perturbation added, which
can induce the network to make arbitrary incorrect predictions at test time,
even when the examples are generated against diﬀerent models [27,5,33,46]. The
fact that the adversarial perturbation required to fool a model is often small
and (in the case of images) imperceptible to human observers makes detecting
such examples very challenging. This undesirable property of deep networks has
become a ma jor security concern in real-world applications of DNNs, such as self-
driving cars and identity recognition systems [16,37]. Furthermore, both white-
box and black-box attacks have been performed against DNNs successfully when

2

Xiao et al.

an attacker is given full or zero knowledge about the target systems [2,17,45].
Among black-box attacks, transferability is widely used for generating attacks
against real-world systems which do not allow white-box access. Transferability
refers to the property of adversarial examples in classiﬁcation tasks where one
adversarial example generated against a local model can mislead another unseen
model 
Choose Your Neuron: Incorporating Domain
Knowledge through Neuron-Importance

Ramprasaath R. Selvara ju1 † ∗ , Prithvijit Chattopadhyay1∗ ,
Mohamed Elhoseiny2 , Tilak Sharma2 , Dhruv Batra1,2 ,
Devi Parikh1,2 , and Stefan Lee1

1Georgia Institute of Technology

2Facebook

{ramprs,prithvijit3,dbatra,parikh,steflee}@gatech.edu
{elhoseiny,tilaksharma,dbatra,parikh}@fb.com

Abstract. Individual neurons in convolutional neural networks super-
vised for image-level classiﬁcation tasks have been shown to implicitly
learn semantically meaningful concepts ranging from simple textures and
shapes to whole or partial ob jects – forming a “dictionary” of concepts
acquired through the learning process. In this work we introduce a simple,
eﬃcient zero-shot learning approach based on this observation. Our ap-
proach, which we call Neuron Importance-Aware Weight Transfer (NIWT),
learns to map domain knowledge about novel “ unseen ” classes onto this
dictionary of learned concepts and then optimizes for network parameters
that can eﬀectively combine these concepts – essentially learning classi-
ﬁers by discovering and composing learned semantic concepts in deep
networks. Our approach shows improvements over previous approaches
on the CUBirds and AWA2 generalized zero-shot learning benchmarks.
We demonstrate our approach on a diverse set of semantic inputs as
external domain knowledge including attributes and natural language
captions. Moreover by learning inverse mappings, NIWT can provide
visual and textual explanations for the predictions made by the newly
learned classiﬁers and provide neuron names. Our code is available at

https://github.com/ramprs/neuron- importance- zsl.

Keywords: Zero Shot Learning · Interpretability · Grad-CAM

1

Introduction

† Deep neural networks have pushed the boundaries of standard classiﬁcation
tasks in the past few years, with performance on many challenging benchmarks
reaching near human-level accuracies. One caveat however is that these deep
models require massive labeled datasets – failing to generalize from few examples
or descriptions of unseen classes like humans can. To close this gap, the task
of learning deep classiﬁers for unseen classes from external domain knowledge

∗Equal Contribution
†Work done partly at Facebook

2

Ramprasaath R. Selvara ju and P. Chattopadhyay

Fig. 1: We present our Neuron Importance-Aware Weight Transfer (NIWT) approach
which maps free-form domain knowledge about unseen classes to relevant concept-
sensitive neurons within a pretrained deep network. We then optimize the weights of
a novel classiﬁer such that the activation of this set of neurons results in high output
scores for the unseen classes in the generalized zero-shot learning setting.

alone – termed zero-shot learning (ZSL) – has been the topic of increased interest
within the community [17,16,10,21,29,36,31,2,11,3,25,5,14].
As humans, much of the way we acquire and transfer knowledge about novel
concepts is in reference to or via c
CIRL: Controllable Imitative Reinforcement Learning
for Vision-based Self-driving

Xiaodan Liang1,2 , Tairui Wang1 , Luona Yang2 , and Eric Xing1,2

1 Petuum Inc, tairui.wang@petuum.com
2 Carnegie Mellon University, {luonay1, xiaodan1, epxing}@cs.cmu.edu

Abstract. Autonomous urban driving navigation with complex multi-agent dy-
namics is under-explored due to the difﬁculty of learning an optimal driving pol-
icy. The traditional modular pipeline heavily relies on hand-designed rules and the
pre-processing perception system while the supervised learning-based models are
limited by the accessibility of extensive human experience. We present a general
and principled Controllable Imitative Reinforcement Learning (CIRL) approach
which successfully makes the driving agent achieve higher success rates based
on only vision inputs in a high-ﬁdelity car simulator. To alleviate the low explo-
ration efﬁciency for large continuous action space that often prohibits the use of
classical RL on challenging real tasks, our CIRL explores over a reasonably con-
strained action space guided by encoded experiences that imitate human demon-
strations, building upon Deep Deterministic Policy Gradient (DDPG). Moreover,
we propose to specialize adaptive policies and steering-angle reward designs for
different control signals (i.e. follow, straight, turn right, turn left) based on the
shared representations to improve the model capability in tackling with diverse
cases. Extensive experiments on CARLA driving benchmark demonstrate that
CIRL substantially outperforms all previous methods in terms of the percentage
of successfully completed episodes on a variety of goal-directed driving tasks.
We also show its superior generalization capability in unseen environments. To
our knowledge, this is the ﬁrst successful case of the learned driving policy by
reinforcement learning in the high-ﬁdelity simulator, which performs better than
supervised imitation learning.

Keywords: Imitative reinforcement learning, Autonomous driving

1

Introduction

Autonomous urban driving is a long-studied and still under-explored task [27, 31] par-
ticularly in the crowded urban environments [25]. A desirable system is required to be
capable of solving all visual perception tasks (e.g. object and lane localization, drivable
paths) and determining long-term driving strategies, referred as “driving policy”. Al-
though visual perception tasks have been well studied by resorting to supervised learn-
ing on large-scale datasets [39, 20], simplistic driving policies by manually designed
rules in the modular pipeline is far from sufﬁcient for handling diverse real-world cases
as discussed in [30, 28]. Learning a optimal driving policy that mimics human drivers
is less explored but key to navigate in complex environments that requires understand-
ing of multi-agent dynamics, prescriptive trafﬁc rule, negotiation skills for taking left

2

X. Liang, T. Wang, L. Yang and E. Xing

Fig. 1. An overview of our
Clustering Convolutional Kernels to Compress
Deep Neural Networks

Sanghyun Son, Seungjun Nah, and Kyoung Mu Lee

Department of ECE, ASRI, Seoul National University, 08826, Seoul, Korea

thstkdgus35@snu.ac.kr, seungjun.nah@gmail.com, kyoungmu@snu.ac.kr

Abstract. In this paper, we propose a novel method to compress CNNs
by reconstructing the network from a small set of spatial convolution
kernels. Starting from a pre-trained model, we extract representative
2D kernel centroids using k-means clustering. Each centroid replaces the
corresponding kernels of the same cluster, and we use indexed repre-
sentations instead of saving whole kernels. Kernels in the same cluster
share their weights, and we ﬁne-tune the model while keeping the com-
pressed state. Furthermore, we also suggest an eﬃcient way of removing
redundant calculations in the compressed convolutional layers. We ex-
perimentally show that our technique works well without harming the
accuracy of widely-used CNNs. Also, our ResNet-18 even outperforms its
uncompressed counterpart at ILSVRC2012 classiﬁcation task with over
10x compression ratio.

Keywords: CNNs, Compression, Quantization, Weight sharing, Clus-
tering

1

Introduction

The recent era of computer vision witnessed remarkable advances from deep
learning. The analysis presented in [35] shows that CNNs not only ﬁgure out the
scene types but also well recognizes spatial patterns. Therefore, state-of-the-art
convolutional neural networks [15, 16, 31] and their variants apply to a broad
range of problems such as image classiﬁcation, ob ject detection, segmentation,
image restoration, etc. However, most of the CNNs are designed to be executed
on high-end GPUs with substantial memory and computational power. In mobile
or embedded environments where computational resources are limited, those
networks need to be compressed for practical applications [12, 34].
Most of the studies on network compression have investigated to ﬁgure out
redundancies of weights [6] and unnecessary parameters [14, 24]. Then, those pa-
rameters can be removed while preserving the original performance of the model.
In [7, 20], the weight matrices and tensors were factorized and approximated to
low-rank for eﬃcient computation. Pruning became popular in recent works [10,
13, 26, 27] since it directly saves storage and computations. On the other side,
the quantization based approaches have also become common. Extensive studies
on binary and ternary networks [4, 5, 12, 17, 25, 29, 39] proved that even 1 or 2

2

S. Son, S. Nah and K. M. Lee

bits parameters can make CNNs work. Other works tried to quantize adjacent
elements of a weight tensor as a form of vector quantization [9, 34]. Those meth-
ods utilize k-means clustering so that we can express the compressed model in
the form of codebook and indices. Especially, [34] extracted a sub-vector from
the weight tensor in channel dimension to exploit product quantization.
In this paper, we propose a more structured qua
CNN-PS: CNN-based Photometric Stereo for
General Non-Convex Surfaces

Satoshi Ikehata

National Institute of Informatics, Tokyo, Japan

sikehata@nii.ac.jp

Abstract. Most conventional photometric stereo algorithms inversely
solve a BRDF-based image formation model. However, the actual imag-
ing process is often far more complex due to the global light transport on
the non-convex surfaces. This paper presents a photometric stereo net-
work that directly learns relationships between the photometric stereo
input and surface normals of a scene. For handling unordered, arbitrary
number of input images, we merge all the input data to the intermediate
representation called observation map that has a ﬁxed shape, is able to
be fed into a CNN. To improve both training and prediction, we take
into account the rotational pseudo-invariance of the observation map
that is derived from the isotropic constraint. For training the network,
we create a synthetic photometric stereo dataset that is generated by a
physics-based renderer, therefore the global light transport is considered.
Our experimental results on both synthetic and real datasets show that
our method outperforms conventional BRDF-based photometric stereo
algorithms especially when scenes are highly non-convex.

Keywords: photometric stereo, convolutional neural networks

1

Introduction

In 3-D computer vision problems, the input data is often unstructured (i.e., the
number of input images is varying and the images are unordered). A good exam-
ple is the multi-view stereo problem where the scene geometry is recovered from
unstructured multi-view images. Due to this unstructuredness, 3-D reconstruc-
tion from multiple images less relied on the supervised learning-based algorithms
except for some structured problems such as binocular stereopsis [1] and two-view
SfM [2] whose number of input images is always ﬁxed. However, recent advances
in deep convolutional neural network (CNN) have motivated researchers to ad-
dress unstructured 3-D computer vision problems with deep neural networks.
For instance, a recent work from Kar et al. [3] presented an end-to-end learned
system for the multi-view stereopsis while Kim et al. [4] presented a learning-
based surface reﬂectance estimation from multiple RGB-D images. Either work
intelligently merged all the unstructured input to a structured, intermediate
representation (i.e., 3-D feature grid [3] and 2-D hemispherical image [4]).

This work was supported by JSPS KAKENHI Grant Number JP17H07324.

2

S. Ikehata

Photometric stereo is another 3-D computer vision problem whose input is
unstructured, where surface normals of a scene are recovered from appearance
variations under diﬀerent illuminations. Photometric stereo algorithms typically
solved an inverse problem of the pointwise image formation model which was
based on the Bidirectional Reﬂectance Distribution Function (BRDF). While
eﬀective, a BRDF-based image formation model generally cannot account the
global
Coded Two-Bucket Cameras for Computer Vision

Mian Wei1 , Navid Sarhangnejad2, Zhengfan Xia2 , Nikita Gusev2 , Nikola Katic2 ,
Roman Genov2, and Kiriakos N. Kutulakos1

1 Department of Computer Science, University of Toronto, Canada

{mianwei,kyros}@cs.toronto.edu

2 Department of Electrical Engineering, University of Toronto, Canada

{sarhangn,xia,nikita,roman}@ece.toronto.edu, katic.nik@gmail.com

Abstract. We introduce coded two-bucket (C2B) imaging, a new operating prin-
ciple for computational sensors with applications in active 3D shape estimation
and coded-exposure imaging. A C2B sensor modulates the light arriving at each
pixel by controlling which of the pixel’s two “buckets” should integrate it. C2B
sensors output two images per video frame—one per bucket—and allow rapid,
fully-programmable, per-pixel control of the active bucket. Using these properties
as a starting point, we (1) develop an image formation model for these sensors,
(2) couple them with programmable light sources to acquire illumination mo-
saics, i.e., images of a scene under many different illumination conditions whose
pixels have been multiplexed and acquired in one shot, and (3) show how to pro-
cess illumination mosaics to acquire live disparity or normal maps of dynamic
scenes at the sensor’s native resolution. We present the ﬁrst experimental demon-
stration of these capabilities, using a fully-functional C2B camera prototype. Key
to this unique prototype is a novel programmable CMOS sensor that we designed
from the ground up, fabricated and turned into a working system.

1 Introduction

New camera designs—and new types of imaging sensors—have been instrumental in
driving the ﬁeld of computer vision in exciting new directions. In the last decade alone,
time-of-ﬂight cameras [1, 2] have been widely adopted for vision [3] and computational
photography tasks [4–7]; event cameras [8] that support asynchronous imaging have
led to new vision techniques for high-speed motion analysis [9] and 3D scanning [10];
high-resolution sensors with dual-pixel [11] and assorted-pixel [12] designs are deﬁning
the state of the art for smartphone cameras; and sensors with pixel-wise coded-exposure
capabilities are starting to appear [13, 14] for compressed sensing applications [15].

Against this backdrop, we introduce a new type of computational video camera to the
vision community—the coded two-bucket (C2B) camera (Fig. 1). The C2B camera is
a pixel-wise coded-exposure camera that never blocks the incident light. Instead, each
pixel in its sensor contains two charge-collection sites—two “buckets”—as well as a
one-bit writeable memory that controls which bucket is active. The camera outputs two
images per video frame—one per bucket—and performs exposure coding by rapidly
controlling the active bucket of each pixel, via a programmable sequence of binary 2D
patterns. Key to this unique functionality is a novel programmable CMOS sensor that

2

M. Wei et al.

C2B camera prototype

time-v
Collaborative Deep Reinforcement Learning for
Multi-Ob ject Tracking

Liangliang Ren1 , Jiwen Lu1 ⋆ , Zifeng Wang1 , Qi Tian2,3 ,Jie Zhou1

1Tsinghua University, Beijing China; 2Huawei Noah‘S Ark Lab; 3 UNiversity of Texas
at San Antonio

renll16@mails.tsinghua.edu.cn, lujiwen@tsinghua.edu.cn,
wangzf14@mails.tsinghua.edu.cn, qi.tian@utsa.edu jzhou@tsinghua.edu.cn

Abstract. In this paper, we propose a collaborative deep reinforcement
learning (C-DRL) method for multi-ob ject tracking. Most existing multi-
ob ject tracking methods employ the tracking-by-detection strategy which
ﬁrst detects ob jects in each frame and then associates them across dif-
ferent frames. However, the performance of these methods rely heavily
on the detection results, which are usually unsatisﬁed in many real ap-
plications, especially in crowded scenes. To address this, we develop a
deep prediction-decision network in our C-DRL, which simultaneously
detects and predicts ob jects under a uniﬁed network via deep reinforce-
ment learning. Speciﬁcally, we consider each ob ject as an agent and track
it via the prediction network, and seek the optimal tracked results by
exploiting the collaborative interactions of diﬀerent agents and environ-
ments via the decision network.Experimental results on the challenging
MOT15 and MOT16 benchmarks are presented to show the eﬀectiveness
of our approach.

Keywords: Ob ject tracking, multi-ob ject, deep reinforcement learning

1

Introduction

Multi-ob ject tracking (MOT) has attracted increasing interests in computer vi-
sion over the past few years, which has various practical applications in surveil-
lance, human computer interface, robotics and advanced driving assistant sys-
tems. The goal of MOT is to estimate the tra jectories of diﬀerent ob jects and
track those ob jects across the video. While a variety of MOT methods have been
proposed in recent years [7, 8, 14, 27, 34, 36, 40, 45–47, 52], it remains a challenging
problem to track multiple ob jects in many unconstrained environments, espe-
cially in crowded scenes. This is because occlusions between diﬀerent ob jects
and large intra-class variations usually occur in such scenarios.
Existing MOT approaches can be mainly divided into two categories, 1) of-
ﬂine (batch or semi-batch) [7, 27, 40, 45, 46, 52] and 2) online [8, 14, 34, 36, 47]. The
key idea of oﬄine methods is to group detections into short tra jectory segments

⋆ Corresponding author.

2

Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian,Jie Zhou

Tracking result Frame t

t

t+1

1
n
o

s
r

e
P

2
n
o

s
r

e
P

Q-Net

Q-Net

Action 
Selec t

Action 
Selec t

Q-Net

Q-Net

Action 
Selec t

Action 
Selec t

Predictions & Detections

Environment

Tracking result Frame t+1

Fig. 1: The key idea of our proposed C-DRL method for multi-ob ject tracking.
Given a video and the detection results of diﬀerent ob jects for the tth frame,
we model each ob ject as an agent and predict the location of each ob ject for
the following f
Combining 3D Model Contour Energy
and Keypoints for Ob ject Tracking

Bogdan Bugaev1,2 [0000−0002−0994−1486] ,
Anton Kryshchenko1,2 [0000−0003−0088−1496] , and
Roman Belov3 [0000−0002−5481−6684]

1 National Research University Higher School of Economics, St. Petersburg, Russia
2 Saint Petersburg Academic University, St. Petersburg, Russia

{bogdan.bugaev,a.s.kryshchenko}@gmail.com

3 KeenTools, St. Petersburg, Russia

belovrv@gmail.com

Abstract. We present a new combined approach for monocular model-
based 3D tracking. A preliminary ob ject pose is estimated by using a
keypoint-based technique. The pose is then reﬁned by optimizing the
contour energy function. The energy determines the degree of correspon-
dence between the contour of the model pro jection and the image edges.
It is calculated based on both the intensity and orientation of the raw
image gradient. For optimization, we propose a technique and search
area constraints that allow overcoming the local optima and taking into
account information obtained through keypoint-based pose estimation.
Owing to its combined nature, our method eliminates numerous issues
of keypoint-based and edge-based approaches. We demonstrate the eﬃ-
ciency of our method by comparing it with state-of-the-art methods on
a public benchmark dataset that includes videos with various lighting
conditions, movement patterns, and speed.

Keywords: 3D Tracking · Monocular · Model-based · Pose estimation

1

Introduction

Monocular model-based 3D tracking methods are an essential part of computer
vision. They are applied in a wide range of practical areas, from augmented
reality to visual eﬀects in cinema. 3D tracking implies iterative, frame-by-frame
estimation of an ob ject’s position and orientation relative to the camera, with a
given initial ob ject pose. Fig. 1a shows a scene fragment typical for such a task.
A number of characteristics complicate tracking: the ob ject is partially occluded,
there are ﬂecks and reﬂections, and the background is cluttered.
In recent years, a great number of 3D tracking methods have been developed.
These can be classiﬁed by the image characteristics and 3D model features used
for pose detection. Many approaches [12,26,18,19] are based on calculating key-
points [13,22] on the image and corresponding points on the 3D model. Such
methods make it possible to achieve high performance and robustness against

2

B. Bugaev, A. Kryshchenko, R. Belov

(a)

(b)

(c)

Fig. 1. An example of a tracking algorithm applied to a single frame. (a) A fragment
of the processed frame. Despite the partial occlusions of the tracked ob ject (violin ),
most of its edges are visible. (b) The result of preliminary pose estimation. The model
pro jection (white wireframe ) does not coincide with the ob ject image in the frame
because the position of the keypoints (black crosses ) used to determine its position was
inaccurately calculated. (c) The ob ject’s model with optimized energy 4 of contours

(purple line
Comparator Networks

Weidi Xie, Li Shen and Andrew Zisserman

Visual Geometry Group, Department of Engineering Science
University of Oxford

{weidi,lishen,az}@robots.ox.ac.uk

Abstract. The ob jective of this work is set-based veriﬁcation, e.g. to
decide if two sets of images of a face are of the same person or not. The
traditional approach to this problem is to learn to generate a feature
vector per image, aggregate them into one vector to represent the set,
and then compute the cosine similarity between sets. Instead, we design a
neural network architecture that can directly learn set-wise veriﬁcation.
Our contributions are: (i) We propose a Deep Comparator Network (DCN)
that can ingest a pair of sets (each may contain a variable number of im-
ages) as inputs, and compute a similarity between the pair – this involves
attending to multiple discriminative local regions (landmarks), and com-
paring local descriptors between pairs of faces; (ii) To encourage high-
quality representations for each set, internal competition is introduced
for recalibration based on the landmark score; (iii) Inspired by image
retrieval, a novel hard sample mining regime is proposed to control the
sampling process, such that the DCN is complementary to the standard
image classiﬁcation models. Evaluations on the IARPA Janus face recog-
nition benchmarks show that the comparator networks outperform the
previous state-of-the-art results by a large margin.

1

Introduction

The ob jective of this paper is to determine if two sets of images are of the same
ob ject or not. For example, in the case of face veriﬁcation, the set could be
images of a face; and in the case of person re-identiﬁcation, the set could be
images of the entire person. In both cases the ob jective is to determine if the
sets show the same person or not.
In the following, we will use the example of sets of faces, which are usually
referred to as ‘templates’ in the face recognition literature, and we will use this
term from here on. A template could consist of multiple samples of the same
person (e.g. still images, or frames from a video of the person, or a mixture of
both). With the great success of deep learning for image classiﬁcation [1–4], by
far the most common approach to template-based face veriﬁcation is to generate
a vector representing each face using a deep convolutional neural network (CNN),
and simply average these vectors to obtain a vector representation for the entire
template [5–8]. Veriﬁcation then proceeds by comparing the template vectors
with some similarity metrics, e.g. cosine similarity. Rather than improve on this
simple combination rule, the research drives until now has been to improve the

2

Weidi Xie, Li Shen and Andrew Zisserman

performance of the single image representation by more sophisticated training
losses, such as Triplet Loss, PDDM, and Histogram Loss [6, 7, 9–12]. This ap-
proach has achieved very impressive results on the challenging benchmarks, such
as the IARPA 
Compositing-aware Image Search

Hengshuang Zhao1⋆ , Xiaohui Shen2 , Zhe Lin3 ,
Kalyan Sunkavalli3 , Brian Price3 , Jiaya Jia1,4

1The Chinese University of Hong Kong, 2ByteDance AI Lab,
3Adobe Research, 4 Tencent Youtu Lab

{hszhao,leojia}@cse.cuhk.edu.hk, shenxiaohui@bytedance.com,
{zlin,sunkaval,bprice}@adobe.com

Abstract. We present a new image search technique that, given a back-
ground image, returns compatible foreground ob jects for image com-
positing tasks. The compatibility of a foreground ob ject and a back-
ground scene depends on various aspects such as semantics, surrounding
context, geometry, style and color. However, existing image search tech-
niques measure the similarities on only a few aspects, and may return
many results that are not suitable for compositing. Moreover, the impor-
tance of each factor may vary for diﬀerent ob ject categories and image
content, making it diﬃcult to manually deﬁne the matching criteria. In
this paper, we propose to learn feature representations for foreground
ob jects and background scenes respectively, where image content and
ob ject category information are jointly encoded during training. As a
result, the learned features can adaptively encode the most important
compatibility factors. We pro ject the features to a common embedding
space, so that the compatibility scores can be easily measured using the
cosine similarity, enabling very eﬃcient search. We collect an evaluation
set consisting of eight ob ject categories commonly used in compositing
tasks, on which we demonstrate that our approach signiﬁcantly outper-
forms other search techniques.

1

Introduction

Image compositing is a fundamental task in photo editing and graphic design,
in which foreground ob jects and background scenes from diﬀerent sources are
blended together to generate new composites. While previous work has consid-
ered the problem of rendering realistic composites [1–5] when the foreground and
background images are given, users often ﬁnd it challenging and time-consuming
to ﬁnd compatible foreground and background images to begin with.
Speciﬁcally, a foreground is considered compatible with the background if
they roughly match in terms of semantics, viewpoint, style, color, etc., so that
realistic composites can be generated with a reasonable amount of subsequent

⋆This work was partly done when H. Zhao was an intern at Adobe Research.

2

H. Zhao, X. Shen, Z. Lin, K. Sunkavalli, B. Price, J. Jia

Fig. 1: Compositing-aware image search. Given a background image as a query, the
task is to ﬁnd foreground ob jects of a certain category that can be composited into the
background at a speciﬁc location, as indicated by the rectangle.

editing. For example in Fig. 1, a user intends to insert a person standing on
the street at the location indicated by the yellow box. With the foreground in
the green box, a realistic image can be rendered (Fig. 1(b)) by adjusting the
color and adding a shadow. On the other hand, when given a
Compositional Learning
for Human Ob ject Interaction

Keizo Kato1 ⋆ , Yin Li2 , and Abhinav Gupta2

1 Fujitsu Laboratories Ltd.

kato.keizo@jp.fujitsu.com

2 Carnegie Mellon University

yinl2@andrew.cmu.edu, abhinavg@cs.cmu.edu

Abstract. The world of human-ob ject interactions is rich. While gener-
ally we sit on chairs and sofas, if need be we can even sit on TVs or top of
shelves. In recent years, there has been progress in modeling actions and
human-ob ject interactions. However, most of these approaches require
lots of data. It is not clear if the learned representations of actions are
generalizable to new categories. In this paper, we explore the problem of
zero-shot learning of human-ob ject interactions. Given limited verb-noun
interactions in training data, we want to learn a model than can work
even on unseen combinations. To deal with this problem, In this paper,
we propose a novel method using external knowledge graph and graph
convolutional networks which learns how to compose classiﬁers for verb-
noun pairs. We also provide benchmarks on several dataset for zero-shot
learning including both image and video. We hope our method, dataset
and baselines will facilitate future research in this direction.

1

Introduction

Our daily actions and activities are rich and complex. Consider the examples
in Fig 1(a). The same verb “sit” is combined with diﬀerent nouns (chair, bed,
ﬂoor) to describe visually distinctive actions (“sit on chair” vs. “sit on ﬂoor”).
Similarly, we can interact with the same ob ject (TV) in many diﬀerent ways
(turn on, clean, watch). Even small sets of common verbs and nouns will create
a huge combination of action labels. It is highly unlikely that we can capture
action samples covering all these combinations. What if we want to recognize an
action category that we had never seen before, e.g., the one in Fig 1(b)?
This problem is known as zero shot learning, where categories at testing time
are not presented during training. It has been widely explored for ob ject recog-
nition [31, 1, 11, 12, 15, 37, 60]. And there is an emerging interest for zero-shot
action recognition [55, 24, 21, 51, 35, 18]. How are actions diﬀerent from ob jects
in zero shot learning? What we know is that human actions are naturally compo-
sitional and humans have amazing ability to achieve similar goals with diﬀerent
ob jects and tools. For example, while one can use hammer for the hitting the

⋆ Work was done when K. Kato was at CMU.

2

K. Kato, Y. Li and A. Gupta

Sit on table, chair, floor, …

Sit on TV

?

watch, carry,  turn on, … TV

(a)

(b)

(c)

Fig. 1. (a-b) many of our daily actions are compositional . These actions can be
described by motion (verbs) and the ob jects (nouns). We build on this composition
for zero shot recognition of human-ob ject interactions. Our method encodes motion
and ob ject cues as visual embeddings of verbs (e.g., sit) and nouns (e.g., TV), uses
external knowledge for learning to assemble these embeddings 
Compound Memory Networks for
Few-shot Video Classiﬁcation

Linchao Zhu and Yi Yang

CAI, University of Technology Sydney, NSW

Linchao.Zhu@student.uts.edu.au; Yi.Yang@uts.edu.au

Abstract. In this paper, we propose a new memory network structure
for few-shot video classiﬁcation by making the following contributions.
First, we propose a compound memory network (CMN) structure un-
der the key-value memory network paradigm, in which each key memory
involves multiple constituent keys. These constituent keys work collabo-
ratively for training, which enables the CMN to obtain an optimal video
representation in a larger space. Second, we introduce a multi-saliency
embedding algorithm which encodes a variable-length video sequence
into a ﬁxed-size matrix representation by discovering multiple saliencies
of interest. For example, given a video of car auction, some people are
interested in the car, while others are interested in the auction activities.
Third, we design an abstract memory on top of the constituent keys. The
abstract memory and constituent keys form a layered structure, which
makes the CMN more eﬃcient and capable of being scaled, while also
retaining the representation capability of the multiple keys. We compare
CMN with several state-of-the-art baselines on a new few-shot video
classiﬁcation dataset and show the eﬀectiveness of our approach.

Keywords: Few-Shot Video Learning · Video Classiﬁcation · Memory-
augmented Neural Networks · Compound Memory Networks

1

Introduction

Deep learning models have been successfully applied to many tasks, e.g., image
classiﬁcation [16, 25, 29, 8], image detection [22], video classiﬁcation [12, 24] and
machine translation [28, 36]. Convolutional Neural Networks (ConvNets) and
Recurrent Neural Networks (RNNs) have become built-in modules in various
ﬁelds. However, large amounts of labeled training data are required to train a
deep neural network. To adapt an existing model to recognize a new category
which was unseen during training, it may be necessary to manually collect hun-
dreds of new training samples. Such a procedure is rather tedious and labor
intensive, especially when there are many new categories. There is an increasing
need to learn a classiﬁcation model from a few examples in a life-long manner,
which is also known as the few-shot learning task [23, 32].
In a few-shot recognition setting, the network needs to eﬀectively learn clas-
siﬁers for novel concepts from only a few examples. Unlike traditional models

2

L. Zhu and Y. Yang

Meta-training	set

Training episode	1

Training episode	2

Meta-testing	set

Testing episode	1

Support	set

Support	set

Support	set

Training	example	1

Making	a	cake

Training	example	1

Kissing

Training	example	1

Playing	basketball

Training	example	2

Auctioning

Training	example	2

Walking	the	dog

…

Training	example	2

Playing	drums

…

Training	example	3

Unboxing

Training	example	3

Golf	chipping

Training	example	3

Welding

Query

Query

Query

(c
Compressing the Input for CNNs with the
First-Order Scattering Transform

Edouard Oyallon,1,4,5 Eugene Belilovsky,2 Sergey Zagoruyko,3 Michal Valko4

1CentraleSupelec, Universit´e Paris-Saclay
2MILA, University of Montreal
3WILLOW – Inria Paris, 4SequeL – Inria Lille, 5GALEN – Inria Saclay

Abstract. We study the ﬁrst-order scattering transform as a candi-
date for reducing the signal processed by a convolutional neural network
(CNN). We show theoretical and empirical evidence that in the case of
natural images and suﬃciently small translation invariance, this trans-
form preserves most of the signal information needed for classiﬁcation
while substantial ly reducing the spatial resolution and total signal size.
We demonstrate that cascading a CNN with this representation per-
forms on par with ImageNet classiﬁcation models, commonly used in
downstream tasks, such as the ResNet-50. We subsequently apply our
trained hybrid ImageNet model as a base model on a detection system,
which has typically larger image inputs. On Pascal VOC and COCO de-
tection tasks we demonstrate improvements in the inference speed and
training memory consumption compared to models trained directly on
the input image.

Keywords: CNN, SIFT, image descriptors, ﬁrst-order scattering

1

Introduction

Convolutional neural networks (CNNs) for supervised vision tasks learn often
from raw images [1] that could be arbitrarily large. Eﬀective reduction of the
spatial dimension and total signal size for CNN processing is diﬃcult. One way
is to learn this dimensionality reduction during the training of a supervised
CNN. Indeed, the very ﬁrst layers of standard CNNs play often this role and
reduce the spatial resolution of an image via pooling or stride operators. Yet,
they generally maintain the input layer sizes and even increase it by expanding
the number of channels. These pooling functions can correspond to a linear
pooling such as wavelet pooling [2], a spectral pooling [3], an average pooling, or
a non-linear pooling such as ℓ2 -pooling [4], or max-pooling. For example, the two
ﬁrst layers of an AlexNet [5], a VGG [6] or a ResNet [7] reduce the resolution
respectively by 23 , 21 , and 22 , while the dimensionality of the layer is increased
by a factor 1.2, 5.3, and 1.3 respectively. This spatial size reduction is important
for computational reasons because the complexity of convolutions is quadratic
in spatial size while being linear in the number of channels. This suggests that
reducing the input size to subsequent CNN layers calls for a careful design. In

2

E. Oyallon, E. Belilovsky, S. Zagoruyko, M. Valko

this work, we (a) analyze a generic method that, without learning, reduces input

size as well as resolution and (b) show that it retains enough information and

structure that permits applying a CNN to obtain competitive performance on
classiﬁcation and detection.

Natural images have a lot of redundancy, that can be exploited by ﬁnding a
frame to obtain a sparse represe
Conditional Image-Text Embedding Networks

Bryan A. Plummer† , Paige Kordas† , M. Hadi Kiapour‡ , Shuai Zheng‡ ,
Robinson Piramuthu‡ , and Svetlana Lazebnik†

University of Illinois at Urbana-Champaign†

{bplumme2,pkordas2,slazebni}@illinois.edu

Ebay Inc.‡

{mkiapour,shuzheng,rpiramuthu}@ebay.com

Abstract. This paper presents an approach for grounding phrases in
images which jointly learns multiple text-conditioned embeddings in a
single end-to-end model. In order to diﬀerentiate text phrases into seman-
tically distinct subspaces, we propose a concept weight branch that auto-
matically assigns phrases to embeddings, whereas prior works predeﬁne
such assignments. Our proposed solution simpliﬁes the representation
requirements for individual embeddings and allows the underrepresented
concepts to take advantage of the shared representations before feed-
ing them into concept-speciﬁc layers. Comprehensive experiments verify
the eﬀectiveness of our approach across three phrase grounding datasets,
Flickr30K Entities, ReferIt Game, and Visual Genome, where we obtain
a (resp.) 4%, 3%, and 4% improvement in grounding performance over
a strong region-phrase embedding baseline1 .

Keywords: Natural language grounding, phrase localization, embed-
ding methods, conditional models

1

Introduction

Phrase grounding attempts to localize a given natural language phrase in an
image. This constituent task has applications to image captioning [6, 12, 14, 19,
34], image retrieval [9, 26], and visual question answering [1, 29, 7]. Research on
phrase grounding has been spurred by the release of several datasets, some of
which primarily contain relatively short phrases [15, 18], while others contain
longer queries, including entire sentences that can provide rich context [25, 22].
The diﬀerence in query length compounds the already challenging problem of
generalizing to any (including never before seen) natural language input. Despite
this, much of the recent attention has focused on learning a single embedding
model between image regions and phrases [7, 22, 10, 28, 31, 32, 35, 21].
In this paper, we propose a Conditional Image-Text Embedding (CITE) net-
work that jointly learns diﬀerent embeddings for subsets of phrases (Figure 1).
This enables our model to train separate embeddings for phrases that share a
concept. Each conditional embedding can learn a representation speciﬁc to a

1 Code: https://github.com/BryanPlummer/cite

2

B. A. Plummer et al .

Concept Weight Branch

Embedding assignments known?

No

E1

ReLU

E2

Softmax

Yes

(cid:862)A s(cid:373)ili(cid:374)g 

bearded 

(cid:373)a(cid:374)(cid:863)

word2vec + 
fisher vector

T1

ReLU

T2

L2 
Norm

C1

Elementwise 
Product

P1

ReLU

C2

Embedding 
Fusion

Logistic 
Loss

VGG16

V1

ReLU

V2

L2 
Norm

Input Image + 
Edge Boxes

CK

Conditional 
Embeddings

Fig. 1. Our CITE model separates phrases into diﬀerent groups and learns conditional
embeddings for these groups in a single end-to-end model. Ass
Conditional Prior Networks for Optical Flow

Yanchao Yang

Stefano Soatto

UCLA Vision Lab
University of California, Los Angeles, CA 90095

{yanchao.yang, soatto}@cs.ucla.edu

Abstract. Classical computation of optical ﬂow involves generic pri-
ors (regularizers) that capture rudimentary statistics of images, but not
long-range correlations or semantics. On the other hand, fully supervised
methods learn the regularity in the annotated data, without explicit reg-
ularization and with the risk of overﬁtting. We seek to learn richer priors
on the set of possible ﬂows that are statistically compatible with an
image. Once the prior is learned in a supervised fashion, one can eas-
ily learn the full map to infer optical ﬂow directly from two or more
images, without any need for (additional) supervision. We introduce a
novel architecture, called Conditional Prior Network (CPN), and show
how to train it to yield a conditional prior. When used in conjunction
with a simple optical ﬂow architecture, the CPN beats all variational
methods and all unsupervised learning-based ones using the same data
term. It performs comparably to fully supervised ones, that however are
ﬁne-tuned to a particular dataset. Our method, on the other hand, per-
forms well even when transferred between datasets. Code is available at:
https://github.com/YanchaoYang/Conditional-Prior-Networks

1

Introduction

Consider Fig. 1: A given image (left) could give rise to many diﬀerent optical
ﬂows (OF) [18] depending on what another image of the same scene looks like:
It could show a car moving to the right (top), or the same apparently moving to
the left due to camera motion to the right (middle), or it could be an artiﬁcial
motion because the scene was a picture portraying the car, rather than the actual
physical scene. A single image biases, but does not constrain, the set of possible
ﬂows the underlying scene can generate. We wish to leverage the information an
image contains about possible compatible ﬂows to learn better priors than those
implied by generic regularizers. Note that all three ﬂows in Fig. 1 are equally
valid under a generic prior (piecewise smoothness), but not under a natural prior
(cars moving in the scene).
A regularizer is a criterion that, when added to a data ﬁtting term, constrains
the solution of an inverse problem. These two criteria (data term and regularizer)

2

Y. Yang and S. Soatto

Fig. 1. A single image biases, but does not constrain, the set of optical ﬂows that can
be generated from it, depending on whether the camera was static but ob jects were
moving (top), or the camera was moving (center), or the scene was ﬂat (bottom) and
moving on a plane in an un-natural scenario. Flow ﬁelds here are generated by our
CPNFlow.

are usually formalized as an energy function, which is minimized to, ideally, ﬁnd
a unique global optimum.1

1.1 Our approach in context

In classical (variational) OF, the regularizer captures very rudimentary low-order
statistics [4,9,2
Connecting Gaze, Scene, and Attention:
Generalized Attention Estimation via Joint
Modeling of Gaze and Scene Saliency

Eunji Chong, Nataniel Ruiz, Yongxin Wang, Yun Zhang, Agata Rozga, and
James M. Rehg

School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, USA

{eunjichong,nataniel.ruiz,ywang751,yzhang467,agata,rehg}@gatech.edu

Abstract. This paper addresses the challenging problem of estimating
the general visual attention of people in images. Our proposed method
is designed to work across multiple naturalistic social scenarios and pro-
vides a full picture of the sub ject’s attention and gaze. In contrast, ear-
lier works on gaze and attention estimation have focused on constrained
problems in more speciﬁc contexts. In particular, our model explicitly
represents the gaze direction and handles out-of-frame gaze targets. We
leverage three diﬀerent datasets using a multi-task learning approach.
We evaluate our method on widely used benchmarks for single-tasks
such as gaze angle estimation and attention-within-an-image, as well as
on the new challenging task of generalized visual attention prediction.
In addition, we have created extended annotations for the MMDB and
GazeFollow datasets which are used in our experiments, which we will
publicly release.

Keywords: Visual attention · Gaze estimation · Saliency

1

Introduction

As humans, we are exquisitely sensitive to the gaze of others. We can rapidly
infer if another person is making eye contact, follow their gaze to identify their
gaze target, categorize quick glances to ob jects, and even identify when someone
is not paying attention [19]. Automatically detecting and quantifying these types
of visual attention from images and video remains a complex, open challenge.
Although gaze estimation has long been an active area of research, most work
has focused on relatively constrained versions of the problem in speciﬁc prede-
termined contexts. For example, [31, 18] predict the gaze target given that the
person is looking at a point on a smartphone screen, [23] predicts ﬁxation on
an ob ject given that the person is looking at salient ob ject within the frame,
[7, 30] predict eye contact given that the camera is located near the sub ject’s
eyes, and [24] predicts the focus of a person’s gaze across views in commercial
movies which include camera views that follow the actor’s attention. It remains
a signiﬁcant challenge to design a system that can model the visual attention of

2

E. Chong, N. Ruiz, Y. Wang, Y. Zhang, A. Rozga, and J. M. Rehg

Fig. 1. We present a model which aims to understand diﬀerent aspects of the gener-
alized attention prediction problem which are exempliﬁed above. In (a) sub jects are
looking at a salient ob ject in the scene, in (b) the sub ject is looking somewhere outside
of the frame and in (c) the sub ject is looking at or around the camera. Our model
predicts the 3D gaze vector of sub jects in each of these images, along with the location
of the
Consensus-Driven Propagation in
Massive Unlabeled Data for Face Recognition

Xiaohang Zhan1 [0000−0003−2136−7592] , Ziwei Liu1 [0000−0002−4220−5958] ,
Junjie Yan2 , Dahua Lin1 [0000−0002−8865−7896] , and
Chen Change Loy3 [0000−0001−5345−1591]

1 CUHK - SenseTime Joint Lab, The Chinese University of Hong Kong

{zx017, zwliu, dhlin}@ie.cuhk.edu.hk

2 SenseTime Group Limited

3 Nanyang Technological University

yanjunjie@sensetime.com

ccloy@ieee.org

Abstract. Face recognition has witnessed great progress in recent years,
mainly attributed to the high-capacity model designed and the abundant
labeled data collected. However, it becomes more and more prohibitive
to scale up the current million-level identity annotations. In this work,
we show that unlabeled face data can be as eﬀective as the labeled ones.
Here, we consider a setting closely mimicking the real-world scenario,
where the unlabeled data are collected from unconstrained environments
and their identities are exclusive from the labeled ones. Our main in-
sight is that although the class information is not available, we can still
faithfully approximate these semantic relationships by constructing a re-
lational graph in a bottom-up manner. We propose Consensus-Driven
Propagation (CDP) to tackle this challenging problem with two mod-

2

X. Zhan, Z. Liu, D. Lin, and C. C. Loy

To alleviate the aforementioned challenges, we shift the focus from obtain-
ing more manually labels to leveraging more unlabeled data. Unlike large-scale
identity annotations, unlabeled face images are extremely easy to obtain. For
example, using a web crawler facilitated by an oﬀ-the-shelf face detector would
produce abundant in-the-wild face images or videos [24]. Now the critical ques-
tion becomes how to leverage the huge existing unlabeled data to boost the
performance of large-scale face recognition. This problem is reminiscent of the
conventional semi-supervised learning (SSL) [34], but signiﬁcantly diﬀers from
SSL in two aspects: First, the unlabeled data are collected from unconstrained
environments, where pose, illumination, occlusion variations are extremely large.
It is non-trivial to reliably compute the similarity between diﬀerent unlabeled
samples in this in-the-wild scenario. Second, there is usually no identity overlap-
ping between the collected unlabeled data and the existing labeled data. Thus,
the popular label propagation paradigm [35] is no longer feasible here.
In this work, we study this challenging yet meaningful semi-supervised face
recognition problem, which can be formally described as follows. In addition to
some labeled data with known face identities, we also have access to a massive
number of in-the-wild unlabeled samples whose identities are exclusive from the
labeled ones. Our goal is to maximize the utility of the unlabeled data so that
the ﬁnal performance can closely match the performance when all the samples
are labeled. One key insight here is that although unlabeled data do not
Constraint-Aware Deep Neural Network
Compression

Changan Chen, Frederick Tung, Naveen Vedula, and Greg Mori

School of Computing Science,
Simon Fraser University

{cca278,ftung,nvedula}@sfu.ca, mori@cs.sfu.ca

Abstract. Deep neural network compression has the potential to bring
modern resource-hungry deep networks to resource-limited devices. How-
ever, in many of the most compelling deployment scenarios of compressed
deep networks, the operational constraints matter: for example, a pedes-
trian detection network on a self-driving car may have to satisfy a latency
constraint for safe operation. We propose the ﬁrst principled treatment of
deep network compression under operational constraints. We formulate
the compression learning problem from the perspective of constrained
Bayesian optimization, and introduce a cooling (annealing) strategy to
guide the network compression towards the target constraints. Experi-
ments on ImageNet demonstrate the value of modelling constraints di-
rectly in network compression.

1

Introduction

Modern deep neural networks contain millions of parameters over dozens or even
hundreds of layers [1, 2]. Standard benchmarks such as ImageNet [3] have incen-
tivized the design of increasingly expensive networks, as the additional expres-
siveness seems necessary to correctly handle the remaining hard test samples
[4]. However, the deployment of deep networks in real-world systems requires
consideration of the computation cost. The issue of computation cost has led to
a natural surge in interest in deep network compression [5–22].
Constraints matter in many of the most compelling deployment scenarios for
compressed deep neural networks. For example, deep neural network compression
enables us to deploy powerful networks in systems such as self-driving vehicles
with real-time operation requirements. A self-driving vehicle may have latency
constraints for executing a scene segmentation routine: if the network cannot
return predictions within 50 ms on average, for instance, the safe operation of
the vehicle may be compromised. As another example, deep network compression
enables us to deploy compact networks on embedded platforms with limited
computing power. A drone may have ﬁxed memory constraints and only be able
to run a 12 MB network on-board.
Previous work on deep network compression has focused on achieving the
highest compression rate while maintaining an acceptable level of accuracy (e.g.
within 1-2% of the uncompressed network’s accuracy). We refer to this general

2

C. Chen, F. Tung, N. Vedula, and G. Mori

Constraint-Aware Network Compression (One-step) 

F0 

H/M 

[if max iters not reached] 

F0 ,  

[if max iters 
reached] 

 

A 

best  

F 

Φ 

Φ 

F’ 

Constraint-Aware Network Compression with Cooling 

t := 1 

Ft-1 

H/M 

[if max iters not reached] 
Ft-1 , t 

Φ 

[if max iters 
reached] 

 

A 

Ft’ 

best t 

Φ 

 

Ft 

[if t = T] 

[if t ≠ T] 

t := t+1 

H/M 

Hyperparameter 
selection /  
M
Context Reﬁnement for Ob ject Detection

Zhe Chen, Shaoli Huang, and Dacheng Tao

UBTECH Sydney AI Centre, SIT, FEIT, University of Sydney, Australia

{zche4307}@uni.sydney.edu.au

{shaoli.huang, dacheng.tao}@sydney.edu.au

Abstract. Current two-stage ob ject detectors, which consists of a re-
gion proposal stage and a reﬁnement stage, may produce unreliable re-
sults due to ill-localized proposed regions. To address this problem, we
propose a context reﬁnement algorithm that explores rich contextual in-
formation to better reﬁne each proposed region. In particular, we ﬁrst
identify neighboring regions that may contain useful contexts and then
perform reﬁnement based on the extracted and uniﬁed contextual in-
formation. In practice, our method eﬀectively improves the quality of
the ﬁnal detection results as well as region proposals. Empirical studies
show that context reﬁnement yields substantial and consistent improve-
ments over diﬀerent baseline detectors. Moreover, the proposed algorithm
brings around 3% performance gain on PASCAL VOC benchmark and
around 6% gain on MS COCO benchmark respectively.

Keywords: Ob ject Detection · Context Analysis · Deep Convolutional
Neural Network

1

Introduction

Recent top-performing ob ject detectors, such as Faster RCNN [29] and Mask
RCNN [16], are mostly based on a two-stage paradigm which ﬁrst generates
a sparse set of ob ject proposals and then reﬁnes the proposals by adjusting
their coordinates and predicting their categories. Despite great success, these
methods tend to produce inaccurate bounding boxes and false labels after the
reﬁnement because of the poor-quality proposals generated in the ﬁrst stage. As
illustrated in Figure 1, if a proposed region has a partial overlap with a true
ob ject, existing methods would suﬀer reﬁnement failures since this region does
not contain suﬃcient information for holistic ob ject perception. Although much
eﬀort such as [21] has been dedicated to enhance the quality of ob ject proposals,
it still cannot guarantee that the proposed regions can have a satisfactory overlap
for each ground truth.
To tackle the aforementioned issue, we augment the representation for each
proposed region by leveraging its surrounding regions. This is motivated by the
fact that surrounding regions usually contain complementary information on
ob ject appearance and high-level characteristics, e.g., semantics and geometric
relationships, for a proposed region. Diﬀerent from related approaches [36, 12, 37,
26] that mainly include additional visual features from manually picked regions

2

Zhe Chen, Shaoli Huang, and Dacheng Tao

(a)

v i s u al features

(b)

dog

dog

cat

Refi n ement

Existing Refinement

(c)

dog

Context	
Ref inement

v i s u al features &
co n t ex ts

Proposed Refinement

Pro p o s ed Reg i o n s

Fig. 1. Overview of the pipeline for the proposed context reﬁnement algorithm compar-
ing to existing reﬁnement pipeline. Existing pipeline (b) reﬁnes each proposed region
Contextual-based Image Inpainting: Infer,
Match, and Translate

Yuhang Song*1 [0000−0003−4990−2964] , Chao Yang*1 [0000−0002−6553−7963] , Zhe
Lin2 [0000−0003−1154−9907] , Xiaofeng Liu3 [0000−0002−4514−2016] , Qin
Huang1 [0000−0002−3031−0208] , Hao Li1,4,5 [0000−0002−4019−3420] , and C.-C. Jay
Kuo1 [0000−0001−9474−5035]

1 University of Southern California, 3740 McClintock Ave, Los Angeles, USA

{yuhangso,chaoy,qinhuang}@usc.edu,cckuo@sipi.usc.edu

2 Adobe Research, 345 Park Ave, San Jose, USA zlin@adobe.com
3 Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, USA

4 Pinscreen, 525 Broadway, Santa Monica, USA hao@hao-li.com
5 USC Institute for Creative Technologies, 12015 E Waterfront Dr, Los Angeles, USA

liuxiaofeng@cmu.edu

Abstract. We study the task of image inpainting, which is to ﬁll in the
missing region of an incomplete image with plausible contents. To this
end, we propose a learning-based approach to generate visually coherent
completion given a high-resolution image with missing components. In
order to overcome the diﬃculty to directly learn the distribution of high-
dimensional image data, we divide the task into inference and translation
as two separate steps and model each step with a deep neural network.
We also use simple heuristics to guide the propagation of local textures
from the boundary to the hole. We show that, by using such techniques,
inpainting reduces to the problem of learning two image-feature transla-
tion functions in much smaller space and hence easier to train. We eval-
uate our method on several public datasets and show that we generate
results of better visual quality than previous state-of-the-art methods.

Keywords: Image inpainting · GANs · Feature manipulation.

1

Introduction

The problem of generating photo-realistic images from sampled noise or con-
ditioning on other inputs such as images, texts or labels has been heavily in-
vestigated. In spite of recent progress of deep generative models such as Pixel-
CNN [26], VAE [20] and GANs [12], generating high-resolution images remains
a diﬃcult task. This is mainly because modeling the distribution of pixels is dif-
ﬁcult and the trained models easily introduce blurry components and artifacts
when the dimensionality becomes high. Several approaches have been proposed

* indicates equal contributions.

2

Y. Song et al.

(a)

(b)

(c)

(d)

(e)

(f )

Fig. 1. Our result comparing with GL inpainting [14]. (a) & (d) The input image with
missing hole. (b) & (d) Inpainting result given by GL inpainting [14]. (c) & (f ) Final
inpainting result using our approach. The size of images are 512x512.

to alleviate the problem, usually by leveraging multi-scale training [36, 6] or in-
corporating prior information [24].
In addition to the general image synthesis problem, the task of image in-
painting can be described as: given an incomplete image as input, how do we
ﬁll in the missing parts with semantically and visually plausible contents. We
are interested in t
ContextVP: Fully Context-Aware Video
Prediction

Wonmin Byeon1,2,3,4 , Qin Wang2 ,
Rupesh Kumar Srivastava4 , and Petros Koumoutsakos2

1 NVIDIA, Santa Clara, CA, USA

wbyeon@nvidia com

2 ETH Zurich, Zurich, Switzerland
3 The Swiss AI Lab IDSIA, Manno Switzerland
4 NNAISENSE, Lugano, Switzerland

Abstract. Video prediction models based on convolutional networks,
recurrent networks, and their combinations often result in blurry pre-
dictions. We identify an important contributing factor for imprecise pre-
dictions that has not been studied adequately in the literature: blind
spots, i.e., lack of access to all relevant past information for accurately
predicting the future. To address this issue, we introduce a fully context-
aware architecture that captures the entire available past context for each
pixel using Parallel Multi-Dimensional LSTM units and aggregates it us-
ing blending units. Our model outperforms a strong baseline network of
20 recurrent convolutional layers and yields state-of-the-art performance
for next step prediction on three challenging real-world video datasets:
Human 3.6M, Caltech Pedestrian, and UCF-101. Moreover, it does so
with fewer parameters than several recently proposed models, and does
not rely on deep convolutional networks, multi-scale architectures, sepa-
ration of background and foreground modeling, motion ﬂow learning, or
adversarial training. These results highlight that full awareness of past
context is of crucial importance for video prediction.

1

Introduction

Unsupervised learning from unlabeled videos has recently emerged as an im-
portant direction of research. In the most common setting, a model is trained
to predict future frames conditioned on the past and learns a representation
that captures information about the appearance and the motion of ob jects in a
video without external supervision. This opens up several possibilities: the model
can be used as a prior for video generation, it can be utilized for model-based
reinforcement learning [32], or the learned representations can be transferred
to other video analysis tasks such as action recognition [30]. However, learning
such predictive models for natural videos is a rather challenging problem due to
the diversity of ob jects and backgrounds, various resolutions, ob ject occlusion,
camera movement, dynamic scene and light changes between frames. As a re-
sult, current video prediction models based on convolutional networks, recurrent
networks, and their combinations often result in imprecise (blurry) predictions.

2

W. Byeon, Q. Wang, R. K. Srivastava, and P. Koumoutsakos

Fig. 1: (left) The Convolutional LSTM (ConvLSTM) context dependency between two
successive frames. (right) The context dependency ﬂow in ConvLSTM over time for
frame t = T . Blind areas shown in gray cannot be used to predict the pixel value at
time T + 1. Closer time frames have larger blind areas.

Fig. 2: (top) Context dependency between two frames when using Parallel MD-LS
Contour Knowledge Transfer for Salient Ob ject
Detection

Xin Li1∗ , Fan Yang1∗ , Hong Cheng1 , Wei Liu1 , Dinggang Shen2

1 University of Electronic Science and Technology of China, Chengdu 611731, China
2 Department of Radiology and BRIC, University of North Carolina at Chapel Hill,
North Carolina 27599, USA

{xinli uestc,fanyang uestc}@hotmail.com, hcheng@uestc.edu.cn,
dgshen@med.unc.edu

Abstract. In recent years, deep Convolutional Neural Networks (CNNs)
have broken all records in salient ob ject detection. However, training such
a deep model requires a large amount of manual annotations. Our goal is
to overcome this limitation by automatically converting an existing deep
contour detection model into a salient ob ject detection model without us-
ing any manual salient ob ject masks. For this purpose, we have created a
deep network architecture, namely Contour-to-Saliency Network (C2S-
Net), by grafting a new branch onto a well-trained contour detection
network. Therefore, our C2S-Net has two branches for performing two
diﬀerent tasks: 1) predicting contours with the original contour branch,
and 2) estimating per-pixel saliency score of each image with the newly-
added saliency branch. To bridge the gap between these two tasks, we
further propose a contour-to-saliency transferring method to automati-
cally generate salient ob ject masks which can be used to train the saliency
branch from outputs of the contour branch. Finally, we introduce a novel
alternating training pipeline to gradually update the network parame-
ters. In this scheme, the contour branch generates saliency masks for
training the saliency branch, while the saliency branch, in turn, feeds
back saliency knowledge in the form of saliency-aware contour labels,
for ﬁne-tuning the contour branch. The proposed method achieves state-
of-the-art performance on ﬁve well-known benchmarks, outperforming
existing fully supervised methods while also maintaining high eﬃciency.

Keywords: Saliency detection · deep learning · transfer learning

1

Introduction

Salient ob ject detection, which aims at locating the most visually conspicuous
ob ject(s) in natural images, is critically important to computer vision. It can be
used in a variety of tasks such as human pose estimation [5], semantic segmenta-
tion [11], image/video captioning [25], and dense semantic correspondences [34].

* Both authors contribute equally to this work.
Code and pre-trained models are available at https://github.com/lixin666/C2SNet.

2

X. Li, F. Yang, H. Cheng, W. Liu, D. Sheng

Fig. 1. Saliency maps produced by currently best deep saliency models (DSS [8],
UCF [38], and Amulet [37]) and ours. Diﬀerent from these fully supervised methods,
our method requires no groundtruth salient object mask for training deep CNNs.

Over the past decades, the techniques of salient ob ject detection have evolved
dramatically. Traditional methods [3, 4, 20] only use low-level features and cues
for identifying salient regions in an i
Convolutional Networks with
Adaptive Inference Graphs

Andreas Veit

Serge Belongie

Department of Computer Science & Cornell Tech, Cornell University, New York

{av443,sjb344}@cornell.edu

Abstract. Do convolutional networks really need a ﬁxed feed-forward
structure? What if, after identifying the high-level concept of an im-
age, a network could move directly to a layer that can distinguish ﬁne-
grained diﬀerences? Currently, a network would ﬁrst need to execute
sometimes hundreds of intermediate layers that specialize in unrelated
aspects. Ideally, the more a network already knows about an image,
the better it should be at deciding which layer to compute next. In
this work, we propose convolutional networks with adaptive inference
graphs (ConvNet-AIG) that adaptively deﬁne their network topology
conditioned on the input image. Following a high-level structure simi-
lar to residual networks (ResNets), ConvNet-AIG decides for each input
image on the ﬂy which layers are needed. In experiments on ImageNet
we show that ConvNet-AIG learns distinct inference graphs for diﬀer-
ent categories. Both ConvNet-AIG with 50 and 101 layers outperform
their ResNet counterpart, while using 20% and 33% less computations
respectively. By grouping parameters into layers for related classes and
only executing relevant layers, ConvNet-AIG improves both eﬃciency
and overall classiﬁcation quality. Lastly, we also study the eﬀect of adap-
tive inference graphs on the susceptibility towards adversarial examples.
We observe that ConvNet-AIG shows a higher robustness than ResNets,
complementing other known defense mechanisms.

1

Introduction

Often, convolutional networks (ConvNets) are already conﬁdent about the high-
level concept of an image after only a few layers. This raises the question of what
happens in the remainder of the network that often comprises hundreds of layers
for many state-of-the-art models. To shed light on this, it is important to note
that due to their success, ConvNets are used to classify increasingly large sets
of visually diverse categories. Thus, most parameters model high-level features
that, in contrast to low-level and many mid-level concepts, cannot be broadly
shared across categories. As a result, the networks become larger and slower as
the number of categories rises. Moreover, for any given input image the number
of computed features focusing on unrelated concepts increases.
What if, after identifying that an image contains a bird, a ConvNet could
move directly to a layer that can distinguish diﬀerent bird species, without ex-
ecuting intermediate layers that specialize in unrelated aspects? Intuitively, the

2

A. Veit and S. Belongie

Tradi�onal feed-forward ConvNet:

f1

f2

ResNet:

ConvNet-AIG:

+

f1

+

f2

+

f1

+

f2

Fig. 1. ConvNet-AIG (right) follows a high level structure similar to ResNets (center)
by introducing identity skip-connections that bypass each layer. The key diﬀerence is
that for each layer, a gate determine
Coreset-Based Neural Network Compression

Abhimanyu Dubey⋆ 1 , Moitreya Chatterjee⋆ 2 , and Narendra Ahuja2

1 Massachusetts Institute of Technology, Cambridge MA 02139, USA

2 University of Illinois at Urbana-Champaign, Champaign IL 61820, USA

metro.smiles@gmail.com, n-ahuja@illinois.edu

dubeya@mit.edu

Abstract. We propose a novel Convolutional Neural Network (CNN)
compression algorithm based on coreset representations of ﬁlters. We
exploit the redundancies extant in the space of CNN weights and neuronal
activations (across samples) in order to obtain compression. Our method
requires no retraining, is easy to implement, and obtains state-of-the-art
compression performance across a wide variety of CNN architectures.
Coupled with quantization and Huﬀman coding, we create networks that
provide AlexNet-like accuracy, with a memory footprint that is 832×
smaller than the original AlexNet, while also introducing signiﬁcant reduc-
tions in inference time as well. Additionally these compressed networks
when ﬁne-tuned, successfully generalize to other domains as well.

1

Introduction

Convolutional neural networks, while immensely powerful, often are resource-
intensive[35,50,54,24,26]. Popular CNN models such as AlexNet [35] and VGG-
16 [50], for instance, have 61 and 138 million parameters and consume in excess
of 200MB and 500MB of memory space respectively. This characteristic of deep
CNN architectures reduces their portability, and poses a severe bottleneck for
implementation in resource constrained environments [17]. Additionally, design
choices for CNN architectures, such as network depth, ﬁlter sizes, and number of
ﬁlters seem arbitrary and motivated purely by empirical performance at a partic-
ular task, permitting little room for interpretability. Moreover, the architecture
design is not necessarily fully optimized for the network to be yielding a certain
level of precision, making these models highly resource-ineﬃcient.
Several prior approaches have thus sought to reduce the computational com-
plexity of these models. Work aimed at designing eﬃcient CNN architectures,
such as Residual Networks (ResNets) [25] and DenseNets [28] have shown promise
at alleviating the challenge of model complexity. These CNNs provide higher
performance on classiﬁcation at only a fraction of the number of parameters of
their more resource intensive counterparts. However, despite being more compact,
redundancies remain in such networks, leaving room for further compression.
In this work, we propose a novel method that exploits inter-ﬁlter dependen-
cies extant in the convolutional ﬁlter banks of CNNs to compress pre-trained

⋆ Equal Contribution.

2

A. Dubey∗ , M. Chatterjee∗ and N. Ahuja

computationally intensive neural networks. Additionally we leverage neuronal
activation patterns across samples to prune out irrelevant ﬁlters. Our compres-
sion pipeline consists of ﬁnely pruning the ﬁlters of every layer of the CNN
based on sample activation patterns, follo
CornerNet: Detecting Ob jects as
Paired Keypoints

Hei Law[0000−0003−1009−164X ] , Jia Deng[0000−0001−9594−4554]

University of Michigan, Ann Arbor

{heilaw,jiadeng}@umich.edu

Abstract. We propose CornerNet, a new approach to ob ject detection
where we detect an ob ject bounding box as a pair of keypoints, the
top-left corner and the bottom-right corner, using a single convolution
neural network. By detecting ob jects as paired keypoints, we eliminate
the need for designing a set of anchor boxes commonly used in prior
single-stage detectors. In addition to our novel formulation, we introduce
corner pooling, a new type of pooling layer that helps the network better
localize corners. Experiments show that CornerNet achieves a 42.1% AP
on MS COCO, outperforming all existing one-stage detectors.

Keywords: Ob ject Detection

1

Introduction

Ob ject detectors based on convolutional neural networks (ConvNets) [20, 36, 15]
have achieved state-of-the-art results on various challenging benchmarks [24, 8,
9]. A common component of state-of-the-art approaches is anchor boxes [32,
25], which are boxes of various sizes and aspect ratios that serve as detection
candidates. Anchor boxes are extensively used in one-stage detectors [25, 10, 31,
23], which can achieve results highly competitive with two-stage detectors [32, 12,
11, 13] while being more eﬃcient. One-stage detectors place anchor boxes densely
over an image and generate ﬁnal box predictions by scoring anchor boxes and
reﬁning their coordinates through regression.
But the use of anchor boxes has two drawbacks. First, we typically need
a very large set of anchor boxes, e.g. more than 40k in DSSD [10] and more
than 100k in RetinaNet [23]. This is because the detector is trained to classify
whether each anchor box suﬃciently overlaps with a ground truth box, and a
large number of anchor boxes is needed to ensure suﬃcient overlap with most
ground truth boxes. As a result, only a tiny fraction of anchor boxes will overlap
with ground truth; this creates a huge imbalance between positive and negative
anchor boxes and slows down training [23].
Second, the use of anchor boxes introduces many hyperparameters and design
choices. These include how many boxes, what sizes, and what aspect ratios. Such
choices have largely been made via ad-hoc heuristics, and can become even more
complicated when combined with multiscale architectures where a single network

2

H. Law, J. Deng

Heatmaps

Embeddings

Top-Left Corners

ConvNet

Bottom-Right Corners

Fig. 1. We detect an ob ject as a pair of bounding box corners grouped together. A
convolutional network outputs a heatmap for all top-left corners, a heatmap for all
bottom-right corners, and an embedding vector for each detected corner. The network
is trained to predict similar embeddings for corners that belong to the same ob ject.

makes separate predictions at multiple resolutions, with each scale using diﬀerent
features and its own set of anchor boxes [25, 10, 23]
Correcting the Triplet Selection Bias
for Triplet Loss

Baosheng Yu1 , Tongliang Liu1 , Mingming Gong2,3 ,
Changxing Ding4 , and Dacheng Tao1

1 UBTECH Sydney AI Centre and SIT, FEIT, The University of Sydney
2 Department of Biomedical Informatics, University of Pittsburgh
3 Department of Philosophy, Carnegie Mellon University
4 School of Electronic and Information Engineering, South China University of
Technology

bayu0826@uni.sydney.edu.au, tongliang.liu@sydney.edu.au,
mig73@pitt.edu.com, chxding@scut.edu.cn, dacheng.tao@sydney.edu.au

Abstract. Triplet loss, popular for metric learning, has made a great
success in many computer vision tasks, such as ﬁne-grained image clas-
siﬁcation, image retrieval, and face recognition. Considering that the
number of triplets grows cubically with the size of training data, triplet
selection is thus indispensable for eﬃciently training with triplet loss.
However, in practice, the training is usually very sensitive to the selec-
tion of triplets, e.g., it almost does not converge with randomly selected
triplets and selecting the hardest triplets also leads to bad local minima.
We argue that the bias in the selection of triplets degrades the per-
formance of learning with triplet loss. In this paper, we propose a new
variant of triplet loss, which tries to reduce the bias in triplet selection by
adaptively correcting the distribution shift on the selected triplets. We
refer to this new triplet loss as adapted triplet loss. We conduct a number
of experiments on MNIST and Fashion-MNIST for image classiﬁcation,
and on CARS196, CUB200-2011, and Stanford Online Products for im-
age retrieval. The experimental results demonstrate the eﬀectiveness of
the proposed method.

Keywords: Triplet Loss · Selection Bias · Domain Adaptation

1

Introduction

Deep metric learning aims to learn a similarity or distance metric which enjoys
a small intra-class variation and a large inter-class variation [42]. Triplet loss is
a popular loss function for deep metric learning and has made a great success in
many computer vision tasks, such as ﬁne-grained image classiﬁcation [39], image
retrieval [17, 22], person re-identiﬁcation [6, 14], and face recognition [34, 31].
Recently, deep metric learning approaches employing triplet loss have attracted a
lot of attention due to their eﬃciency for dealing with enormous of labels, e.g., the
extreme multi-label classiﬁcation problem [32]. More speciﬁcally, for conventional
classiﬁcation approaches, the number of parameters will increase linearly with

2

B. Yu, T. Liu, M. Gong, C. Ding, and D. Tao

1

2

3

4

5

6

…

k

128-D

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

,

prepare data

extract feature embedding

select triplets

evaluate loss

Fig. 1: The pipeline of triplet loss based deep metric learning. In the ﬁrst stage, a
mini-batch is sampled from the training data, which usually contains k identities
with several images per identity. Deep neural networks the
Cross-Modal and Hierarchical Modeling of
Video and Text

Bowen Zhang⋆ 1 , Hexiang Hu⋆ 1 , and Fei Sha2

1 Dept. of Computer Science, U. of Southern California, Los Angeles, CA 90089
2 Netﬂix, 5808 Sunset Blvd, Los Angeles, CA 90028

zhan734@usc.edu,hexiangh@usc.edu, fsha@netflix.com⋆⋆

Abstract. Visual data and text data are composed of information at
multiple granularities. A video can describe a complex scene that is com-
posed of multiple clips or shots, where each depicts a semantically coher-
ent event or action. Similarly, a paragraph may contain sentences with
diﬀerent topics, which collectively conveys a coherent message or story. In
this paper, we investigate the modeling techniques for such hierarchical
sequential data where there are correspondences across multiple modali-
ties. Speciﬁcally, we introduce hierarchical sequence embedding (hse), a
generic model for embedding sequential data of diﬀerent modalities into
hierarchically semantic spaces, with either explicit or implicit correspon-
dence information. We perform empirical studies on large-scale video and
paragraph retrieval datasets and demonstrated superior performance by
the proposed methods. Furthermore, we examine the eﬀectiveness of our
learned embeddings when applied to downstream tasks. We show its
utility in zero-shot action recognition and video captioning.

Keywords: Hierarchical Sequence Embedding, Video Text Retrieval,
Video Description Generation, Action Recognition, Zero-shot Transfer

1

Introduction

Recently, there has been an intensive interest in multi-modal learning of vision
+ language. A few challenging tasks have been proposed: visual semantic em-
bedding (VSE) [16, 15, 5], image captioning [37, 42, 12, 21], and visual question
answering (VQA) [2, 47, 3]. To jointly understand these two modalities of data
and make inference over them, the main intuition is that diﬀerent types of data
can share a common semantic representation space. Examples are embedding
images and the visual categories [7], embedding images and texts for VSE [16],
and embedding images, questions, and answers for VQA [11]. Once embedded
into this common (vector) space, similarity and distances among originally het-
erogeneous data can be captured by learning algorithms.
While there has been a rich study on how to discover this shared seman-
tic representation on structures such as images, noun phrases (visual ob ject or

⋆ equal contribution
⋆⋆ On leave from U. of Southern California (feisha@usc.edu)

2

B. Zhang, H. Hu, and F. Sha

Fig. 1. Conceptual diagram of our approach for cross-modal modeling of video and
texts. The main idea is to embed both low-level (clips and sentences) and high-level
(video and paragraph) in their own semantic spaces coherently. As shown in the ﬁgure,
the 3 sentences (and the corresponding 3 clips) are mapped into a local embedding
space where the corresponding pairs of clips and sentences are placed close to each
other. As a whole, the videos and the paragrap
Cross-Modal Hamming Hashing

Yue Cao, Bin Liu, Mingsheng Long(B), and Jianmin Wang

School of Software, Tsinghua University, China
National Engineering Laboratory for Big Data Software
Beijing National Research Center for Information Science and Technology

{caoyue10,liubinthss}@gmail.com, {mingsheng,jimwang}@tsinghua.edu.cn

Abstract. Cross-modal hashing enables similarity retrieval across dif-
ferent content modalities, such as searching relevant images in response
to text queries. It provides with the advantages of computation eﬃciency
and retrieval quality for multimedia retrieval. Hamming space retrieval
enables eﬃcient constant-time search that returns data items within a
given Hamming radius to each query, by hash lookups instead of linear
scan. However, Hamming space retrieval is ineﬀective in existing cross-
modal hashing methods, sub ject to their weak capability of concentrating
the relevant items to be within a small Hamming ball, while worse still,
the Hamming distances between hash codes from diﬀerent modalities are
inevitably large due to the large heterogeneity across diﬀerent modalities.
This work presents Cross-Modal Hamming Hashing (CMHH), a novel
deep cross-modal hashing approach that generates compact and highly
concentrated hash codes to enable eﬃcient and eﬀective Hamming space
retrieval. The main idea is to penalize signiﬁcantly on similar cross-modal
pairs with Hamming distance larger than the Hamming radius threshold,
by designing a pairwise focal loss based on the exponential distribution.
Extensive experiments demonstrate that CMHH can generate highly con-
centrated hash codes and achieve state-of-the-art cross-modal retrieval
performance for both hash lookups and linear scan scenarios on three
benchmark datasets, NUS-WIDE, MIRFlickr-25K, and IAPR TC-12.

Keywords: deep hashing, cross-modal hashing, Hamming space retrieval

1

Introduction

With the explosion of big data, large-scale and high-dimensional data has been
widespread in search engines and social networks. As relevant data items from
diﬀerent modalities may convey semantic correlations, it is signiﬁcant to sup-
port cross-modal retrieval, which returns semantically-relevant results from one
modality in response to a query of another modality. Recently, a popular and ad-
vantageous solution to cross-modal retrieval is learning to hash [1], an approach
to approximate nearest neighbors (ANN) search across diﬀerent modalities with
both computation eﬃciency and search quality. It transforms high-dimensional
data into compact binary codes with similar binary codes for similar data, largely

2

Yue Cao, Bin Liu, Mingsheng Long, and Jianmin Wang

reducing the computational burdens of distance calculation and candidates prun-
ing on large-scale high-dimensional data. Although the semantic gap across low-
level descriptors and high-level semantics [2] has been reduced by deep learning,
the intrinsic heterogeneity across modalities remains another challenge.

Previous 
Cross-Modal Ranking with Soft Consistency and
Noisy Labels for Robust RGB-T Tracking

Chenglong Li1,2 [0000−0002−7233−2739] , Chengli Zhu2 [0000−0001−8714−6755] , Yan
Huang1 [0000−0002−8239−7229] , Jin Tang2 [0000−0002−4123−268X ] , and Liang
Wang1 [0000−0001−5224−8647]

1Center for Research on Intelligent Perception and Computing (CRIPAC),
National Labotary of Pattern Recognition (NLPR),
Institute of Automation, Chinese Academy of Sciences (CASIA)
2School of Computer Science and Technology, Anhui Univeristy

{lcl1314,zcl912,jtang99029}@foxmail.com, {yhuang,wangliang}@nlpr.ia.ac.cn

Abstract. Due to the complementary beneﬁts of visible (RGB) and
thermal infrared (T) data, RGB-T ob ject tracking attracts more and
more attention recently for boosting the performance under adverse illu-
mination conditions. Existing RGB-T tracking methods usually localize
a target ob ject with a bounding box, in which the trackers or detectors
is often aﬀected by the inclusion of background clutter. To address this
problem, this paper presents a novel approach to suppress background
eﬀects for RGB-T tracking. Our approach relies on a novel cross-modal
manifold ranking algorithm. First, we integrate the soft cross-modality
consistency into the ranking model which allows the sparse inconsisten-
cy to account for the diﬀerent properties between these two modalities.
Second, we propose an optimal query learning method to handle label
noises of queries. In particular, we introduce an intermediate variable
to represent the optimal labels, and formulate it as a l1 -optimization
based sparse learning problem. Moreover, we propose a single uniﬁed
optimization algorithm to solve the proposed model with stable and eﬃ-
cient convergence behavior. Finally, the ranking results are incorporated
into the patch-based ob ject features to address the background eﬀects,
and the structured SVM is then adopted to perform RGB-T tracking.
Extensive experiments suggest that the proposed approach performs well
against the state-of-the-art methods on large-scale benchmark datasets.

Keywords: Visual tracking, Information fusion, Manifold ranking, Soft
cross-modality consistency, Label optimization

1

Introduction

The goal of RGB-T tracking is to estimate the states of the target ob ject in
videos by fusing RGB and thermal (corresponds the visible and thermal in-
frared spectrum data, respectively) information, given the initial ground truth
bounding box. Recently, researchers pay more and more attention on RGB-T
tracking [1,2,3,4,5] partly due to the following reasons. i) The imaging quality of

2

Chenglong Li, Chengli Zhu, Yan Huang, Jin Tang, Liang Wang

(a)

(b)

Fig. 1. Typically complementary beneﬁts of RGB and thermal data [5]. (a) Beneﬁts of
thermal sources over RGB ones, where visible spectrum is disturbed by low illumina-
tion, high illumination and fog. (b) Beneﬁts of RGB sources over thermal ones, where
thermal spectrum is disturbed by glass and thermal crossover.

visible spect
CTAP: Complementary Temporal Action
Proposal Generation

Jiyang Gao⋆ , Kan Chen⋆ , Ram Nevatia

University of Southern California

{jiyangga, kanchen, nevatia}@usc.edu

Abstract. Temporal action proposal generation is an important task,
akin to ob ject proposals, temporal action proposals are intended to cap-

2

Jiyang Gao⋆ , Kan Chen⋆ , Ram Nevatia

adjustment

SW+R&A

sl iding w indows

ranking

TAG

actionness score

TAG

adjustment

TAG+R&A

actionness score

TAG

ranking

A

B

C

Fig. 1. The architectures of three baseline methods are shown: (1) SW+R&A: sliding
windows are processed by a model for proposal ranking and boundary adjustment,
e.g. TURN[2], SCNN [3]; (2) TAG: TAG [4] generate proposals based on unit-level
actionness; (3) TAG+R&A: actionness proposals are processed with proposal ranking
and boundary adjustment.

from sliding windows as input, and outputs scores for proposals. SCNN-prop [3]
is a representative of this type; it applies a binary classiﬁer to rank the sliding
windows. TURN [2] adopts temporal regression in additional to binary classiﬁ-
cation to adjust the boundary of sliding windows. The architecture of this type is
outlined as “SW-R&A” in Fig. 1. Sliding windows uniformly cover all segments
in the videos (thus cover every ground truth segment), however the drawback is
that the temporal boundaries are imprecise, in spite of the use of boundary ad-
justment, and thus high AR is reached at large number of retrieved of proposals,
as shown in circle A in Fig. 1.
The second type of action proposal generation methods can be summarized
as actionness score based. It applies binary classiﬁcation on a ﬁner level, i.e.,
unit or snippet (a few contiguous frames) level, to generate actionness scores for
each unit. A Temporal Action Grouping (TAG) [4] technique, derived from the
watershed algorithm [5], is designed to group continuous high-score regions as
proposals. Each proposal’s score is calculated as the average of its unit actionness
scores. The structure is shown as “TAG” in Fig. 1. This type of method generates
high precision boundaries, as long as the quality of actionness scores is high.
However, the actionness scores have two common failure cases: having high scores
at background segments, and having low scores at action segments. The former
case leads to generation of wrong proposals, while the latter case may omit some
correct proposals. These lead to the upper bound of AR performance limited at
a low value (circle B in Fig. 1).
Based on the above analysis, ranking-sliding-window and grouping-actionness-
score methods have two complementary properties: (1) The boundaries from
actionness-based proposals are more precise as they are predicted on a ﬁner
level, and window-level ranking could be more discriminative as it takes more
global contextual information; (2) actionness-based methods may omit some
correct proposals when quality of actionness scores is low, sliding windows can

CTAP: Complementary Temporal Action P
CubeNet: Equivariance to 3D Rotation
and Translation

Daniel Worrall1 [0000−0002−9810−0709] and Gabriel Brostow1 [0000−0001−8472−3828]

Computer Science Department, University College London, UK

{d.worrall,g.brostow}@cs.ucl.ac.uk

Abstract. 3D Convolutional Neural Networks are sensitive to transforma-
tions applied to their input. This is a problem because a voxelized version of a
3D ob ject, and its rotated clone, will look unrelated to each other after passing
through to the last layer of a network. Instead, an idealized model would
preserve a meaningful representation of the voxelized ob ject, while explaining
the pose-difference between the two inputs. An equivariant representation
vector has two components: the invariant identity part, and a discernable
encoding of the transformation. Models that can’t explain pose-differences

2

D. Worrall and G. Brostow

why should we bother learning these invariances, if we can enforce them a priori ?
If successful, we would not need as much training data [8, 50]. Indeed, convolutional
neural networks already have i) filter locality and ii) translational weight-tying built
directly into their architectures, which arguably could be learned using a multilayer
perceptron with a enough computational budget and training data.

We introduce a CNN architecture, which is linearly equivariant (a generalization of
invariance defined in the next section) to 3D rotations about patch centers. To the
best of our knowledge, this paper provides the first example of a group-CNN [8] with
linear equivariance to 3D rotations and 3D translations of voxelized data. By exploit-
ing the symmetries of the classification task, we are able to reduce the number of
trainable parameters using judicious weight tying. We also need less training and test
time data augmentation, since some aspects of 3D geometry are already ‘hard-baked’
into the network. We demonstrate state-of-the-art and comparable performance on
i) the ModelNet10 classification challenge, which is a standard 3D classification
benchmark task, and ii) the ISBI 2012 connectome segmentation benchmark, which
is a 3D anisotropic boundary segmentation problem. We have released our code at

https://deworrall92.github.com.

2 Background

For completeness, we set out our terminology and definitions. We outline definitions of
linear equivariance, invariance, groups, and convolution, and then combine these ideas
into the group convolution, which is the workhorse of the paper. These definitions
are not our contribution and can be found in textbooks such as [7], but we have tried
to standardize them and simplify notation.

Definition 1 (Equivariance) Consider a set of transformations G, where individ-
ual transformations are indexed as g ∈ G. Consider also a function or feature map
Φ : X → Y mapping inputs x ∈ X to outputs y ∈ Y . Transformations can be applied
to any x ∈ X using the operator T X
g : X → X , so that x 7→ T X
g [x]. The same can be
done for the outputs with y 7→ T 
CurriculumNet: Weakly Supervised Learning
from Large-Scale Web Images

Sheng Guo, Weilin Huang⋆ , Haozhi Zhang, Chenfan Zhuang, Dengke Dong,
Matthew R. Scott, and Dinglong Huang

Malong Technologies, Shenzhen, China
Shenzhen Malong Artiﬁcial Intelligence Research Center, Shenzhen, China

{sheng,whuang,haozhang,fan,dongdk,mscott,dlong}@malong.com

Abstract. We present a simple yet eﬃcient approach capable of train-
ing deep neural networks on large-scale weakly-supervised web images,
which are crawled rawly from the Internet by using text queries, with-
out any human annotation. We develop a principled learning strategy
by leveraging curriculum learning, with the goal of handling massive
amount of noisy labels and data imbalance eﬀectively. We design a new
learning curriculum by measuring the complexity of data using its distri-
bution density in a feature space, and rank the complexity in an unsuper-
vised manner. This allows for an eﬃcient implementation of curriculum
learning on large-scale web images, resulting in a high-performance CNN
model, where the negative impact of noisy labels is reduced substantially.
Importantly, we show by experiments that those images with highly noisy
labels can surprisingly improve the generalization capability of model, by
serving as a manner of regularization. Our approaches obtain the state-
of-the-art performance on four benchmarks, including Webvision, Ima-
geNet, Clothing-1M and Food-101. With an ensemble of multiple models,
we achieve a top-5 error rate of 5.2% on the Webvision challenge [18] for
1000-category classiﬁcation, which is the top performance that surpasses
other results by a large margin of about 50% relative error rate. Codes
and models are available at: https://github.com/guoshengcv/CurriculumNet.

Keywords: Curriculum learning · weakly supervised · noisy data · large-
scale · web images

1

Introduction

Deep convolutional networks have rapidly advanced numerous computer vision
tasks, providing the state-of-the-art performance on image classiﬁcation [9, 31,
34, 14, 37, 8], ob ject detection [28, 27, 22, 20], sematic segmentation [23, 11, 4, 10],
etc. They produce strong visual features by training the networks in a fully-
supervised manner using large-scale manually annotated datasets, such as Im-
ageNet [5], MS-COCO [21] and PASCAL VOC [6]. Obviously, full and clean
human annotations are of crucial importance to achieving a high-performance

⋆ Weilin Huang is the corresponding author (e-mail:whuang@malong.com).

2

S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong, M. R. Scott, D. Huang

Fig. 1. Image samples of WebVision dataset [19] from the categories of Carton, Dog,
Taxi and Banana. The dataset was collected from the Internet by using text enquires
generated from the 1, 000 semantic concepts of the Imagenet benchmark [5]. Obviously,
each category includes a number of mislabeled images as shown on the right.

model, and better results can be reasonably expected if a larger dataset is pro-
v
Data-Driven Sparse Structure Selection for Deep Neural
Networks

Zehao Huang[0000−0003−1653−208X ] and Naiyan Wang[0000−0002−0526−3331]

TuSimple

{zehaohuang18,winsty}@gmail.com

Abstract. Deep convolutional neural networks have liberated its extraordinary
power on various tasks. However, it is still very challenging to deploy state-
of-the-art models into real-world applications due to their high computational
complexity. How can we design a compact and effective network without mas-
sive experiments and expert knowledge? In this paper, we propose a simple and
effective framework to learn and prune deep models in an end-to-end manner.
In our framework, a new type of parameter – scaling factor is ﬁrst introduced
to scale the outputs of speciﬁc structures, such as neurons, groups or residual
blocks. Then we add sparsity regularizations on these factors, and solve this opti-
mization problem by a modiﬁed stochastic Accelerated Proximal Gradient (APG)
method. By forcing some of the factors to zero, we can safely remove the corre-
sponding structures, thus prune the unimportant parts of a CNN. Comparing with
other structure selection methods that may need thousands of trials or iterative
ﬁne-tuning, our method is trained fully end-to-end in one training pass without
bells and whistles. We evaluate our method, Sparse Structure Selection with sev-
eral state-of-the-art CNNs, and demonstrate very promising results with adaptive
depth and width selection. Code is available at: https://github.com/huangzehao/
sparse- structure- selection.

Keywords: sparse · model acceleration · deep network structure learning

1

Introduction

Deep learning methods, especially convolutional neural networks (CNNs) have achieved
remarkable performances in many ﬁelds, such as computer vision, natural language pro-
cessing and speech recognition. However, these extraordinary performances are at the
expense of high computational and storage demand. Although the power of modern
GPUs has skyrocketed in the last years, these high costs are still prohibitive for CNNs
to deploy in latency critical applications such as self-driving cars and augmented reality,
etc.

Recently, a signiﬁcant amount of works on accelerating CNNs at inference time
have been proposed. Methods focus on accelerating pre-trained models include direct
pruning [9, 24, 29, 13, 27], low-rank decomposition [7, 20, 49], and quantization [31, 6,
44]. Another stream of researches trained small and efﬁcient networks directly, such as
knowledge distillation [14, 33, 35], novel architecture designs [18, 15] and sparse learn-
ing [25, 50, 1, 43]. In spare learning, prior works [25] pursued the sparsity of weights.

2

Z. Huang, N. Wang

However, non-structure sparsity only produce random connectivities and can hardly uti-
lize current off-the-shelf hardwares such as GPUs to accelerate model inference in wall
clock time. To address this problem, recently methods [50, 1, 43] proposed to apply
group sparsity to retain a ha
